{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import preprocessing\n",
    "from model import CharCNN\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myload():\n",
    "    filelist=['1F60A','1F62D','1F64F','1F605','1F614','1F629','2764']\n",
    "    filelist_processed=['./tweet_test/data_processed_'+str(filename)+'.csv' for filename in filelist]\n",
    "    mydata=pd.DataFrame(columns=['phrase','label'])\n",
    "    for idx,fname in enumerate(filelist_processed):\n",
    "        temp=pd.read_csv(fname, header=None).rename(columns={ 0:'phrase'})\n",
    "        temp['label']=idx\n",
    "        mydata=mydata.append(temp,sort=False)\n",
    "\n",
    "    label_onehot=pd.get_dummies(mydata['label'])\n",
    "\n",
    "    label_onehot=pd.get_dummies(mydata['label'])\n",
    "    examples = []\n",
    "    labels = []\n",
    "    alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\\n\"\n",
    "    \n",
    "    cnt=0\n",
    "    for idx,values in enumerate(zip(mydata['phrase'].values, label_onehot.values)):\n",
    "        text_end_extracted=preprocessing.extract_end(list(values[0].lower()))\n",
    "        padded=preprocessing.pad_sentence(text_end_extracted)\n",
    "        text_int8_repr = preprocessing.string_to_int8_conversion(padded,alphabet)\n",
    "\n",
    "        examples.append(text_int8_repr)\n",
    "        labels.append(values[1])\n",
    "        \n",
    "        if idx%100000==0:\n",
    "            print(idx,'completed')       \n",
    "        \n",
    "    return np.array(examples, dtype=np.int8), np.array(labels, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "0 completed\n",
      "100000 completed\n",
      "200000 completed\n",
      "300000 completed\n",
      "Train/Dev split: 358158/31143\n",
      "Writing to d:\\dropbox\\machine_learning\\project\\cnn_character_level_recognition\\runs\\1540218159\n",
      "\n",
      "In epoch >> 1\n",
      "num batches per epoch is: 2799\n",
      "2018-10-22T23:22:41.501940: step 1, loss 4.89718, acc 0.0625\n",
      "2018-10-22T23:22:42.107322: step 2, loss 2.73686, acc 0.195312\n",
      "2018-10-22T23:22:42.874272: step 3, loss 2.54198, acc 0.234375\n",
      "2018-10-22T23:22:43.375931: step 4, loss 2.52544, acc 0.234375\n",
      "2018-10-22T23:22:43.902523: step 5, loss 2.43242, acc 0.28125\n",
      "2018-10-22T23:22:44.373263: step 6, loss 2.50049, acc 0.164062\n",
      "2018-10-22T23:22:44.880906: step 7, loss 2.24927, acc 0.203125\n",
      "2018-10-22T23:22:45.382564: step 8, loss 2.27937, acc 0.164062\n",
      "2018-10-22T23:22:46.004901: step 9, loss 2.10285, acc 0.21875\n",
      "2018-10-22T23:22:46.541467: step 10, loss 2.11179, acc 0.234375\n",
      "2018-10-22T23:22:47.038138: step 11, loss 2.14497, acc 0.15625\n",
      "2018-10-22T23:22:47.567722: step 12, loss 2.01536, acc 0.234375\n",
      "2018-10-22T23:22:48.104287: step 13, loss 2.09073, acc 0.203125\n",
      "2018-10-22T23:22:48.628884: step 14, loss 2.03313, acc 0.203125\n",
      "2018-10-22T23:22:49.166447: step 15, loss 1.94576, acc 0.265625\n",
      "2018-10-22T23:22:49.707002: step 16, loss 1.93045, acc 0.304688\n",
      "2018-10-22T23:22:50.161786: step 17, loss 2.04637, acc 0.265625\n",
      "2018-10-22T23:22:50.575679: step 18, loss 2.04318, acc 0.226562\n",
      "2018-10-22T23:22:51.038442: step 19, loss 1.92206, acc 0.21875\n",
      "2018-10-22T23:22:51.526139: step 20, loss 1.99525, acc 0.195312\n",
      "2018-10-22T23:22:52.040762: step 21, loss 2.06144, acc 0.21875\n",
      "2018-10-22T23:22:52.617221: step 22, loss 1.83069, acc 0.265625\n",
      "2018-10-22T23:22:53.210634: step 23, loss 1.9407, acc 0.25\n",
      "2018-10-22T23:22:53.719275: step 24, loss 1.96157, acc 0.226562\n",
      "2018-10-22T23:22:54.326650: step 25, loss 1.97157, acc 0.195312\n",
      "2018-10-22T23:22:54.893137: step 26, loss 1.85969, acc 0.234375\n",
      "2018-10-22T23:22:55.425712: step 27, loss 2.05273, acc 0.1875\n",
      "2018-10-22T23:22:55.936345: step 28, loss 1.96773, acc 0.1875\n",
      "2018-10-22T23:22:56.540730: step 29, loss 1.90913, acc 0.25\n",
      "2018-10-22T23:22:57.030420: step 30, loss 2.04849, acc 0.203125\n",
      "2018-10-22T23:22:57.632810: step 31, loss 1.82258, acc 0.25\n",
      "2018-10-22T23:22:58.228218: step 32, loss 1.8666, acc 0.273438\n",
      "2018-10-22T23:22:58.697961: step 33, loss 1.87393, acc 0.257812\n",
      "2018-10-22T23:22:59.177783: step 34, loss 1.79367, acc 0.28125\n",
      "2018-10-22T23:22:59.698191: step 35, loss 1.90297, acc 0.242188\n",
      "2018-10-22T23:23:00.329502: step 36, loss 1.80313, acc 0.296875\n",
      "2018-10-22T23:23:00.906959: step 37, loss 1.86444, acc 0.242188\n",
      "2018-10-22T23:23:01.491397: step 38, loss 1.84251, acc 0.296875\n",
      "2018-10-22T23:23:02.215460: step 39, loss 1.80956, acc 0.25\n",
      "2018-10-22T23:23:02.731081: step 40, loss 1.91369, acc 0.273438\n",
      "2018-10-22T23:23:03.264655: step 41, loss 1.90155, acc 0.304688\n",
      "2018-10-22T23:23:03.776287: step 42, loss 1.8967, acc 0.1875\n",
      "2018-10-22T23:23:04.261988: step 43, loss 1.76659, acc 0.304688\n",
      "2018-10-22T23:23:04.755668: step 44, loss 1.80412, acc 0.25\n",
      "2018-10-22T23:23:05.222419: step 45, loss 1.80954, acc 0.25\n",
      "2018-10-22T23:23:05.712111: step 46, loss 1.9012, acc 0.234375\n",
      "2018-10-22T23:23:06.324473: step 47, loss 1.83639, acc 0.296875\n",
      "2018-10-22T23:23:06.839097: step 48, loss 1.94739, acc 0.25\n",
      "2018-10-22T23:23:07.371673: step 49, loss 1.85625, acc 0.304688\n",
      "2018-10-22T23:23:07.856378: step 50, loss 1.87284, acc 0.25\n",
      "2018-10-22T23:23:08.304181: step 51, loss 1.80454, acc 0.242188\n",
      "2018-10-22T23:23:08.822793: step 52, loss 1.88216, acc 0.15625\n",
      "2018-10-22T23:23:09.359359: step 53, loss 1.79676, acc 0.265625\n",
      "2018-10-22T23:23:09.870991: step 54, loss 1.86232, acc 0.226562\n",
      "2018-10-22T23:23:10.546185: step 55, loss 1.81582, acc 0.273438\n",
      "2018-10-22T23:23:11.038867: step 56, loss 1.84333, acc 0.242188\n",
      "2018-10-22T23:23:11.529556: step 57, loss 1.82168, acc 0.34375\n",
      "2018-10-22T23:23:12.000299: step 58, loss 1.94764, acc 0.195312\n",
      "2018-10-22T23:23:12.490987: step 59, loss 1.83048, acc 0.296875\n",
      "2018-10-22T23:23:13.028548: step 60, loss 1.82566, acc 0.25\n",
      "2018-10-22T23:23:13.476351: step 61, loss 1.79652, acc 0.265625\n",
      "2018-10-22T23:23:13.980004: step 62, loss 1.79665, acc 0.234375\n",
      "2018-10-22T23:23:14.463711: step 63, loss 1.79717, acc 0.273438\n",
      "2018-10-22T23:23:14.999279: step 64, loss 1.95505, acc 0.210938\n",
      "2018-10-22T23:23:15.488969: step 65, loss 1.83829, acc 0.265625\n",
      "2018-10-22T23:23:15.991626: step 66, loss 1.8919, acc 0.203125\n",
      "2018-10-22T23:23:16.482314: step 67, loss 1.8508, acc 0.289062\n",
      "2018-10-22T23:23:17.002921: step 68, loss 1.71987, acc 0.296875\n",
      "2018-10-22T23:23:17.658170: step 69, loss 1.83079, acc 0.242188\n",
      "2018-10-22T23:23:18.246597: step 70, loss 1.88079, acc 0.273438\n",
      "2018-10-22T23:23:18.759226: step 71, loss 1.86694, acc 0.257812\n",
      "2018-10-22T23:23:19.276841: step 72, loss 1.89082, acc 0.265625\n",
      "2018-10-22T23:23:19.731625: step 73, loss 1.78672, acc 0.28125\n",
      "2018-10-22T23:23:20.230292: step 74, loss 1.85663, acc 0.328125\n",
      "2018-10-22T23:23:20.732948: step 75, loss 1.84899, acc 0.265625\n",
      "2018-10-22T23:23:21.294447: step 76, loss 1.87687, acc 0.203125\n",
      "2018-10-22T23:23:21.843977: step 77, loss 1.75482, acc 0.28125\n",
      "2018-10-22T23:23:22.422431: step 78, loss 1.88371, acc 0.265625\n",
      "2018-10-22T23:23:22.942042: step 79, loss 1.89506, acc 0.289062\n",
      "2018-10-22T23:23:23.375882: step 80, loss 1.93417, acc 0.210938\n",
      "2018-10-22T23:23:23.876543: step 81, loss 1.78171, acc 0.3125\n",
      "2018-10-22T23:23:24.462976: step 82, loss 1.88958, acc 0.257812\n",
      "2018-10-22T23:23:25.045417: step 83, loss 1.75619, acc 0.320312\n",
      "2018-10-22T23:23:25.582980: step 84, loss 1.77598, acc 0.273438\n",
      "2018-10-22T23:23:26.102592: step 85, loss 1.77321, acc 0.296875\n",
      "2018-10-22T23:23:26.664089: step 86, loss 1.87607, acc 0.203125\n",
      "2018-10-22T23:23:27.161759: step 87, loss 1.89437, acc 0.289062\n",
      "2018-10-22T23:23:27.585626: step 88, loss 1.84952, acc 0.273438\n",
      "2018-10-22T23:23:28.033428: step 89, loss 1.92795, acc 0.21875\n",
      "2018-10-22T23:23:28.521124: step 90, loss 1.90953, acc 0.242188\n",
      "2018-10-22T23:23:28.991865: step 91, loss 1.80581, acc 0.28125\n",
      "2018-10-22T23:23:29.421716: step 92, loss 1.88315, acc 0.226562\n",
      "2018-10-22T23:23:29.872511: step 93, loss 1.87805, acc 0.242188\n",
      "2018-10-22T23:23:30.349235: step 94, loss 1.81361, acc 0.242188\n",
      "2018-10-22T23:23:30.872836: step 95, loss 1.80206, acc 0.234375\n",
      "2018-10-22T23:23:31.476224: step 96, loss 1.92128, acc 0.234375\n",
      "2018-10-22T23:23:32.061658: step 97, loss 1.83285, acc 0.265625\n",
      "2018-10-22T23:23:32.591241: step 98, loss 1.82437, acc 0.210938\n",
      "2018-10-22T23:23:33.176676: step 99, loss 1.76262, acc 0.304688\n",
      "2018-10-22T23:23:33.757124: step 100, loss 1.81574, acc 0.265625\n",
      "2018-10-22T23:23:34.321615: step 101, loss 1.85003, acc 0.242188\n",
      "2018-10-22T23:23:35.014762: step 102, loss 1.83417, acc 0.296875\n",
      "2018-10-22T23:23:35.639403: step 103, loss 1.82159, acc 0.195312\n",
      "2018-10-22T23:23:36.279657: step 104, loss 1.79204, acc 0.296875\n",
      "2018-10-22T23:23:36.858111: step 105, loss 1.80319, acc 0.3125\n",
      "2018-10-22T23:23:37.410634: step 106, loss 1.82778, acc 0.304688\n",
      "2018-10-22T23:23:38.113755: step 107, loss 1.87147, acc 0.25\n",
      "2018-10-22T23:23:38.758031: step 108, loss 1.93983, acc 0.226562\n",
      "2018-10-22T23:23:39.473361: step 109, loss 1.75318, acc 0.328125\n",
      "2018-10-22T23:23:40.111655: step 110, loss 1.77975, acc 0.296875\n",
      "2018-10-22T23:23:40.741969: step 111, loss 1.79863, acc 0.28125\n",
      "2018-10-22T23:23:41.461047: step 112, loss 1.85377, acc 0.21875\n",
      "2018-10-22T23:23:42.063436: step 113, loss 1.87347, acc 0.242188\n",
      "2018-10-22T23:23:42.678791: step 114, loss 1.85724, acc 0.1875\n",
      "2018-10-22T23:23:43.257244: step 115, loss 1.74354, acc 0.28125\n",
      "2018-10-22T23:23:43.898529: step 116, loss 1.7736, acc 0.328125\n",
      "2018-10-22T23:23:44.458033: step 117, loss 1.75433, acc 0.257812\n",
      "2018-10-22T23:23:45.084359: step 118, loss 1.77626, acc 0.3125\n",
      "2018-10-22T23:23:45.778503: step 119, loss 1.80843, acc 0.289062\n",
      "2018-10-22T23:23:46.630226: step 120, loss 1.9236, acc 0.21875\n",
      "2018-10-22T23:23:47.397174: step 121, loss 1.69649, acc 0.320312\n",
      "2018-10-22T23:23:48.193047: step 122, loss 1.85077, acc 0.273438\n",
      "2018-10-22T23:23:48.912124: step 123, loss 1.77626, acc 0.328125\n",
      "2018-10-22T23:23:49.665111: step 124, loss 1.79707, acc 0.234375\n",
      "2018-10-22T23:23:50.224615: step 125, loss 1.90408, acc 0.21875\n",
      "2018-10-22T23:23:50.735249: step 126, loss 1.81773, acc 0.234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:23:51.273808: step 127, loss 1.81718, acc 0.257812\n",
      "2018-10-22T23:23:51.814363: step 128, loss 1.80625, acc 0.226562\n",
      "2018-10-22T23:23:52.326994: step 129, loss 1.76516, acc 0.273438\n",
      "2018-10-22T23:23:52.843612: step 130, loss 1.90768, acc 0.25\n",
      "2018-10-22T23:23:53.416081: step 131, loss 1.8117, acc 0.273438\n",
      "2018-10-22T23:23:53.986556: step 132, loss 1.81727, acc 0.226562\n",
      "2018-10-22T23:23:54.495197: step 133, loss 1.7414, acc 0.296875\n",
      "2018-10-22T23:23:54.958956: step 134, loss 1.83299, acc 0.257812\n",
      "2018-10-22T23:23:55.430694: step 135, loss 1.82481, acc 0.289062\n",
      "2018-10-22T23:23:55.997179: step 136, loss 1.74293, acc 0.328125\n",
      "2018-10-22T23:23:56.702295: step 137, loss 1.75424, acc 0.304688\n",
      "2018-10-22T23:23:57.261799: step 138, loss 1.75173, acc 0.304688\n",
      "2018-10-22T23:23:57.787393: step 139, loss 1.85309, acc 0.296875\n",
      "2018-10-22T23:23:58.313985: step 140, loss 1.87177, acc 0.273438\n",
      "2018-10-22T23:23:58.827613: step 141, loss 1.85829, acc 0.28125\n",
      "2018-10-22T23:23:59.419030: step 142, loss 1.89709, acc 0.265625\n",
      "2018-10-22T23:24:00.016433: step 143, loss 1.71313, acc 0.3125\n",
      "2018-10-22T23:24:00.633782: step 144, loss 1.82659, acc 0.21875\n",
      "2018-10-22T23:24:01.116491: step 145, loss 1.891, acc 0.21875\n",
      "2018-10-22T23:24:01.629121: step 146, loss 1.76852, acc 0.296875\n",
      "2018-10-22T23:24:02.178652: step 147, loss 1.81048, acc 0.289062\n",
      "2018-10-22T23:24:02.679314: step 148, loss 1.79013, acc 0.25\n",
      "2018-10-22T23:24:03.175986: step 149, loss 1.83846, acc 0.234375\n",
      "2018-10-22T23:24:03.714545: step 150, loss 1.82137, acc 0.234375\n",
      "2018-10-22T23:24:04.180300: step 151, loss 1.76856, acc 0.257812\n",
      "2018-10-22T23:24:04.695921: step 152, loss 1.77456, acc 0.320312\n",
      "2018-10-22T23:24:05.191595: step 153, loss 1.86451, acc 0.28125\n",
      "2018-10-22T23:24:05.714199: step 154, loss 1.81326, acc 0.3125\n",
      "2018-10-22T23:24:06.256895: step 155, loss 1.7766, acc 0.3125\n",
      "2018-10-22T23:24:06.784659: step 156, loss 1.87156, acc 0.234375\n",
      "2018-10-22T23:24:07.266372: step 157, loss 1.78955, acc 0.28125\n",
      "2018-10-22T23:24:07.710185: step 158, loss 1.87117, acc 0.28125\n",
      "2018-10-22T23:24:08.133054: step 159, loss 1.75077, acc 0.320312\n",
      "2018-10-22T23:24:08.565898: step 160, loss 1.81779, acc 0.257812\n",
      "2018-10-22T23:24:09.045616: step 161, loss 1.77871, acc 0.28125\n",
      "2018-10-22T23:24:09.540292: step 162, loss 1.88513, acc 0.195312\n",
      "2018-10-22T23:24:10.019012: step 163, loss 1.78968, acc 0.265625\n",
      "2018-10-22T23:24:10.543609: step 164, loss 1.78066, acc 0.304688\n",
      "2018-10-22T23:24:11.074190: step 165, loss 1.72072, acc 0.320312\n",
      "2018-10-22T23:24:11.578843: step 166, loss 1.83875, acc 0.296875\n",
      "2018-10-22T23:24:12.216138: step 167, loss 1.74809, acc 0.296875\n",
      "2018-10-22T23:24:12.810548: step 168, loss 1.75754, acc 0.304688\n",
      "2018-10-22T23:24:13.447879: step 169, loss 1.77031, acc 0.296875\n",
      "2018-10-22T23:24:14.251702: step 170, loss 1.84188, acc 0.234375\n",
      "2018-10-22T23:24:14.828161: step 171, loss 1.82732, acc 0.234375\n",
      "2018-10-22T23:24:15.351762: step 172, loss 1.80602, acc 0.210938\n",
      "2018-10-22T23:24:15.952156: step 173, loss 1.76239, acc 0.289062\n",
      "2018-10-22T23:24:16.734065: step 174, loss 1.74769, acc 0.328125\n",
      "2018-10-22T23:24:17.430204: step 175, loss 1.79843, acc 0.257812\n",
      "2018-10-22T23:24:18.100804: step 176, loss 1.81205, acc 0.28125\n",
      "2018-10-22T23:24:18.644351: step 177, loss 1.72693, acc 0.34375\n",
      "2018-10-22T23:24:19.106116: step 178, loss 1.86332, acc 0.265625\n",
      "2018-10-22T23:24:19.568880: step 179, loss 1.76975, acc 0.28125\n",
      "2018-10-22T23:24:20.108436: step 180, loss 1.9456, acc 0.234375\n",
      "2018-10-22T23:24:20.733765: step 181, loss 1.7186, acc 0.257812\n",
      "2018-10-22T23:24:21.345131: step 182, loss 1.77023, acc 0.34375\n",
      "2018-10-22T23:24:21.811883: step 183, loss 1.85351, acc 0.21875\n",
      "2018-10-22T23:24:22.530960: step 184, loss 1.79937, acc 0.265625\n",
      "2018-10-22T23:24:23.117888: step 185, loss 1.81538, acc 0.25\n",
      "2018-10-22T23:24:23.712383: step 186, loss 1.86677, acc 0.265625\n",
      "2018-10-22T23:24:24.463377: step 187, loss 1.76339, acc 0.28125\n",
      "2018-10-22T23:24:25.328063: step 188, loss 1.73417, acc 0.289062\n",
      "2018-10-22T23:24:25.879590: step 189, loss 1.80253, acc 0.242188\n",
      "2018-10-22T23:24:26.332378: step 190, loss 1.75662, acc 0.28125\n",
      "2018-10-22T23:24:26.821073: step 191, loss 1.73684, acc 0.304688\n",
      "2018-10-22T23:24:27.272863: step 192, loss 1.76073, acc 0.289062\n",
      "2018-10-22T23:24:27.719668: step 193, loss 1.80412, acc 0.273438\n",
      "2018-10-22T23:24:28.215343: step 194, loss 1.82763, acc 0.25\n",
      "2018-10-22T23:24:28.845658: step 195, loss 1.86734, acc 0.265625\n",
      "2018-10-22T23:24:29.405161: step 196, loss 1.81795, acc 0.28125\n",
      "2018-10-22T23:24:29.860942: step 197, loss 1.8712, acc 0.289062\n",
      "2018-10-22T23:24:30.302761: step 198, loss 1.88555, acc 0.242188\n",
      "2018-10-22T23:24:30.768516: step 199, loss 1.76847, acc 0.257812\n",
      "2018-10-22T23:24:31.257210: step 200, loss 1.82472, acc 0.226562\n",
      "2018-10-22T23:24:31.752884: step 201, loss 1.78778, acc 0.289062\n",
      "2018-10-22T23:24:32.268506: step 202, loss 1.80759, acc 0.265625\n",
      "2018-10-22T23:24:32.947690: step 203, loss 1.81611, acc 0.21875\n",
      "2018-10-22T23:24:33.568031: step 204, loss 1.84382, acc 0.265625\n",
      "2018-10-22T23:24:34.302069: step 205, loss 1.82156, acc 0.242188\n",
      "2018-10-22T23:24:34.952330: step 206, loss 1.79857, acc 0.265625\n",
      "2018-10-22T23:24:35.495876: step 207, loss 1.81005, acc 0.21875\n",
      "2018-10-22T23:24:36.163093: step 208, loss 1.79292, acc 0.25\n",
      "2018-10-22T23:24:36.743541: step 209, loss 1.85072, acc 0.226562\n",
      "2018-10-22T23:24:37.245199: step 210, loss 1.79785, acc 0.289062\n",
      "2018-10-22T23:24:37.746858: step 211, loss 1.78517, acc 0.273438\n",
      "2018-10-22T23:24:38.302373: step 212, loss 1.90871, acc 0.257812\n",
      "2018-10-22T23:24:38.926704: step 213, loss 1.79445, acc 0.28125\n",
      "2018-10-22T23:24:39.582948: step 214, loss 1.89579, acc 0.21875\n",
      "2018-10-22T23:24:40.146442: step 215, loss 1.78132, acc 0.273438\n",
      "2018-10-22T23:24:40.714922: step 216, loss 1.84493, acc 0.242188\n",
      "2018-10-22T23:24:41.382139: step 217, loss 1.83047, acc 0.25\n",
      "2018-10-22T23:24:41.915711: step 218, loss 1.74162, acc 0.3125\n",
      "2018-10-22T23:24:42.429338: step 219, loss 1.80437, acc 0.265625\n",
      "2018-10-22T23:24:42.947952: step 220, loss 1.80193, acc 0.296875\n",
      "2018-10-22T23:24:43.453599: step 221, loss 1.82058, acc 0.226562\n",
      "2018-10-22T23:24:43.982185: step 222, loss 1.85436, acc 0.242188\n",
      "2018-10-22T23:24:44.551663: step 223, loss 1.82147, acc 0.257812\n",
      "2018-10-22T23:24:45.068281: step 224, loss 1.75246, acc 0.28125\n",
      "2018-10-22T23:24:45.519077: step 225, loss 1.76735, acc 0.289062\n",
      "2018-10-22T23:24:45.958901: step 226, loss 1.76115, acc 0.320312\n",
      "2018-10-22T23:24:46.496465: step 227, loss 1.79381, acc 0.242188\n",
      "2018-10-22T23:24:46.961220: step 228, loss 1.79178, acc 0.3125\n",
      "2018-10-22T23:24:47.474847: step 229, loss 1.73453, acc 0.328125\n",
      "2018-10-22T23:24:47.929631: step 230, loss 1.82191, acc 0.257812\n",
      "2018-10-22T23:24:48.386410: step 231, loss 1.84065, acc 0.210938\n",
      "2018-10-22T23:24:48.867124: step 232, loss 1.8096, acc 0.25\n",
      "2018-10-22T23:24:49.377758: step 233, loss 1.72062, acc 0.34375\n",
      "2018-10-22T23:24:49.880415: step 234, loss 1.78642, acc 0.257812\n",
      "2018-10-22T23:24:50.482008: step 235, loss 1.84392, acc 0.25\n",
      "2018-10-22T23:24:51.299821: step 236, loss 1.77608, acc 0.242188\n",
      "2018-10-22T23:24:52.101675: step 237, loss 1.75372, acc 0.351562\n",
      "2018-10-22T23:24:52.889569: step 238, loss 1.7953, acc 0.273438\n",
      "2018-10-22T23:24:53.563767: step 239, loss 1.7262, acc 0.320312\n",
      "2018-10-22T23:24:54.118283: step 240, loss 1.759, acc 0.359375\n",
      "2018-10-22T23:24:54.765561: step 241, loss 1.92619, acc 0.257812\n",
      "2018-10-22T23:24:55.352018: step 242, loss 1.79862, acc 0.335938\n",
      "2018-10-22T23:24:56.036188: step 243, loss 1.90377, acc 0.25\n",
      "2018-10-22T23:24:56.682460: step 244, loss 1.67354, acc 0.367188\n",
      "2018-10-22T23:24:57.352668: step 245, loss 1.74399, acc 0.242188\n",
      "2018-10-22T23:24:58.189430: step 246, loss 1.79458, acc 0.34375\n",
      "2018-10-22T23:24:58.862630: step 247, loss 1.83544, acc 0.242188\n",
      "2018-10-22T23:24:59.501922: step 248, loss 1.75306, acc 0.320312\n",
      "2018-10-22T23:25:00.218007: step 249, loss 1.81598, acc 0.296875\n",
      "2018-10-22T23:25:00.869265: step 250, loss 1.80112, acc 0.257812\n",
      "2018-10-22T23:25:01.514539: step 251, loss 1.88631, acc 0.234375\n",
      "2018-10-22T23:25:02.163804: step 252, loss 1.8268, acc 0.273438\n",
      "2018-10-22T23:25:02.862934: step 253, loss 1.8249, acc 0.257812\n",
      "2018-10-22T23:25:03.548102: step 254, loss 1.82343, acc 0.273438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:25:04.199361: step 255, loss 1.81854, acc 0.296875\n",
      "2018-10-22T23:25:04.740913: step 256, loss 1.77455, acc 0.265625\n",
      "2018-10-22T23:25:05.395164: step 257, loss 1.72352, acc 0.34375\n",
      "2018-10-22T23:25:06.009520: step 258, loss 1.78799, acc 0.21875\n",
      "2018-10-22T23:25:06.568029: step 259, loss 1.77012, acc 0.351562\n",
      "2018-10-22T23:25:07.352928: step 260, loss 1.84803, acc 0.257812\n",
      "2018-10-22T23:25:08.119879: step 261, loss 1.84611, acc 0.21875\n",
      "2018-10-22T23:25:08.709302: step 262, loss 1.79866, acc 0.195312\n",
      "2018-10-22T23:25:09.350587: step 263, loss 1.80253, acc 0.273438\n",
      "2018-10-22T23:25:10.085622: step 264, loss 1.77366, acc 0.25\n",
      "2018-10-22T23:25:10.811681: step 265, loss 1.92035, acc 0.226562\n",
      "2018-10-22T23:25:11.555692: step 266, loss 1.85889, acc 0.25\n",
      "2018-10-22T23:25:12.265794: step 267, loss 1.78362, acc 0.257812\n",
      "2018-10-22T23:25:12.964923: step 268, loss 1.80279, acc 0.289062\n",
      "2018-10-22T23:25:13.638124: step 269, loss 1.80959, acc 0.25\n",
      "2018-10-22T23:25:14.214093: step 270, loss 1.81261, acc 0.25\n",
      "2018-10-22T23:25:14.787464: step 271, loss 1.80396, acc 0.28125\n",
      "2018-10-22T23:25:15.403816: step 272, loss 1.76412, acc 0.28125\n",
      "2018-10-22T23:25:16.055075: step 273, loss 1.78708, acc 0.28125\n",
      "2018-10-22T23:25:16.677411: step 274, loss 1.82068, acc 0.289062\n",
      "2018-10-22T23:25:17.200014: step 275, loss 1.80711, acc 0.273438\n",
      "2018-10-22T23:25:17.737576: step 276, loss 1.7617, acc 0.335938\n",
      "2018-10-22T23:25:18.730920: step 277, loss 1.75406, acc 0.296875\n",
      "2018-10-22T23:25:19.420078: step 278, loss 1.87557, acc 0.226562\n",
      "2018-10-22T23:25:19.987560: step 279, loss 1.81831, acc 0.296875\n",
      "2018-10-22T23:25:20.482239: step 280, loss 1.70536, acc 0.351562\n",
      "2018-10-22T23:25:20.932036: step 281, loss 1.73715, acc 0.296875\n",
      "2018-10-22T23:25:21.430701: step 282, loss 1.78683, acc 0.28125\n",
      "2018-10-22T23:25:21.887610: step 283, loss 1.79604, acc 0.304688\n",
      "2018-10-22T23:25:22.470053: step 284, loss 1.72124, acc 0.3125\n",
      "2018-10-22T23:25:22.934809: step 285, loss 1.79189, acc 0.28125\n",
      "2018-10-22T23:25:23.399567: step 286, loss 1.83204, acc 0.25\n",
      "2018-10-22T23:25:23.867316: step 287, loss 1.72175, acc 0.304688\n",
      "2018-10-22T23:25:24.312125: step 288, loss 1.84234, acc 0.28125\n",
      "2018-10-22T23:25:24.759929: step 289, loss 1.81395, acc 0.289062\n",
      "2018-10-22T23:25:25.211721: step 290, loss 1.88212, acc 0.289062\n",
      "2018-10-22T23:25:25.697422: step 291, loss 1.77403, acc 0.273438\n",
      "2018-10-22T23:25:26.232990: step 292, loss 1.76729, acc 0.25\n",
      "2018-10-22T23:25:26.830393: step 293, loss 1.81449, acc 0.25\n",
      "2018-10-22T23:25:27.333049: step 294, loss 1.71892, acc 0.34375\n",
      "2018-10-22T23:25:27.938430: step 295, loss 1.83195, acc 0.195312\n",
      "2018-10-22T23:25:28.453054: step 296, loss 1.67536, acc 0.375\n",
      "2018-10-22T23:25:28.989619: step 297, loss 1.74868, acc 0.28125\n",
      "2018-10-22T23:25:29.612954: step 298, loss 1.74907, acc 0.296875\n",
      "2018-10-22T23:25:30.213348: step 299, loss 1.73302, acc 0.257812\n",
      "2018-10-22T23:25:30.735950: step 300, loss 1.83096, acc 0.25\n",
      "2018-10-22T23:25:31.300442: step 301, loss 1.83385, acc 0.28125\n",
      "2018-10-22T23:25:31.788137: step 302, loss 1.8529, acc 0.210938\n",
      "2018-10-22T23:25:32.289796: step 303, loss 1.74405, acc 0.351562\n",
      "2018-10-22T23:25:32.831348: step 304, loss 1.84154, acc 0.265625\n",
      "2018-10-22T23:25:33.456676: step 305, loss 1.81936, acc 0.210938\n",
      "2018-10-22T23:25:33.971300: step 306, loss 1.81613, acc 0.25\n",
      "2018-10-22T23:25:34.503875: step 307, loss 1.79412, acc 0.265625\n",
      "2018-10-22T23:25:35.022489: step 308, loss 1.79826, acc 0.234375\n",
      "2018-10-22T23:25:35.518164: step 309, loss 1.78664, acc 0.28125\n",
      "2018-10-22T23:25:36.017828: step 310, loss 1.79683, acc 0.273438\n",
      "2018-10-22T23:25:36.532452: step 311, loss 1.78442, acc 0.273438\n",
      "2018-10-22T23:25:37.120878: step 312, loss 1.70658, acc 0.382812\n",
      "2018-10-22T23:25:37.687364: step 313, loss 1.72611, acc 0.28125\n",
      "2018-10-22T23:25:38.197000: step 314, loss 1.87229, acc 0.3125\n",
      "2018-10-22T23:25:38.713621: step 315, loss 1.81843, acc 0.234375\n",
      "2018-10-22T23:25:39.238218: step 316, loss 1.8394, acc 0.234375\n",
      "2018-10-22T23:25:39.754835: step 317, loss 1.7644, acc 0.265625\n",
      "2018-10-22T23:25:40.289407: step 318, loss 1.76517, acc 0.335938\n",
      "2018-10-22T23:25:40.824975: step 319, loss 1.79862, acc 0.257812\n",
      "2018-10-22T23:25:41.401435: step 320, loss 1.78958, acc 0.25\n",
      "2018-10-22T23:25:41.944980: step 321, loss 1.75354, acc 0.304688\n",
      "2018-10-22T23:25:42.372835: step 322, loss 1.80983, acc 0.265625\n",
      "2018-10-22T23:25:42.887460: step 323, loss 1.88449, acc 0.210938\n",
      "2018-10-22T23:25:43.392111: step 324, loss 1.86313, acc 0.226562\n",
      "2018-10-22T23:25:43.965577: step 325, loss 1.78968, acc 0.242188\n",
      "2018-10-22T23:25:44.412382: step 326, loss 1.7455, acc 0.304688\n",
      "2018-10-22T23:25:44.984852: step 327, loss 1.66788, acc 0.359375\n",
      "2018-10-22T23:25:45.489502: step 328, loss 1.78945, acc 0.296875\n",
      "2018-10-22T23:25:45.934314: step 329, loss 1.81439, acc 0.296875\n",
      "2018-10-22T23:25:46.414030: step 330, loss 1.82357, acc 0.25\n",
      "2018-10-22T23:25:46.977523: step 331, loss 1.85889, acc 0.234375\n",
      "2018-10-22T23:25:47.531044: step 332, loss 1.75111, acc 0.21875\n",
      "2018-10-22T23:25:48.174324: step 333, loss 1.69908, acc 0.34375\n",
      "2018-10-22T23:25:48.715876: step 334, loss 1.86248, acc 0.226562\n",
      "2018-10-22T23:25:49.345193: step 335, loss 1.82597, acc 0.25\n",
      "2018-10-22T23:25:49.877769: step 336, loss 1.85937, acc 0.289062\n",
      "2018-10-22T23:25:50.351503: step 337, loss 1.81369, acc 0.273438\n",
      "2018-10-22T23:25:50.887071: step 338, loss 1.71816, acc 0.28125\n",
      "2018-10-22T23:25:51.423635: step 339, loss 1.7497, acc 0.273438\n",
      "2018-10-22T23:25:51.900361: step 340, loss 1.76505, acc 0.25\n",
      "2018-10-22T23:25:52.369107: step 341, loss 1.78472, acc 0.257812\n",
      "2018-10-22T23:25:52.914649: step 342, loss 1.77053, acc 0.320312\n",
      "2018-10-22T23:25:53.566907: step 343, loss 1.82061, acc 0.234375\n",
      "2018-10-22T23:25:54.155332: step 344, loss 1.78364, acc 0.3125\n",
      "2018-10-22T23:25:54.786644: step 345, loss 1.77877, acc 0.289062\n",
      "2018-10-22T23:25:55.354126: step 346, loss 1.7544, acc 0.234375\n",
      "2018-10-22T23:25:55.946542: step 347, loss 1.76812, acc 0.289062\n",
      "2018-10-22T23:25:56.570873: step 348, loss 1.84721, acc 0.265625\n",
      "2018-10-22T23:25:57.148329: step 349, loss 1.7792, acc 0.25\n",
      "2018-10-22T23:25:57.743737: step 350, loss 1.76616, acc 0.320312\n",
      "2018-10-22T23:25:58.676243: step 351, loss 1.68202, acc 0.375\n",
      "2018-10-22T23:25:59.401306: step 352, loss 1.7832, acc 0.265625\n",
      "2018-10-22T23:26:00.019651: step 353, loss 1.80497, acc 0.28125\n",
      "2018-10-22T23:26:00.635007: step 354, loss 1.89221, acc 0.242188\n",
      "2018-10-22T23:26:01.203486: step 355, loss 1.9257, acc 0.226562\n",
      "2018-10-22T23:26:01.767977: step 356, loss 1.75586, acc 0.304688\n",
      "2018-10-22T23:26:02.432202: step 357, loss 1.76067, acc 0.335938\n",
      "2018-10-22T23:26:03.117370: step 358, loss 1.82464, acc 0.265625\n",
      "2018-10-22T23:26:03.732723: step 359, loss 1.75553, acc 0.3125\n",
      "2018-10-22T23:26:04.235380: step 360, loss 1.81275, acc 0.257812\n",
      "2018-10-22T23:26:04.852729: step 361, loss 1.83881, acc 0.242188\n",
      "2018-10-22T23:26:05.721406: step 362, loss 1.66086, acc 0.375\n",
      "2018-10-22T23:26:06.451734: step 363, loss 1.79621, acc 0.226562\n",
      "2018-10-22T23:26:06.966356: step 364, loss 1.77824, acc 0.304688\n",
      "2018-10-22T23:26:07.582710: step 365, loss 1.84635, acc 0.265625\n",
      "2018-10-22T23:26:08.133317: step 366, loss 1.7815, acc 0.265625\n",
      "2018-10-22T23:26:08.685844: step 367, loss 1.76691, acc 0.28125\n",
      "2018-10-22T23:26:09.251333: step 368, loss 1.86537, acc 0.25\n",
      "2018-10-22T23:26:09.804852: step 369, loss 1.88861, acc 0.1875\n",
      "2018-10-22T23:26:10.297533: step 370, loss 1.83817, acc 0.234375\n",
      "2018-10-22T23:26:10.772264: step 371, loss 1.72699, acc 0.265625\n",
      "2018-10-22T23:26:11.255091: step 372, loss 1.76588, acc 0.28125\n",
      "2018-10-22T23:26:11.770712: step 373, loss 1.72868, acc 0.351562\n",
      "2018-10-22T23:26:12.307278: step 374, loss 1.77881, acc 0.289062\n",
      "2018-10-22T23:26:12.876758: step 375, loss 1.76151, acc 0.28125\n",
      "2018-10-22T23:26:13.333534: step 376, loss 1.77619, acc 0.273438\n",
      "2018-10-22T23:26:13.821230: step 377, loss 1.77589, acc 0.296875\n",
      "2018-10-22T23:26:14.302943: step 378, loss 1.8327, acc 0.25\n",
      "2018-10-22T23:26:14.788399: step 379, loss 1.82432, acc 0.3125\n",
      "2018-10-22T23:26:15.236201: step 380, loss 1.84071, acc 0.25\n",
      "2018-10-22T23:26:15.738857: step 381, loss 1.77482, acc 0.3125\n",
      "2018-10-22T23:26:16.251487: step 382, loss 1.81148, acc 0.289062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:26:16.732202: step 383, loss 1.65341, acc 0.382812\n",
      "2018-10-22T23:26:17.249826: step 384, loss 1.76173, acc 0.335938\n",
      "2018-10-22T23:26:17.704347: step 385, loss 1.79207, acc 0.296875\n",
      "2018-10-22T23:26:18.148160: step 386, loss 1.80553, acc 0.257812\n",
      "2018-10-22T23:26:18.859259: step 387, loss 1.76497, acc 0.328125\n",
      "2018-10-22T23:26:19.466634: step 388, loss 1.73877, acc 0.304688\n",
      "2018-10-22T23:26:20.270484: step 389, loss 1.81126, acc 0.234375\n",
      "2018-10-22T23:26:21.151129: step 390, loss 1.82211, acc 0.273438\n",
      "2018-10-22T23:26:21.845273: step 391, loss 1.79659, acc 0.28125\n",
      "2018-10-22T23:26:22.468607: step 392, loss 1.73224, acc 0.351562\n",
      "2018-10-22T23:26:23.206632: step 393, loss 1.83734, acc 0.242188\n",
      "2018-10-22T23:26:23.823982: step 394, loss 1.70305, acc 0.328125\n",
      "2018-10-22T23:26:24.372515: step 395, loss 1.90916, acc 0.242188\n",
      "2018-10-22T23:26:24.892125: step 396, loss 1.73888, acc 0.320312\n",
      "2018-10-22T23:26:25.408745: step 397, loss 1.80357, acc 0.289062\n",
      "2018-10-22T23:26:25.953289: step 398, loss 1.77803, acc 0.28125\n",
      "2018-10-22T23:26:26.600558: step 399, loss 1.88992, acc 0.242188\n",
      "2018-10-22T23:26:27.252814: step 400, loss 1.7907, acc 0.320312\n",
      "2018-10-22T23:26:27.760456: step 401, loss 1.73726, acc 0.3125\n",
      "2018-10-22T23:26:28.432658: step 402, loss 1.84642, acc 0.273438\n",
      "2018-10-22T23:26:28.985182: step 403, loss 1.74276, acc 0.289062\n",
      "2018-10-22T23:26:29.435977: step 404, loss 1.70709, acc 0.34375\n",
      "2018-10-22T23:26:29.965560: step 405, loss 1.77127, acc 0.34375\n",
      "2018-10-22T23:26:30.487166: step 406, loss 1.81879, acc 0.273438\n",
      "2018-10-22T23:26:30.967882: step 407, loss 1.78406, acc 0.351562\n",
      "2018-10-22T23:26:31.542344: step 408, loss 1.84364, acc 0.265625\n",
      "2018-10-22T23:26:32.078911: step 409, loss 1.7019, acc 0.304688\n",
      "2018-10-22T23:26:32.604504: step 410, loss 1.90216, acc 0.257812\n",
      "2018-10-22T23:26:33.107160: step 411, loss 1.75186, acc 0.304688\n",
      "2018-10-22T23:26:33.625773: step 412, loss 1.73404, acc 0.34375\n",
      "2018-10-22T23:26:34.283016: step 413, loss 1.74966, acc 0.265625\n",
      "2018-10-22T23:26:34.787666: step 414, loss 1.79833, acc 0.28125\n",
      "2018-10-22T23:26:35.335203: step 415, loss 1.79287, acc 0.273438\n",
      "2018-10-22T23:26:36.066248: step 416, loss 1.68683, acc 0.34375\n",
      "2018-10-22T23:26:36.772360: step 417, loss 1.77344, acc 0.335938\n",
      "2018-10-22T23:26:37.303563: step 418, loss 1.74064, acc 0.257812\n",
      "2018-10-22T23:26:37.931883: step 419, loss 1.81539, acc 0.242188\n",
      "2018-10-22T23:26:38.569180: step 420, loss 1.73947, acc 0.3125\n",
      "2018-10-22T23:26:39.055877: step 421, loss 1.82514, acc 0.273438\n",
      "2018-10-22T23:26:39.551552: step 422, loss 1.77582, acc 0.289062\n",
      "2018-10-22T23:26:40.071163: step 423, loss 1.83369, acc 0.265625\n",
      "2018-10-22T23:26:40.551878: step 424, loss 1.75539, acc 0.3125\n",
      "2018-10-22T23:26:41.118363: step 425, loss 1.7852, acc 0.335938\n",
      "2018-10-22T23:26:41.689835: step 426, loss 1.80745, acc 0.265625\n",
      "2018-10-22T23:26:42.149605: step 427, loss 1.77582, acc 0.28125\n",
      "2018-10-22T23:26:42.699136: step 428, loss 1.87265, acc 0.25\n",
      "2018-10-22T23:26:43.189823: step 429, loss 1.82234, acc 0.25\n",
      "2018-10-22T23:26:43.665551: step 430, loss 1.74015, acc 0.296875\n",
      "2018-10-22T23:26:44.167212: step 431, loss 1.78835, acc 0.265625\n",
      "2018-10-22T23:26:44.724720: step 432, loss 1.83242, acc 0.210938\n",
      "2018-10-22T23:26:45.312149: step 433, loss 1.69677, acc 0.359375\n",
      "2018-10-22T23:26:45.891600: step 434, loss 1.82581, acc 0.320312\n",
      "2018-10-22T23:26:46.388272: step 435, loss 1.77706, acc 0.257812\n",
      "2018-10-22T23:26:46.879958: step 436, loss 1.80925, acc 0.273438\n",
      "2018-10-22T23:26:47.516255: step 437, loss 1.80725, acc 0.320312\n",
      "2018-10-22T23:26:48.035867: step 438, loss 1.78363, acc 0.25\n",
      "2018-10-22T23:26:48.531541: step 439, loss 1.84634, acc 0.265625\n",
      "2018-10-22T23:26:48.987322: step 440, loss 1.76113, acc 0.3125\n",
      "2018-10-22T23:26:49.434129: step 441, loss 1.86014, acc 0.226562\n",
      "2018-10-22T23:26:50.012581: step 442, loss 1.74947, acc 0.351562\n",
      "2018-10-22T23:26:50.590038: step 443, loss 1.79435, acc 0.226562\n",
      "2018-10-22T23:26:51.099674: step 444, loss 1.79706, acc 0.289062\n",
      "2018-10-22T23:26:51.812768: step 445, loss 1.77831, acc 0.289062\n",
      "2018-10-22T23:26:52.452059: step 446, loss 1.84287, acc 0.234375\n",
      "2018-10-22T23:26:53.062427: step 447, loss 1.7662, acc 0.289062\n",
      "2018-10-22T23:26:53.691743: step 448, loss 1.84214, acc 0.242188\n",
      "2018-10-22T23:26:54.300116: step 449, loss 1.77218, acc 0.328125\n",
      "2018-10-22T23:26:54.826709: step 450, loss 1.75572, acc 0.320312\n",
      "2018-10-22T23:26:55.526837: step 451, loss 1.86454, acc 0.234375\n",
      "2018-10-22T23:26:56.155515: step 452, loss 1.82738, acc 0.25\n",
      "2018-10-22T23:26:56.766465: step 453, loss 1.80627, acc 0.328125\n",
      "2018-10-22T23:26:57.259148: step 454, loss 1.82209, acc 0.242188\n",
      "2018-10-22T23:26:57.884475: step 455, loss 1.81695, acc 0.304688\n",
      "2018-10-22T23:26:58.419047: step 456, loss 1.79854, acc 0.359375\n",
      "2018-10-22T23:26:58.920705: step 457, loss 1.80841, acc 0.273438\n",
      "2018-10-22T23:26:59.692091: step 458, loss 1.90897, acc 0.257812\n",
      "2018-10-22T23:27:00.350332: step 459, loss 1.81563, acc 0.289062\n",
      "2018-10-22T23:27:01.042480: step 460, loss 1.73629, acc 0.265625\n",
      "2018-10-22T23:27:01.641883: step 461, loss 1.86234, acc 0.28125\n",
      "2018-10-22T23:27:02.216999: step 462, loss 1.73124, acc 0.328125\n",
      "2018-10-22T23:27:02.750573: step 463, loss 1.7946, acc 0.234375\n",
      "2018-10-22T23:27:03.343654: step 464, loss 1.72525, acc 0.3125\n",
      "2018-10-22T23:27:03.938586: step 465, loss 1.85255, acc 0.234375\n",
      "2018-10-22T23:27:04.388918: step 466, loss 1.73737, acc 0.328125\n",
      "2018-10-22T23:27:04.832732: step 467, loss 1.72244, acc 0.34375\n",
      "2018-10-22T23:27:05.283880: step 468, loss 1.7869, acc 0.34375\n",
      "2018-10-22T23:27:05.800171: step 469, loss 1.8109, acc 0.296875\n",
      "2018-10-22T23:27:06.299835: step 470, loss 1.73024, acc 0.34375\n",
      "2018-10-22T23:27:06.783542: step 471, loss 1.77378, acc 0.296875\n",
      "2018-10-22T23:27:07.317115: step 472, loss 1.80856, acc 0.28125\n",
      "2018-10-22T23:27:07.817776: step 473, loss 1.81977, acc 0.28125\n",
      "2018-10-22T23:27:08.303923: step 474, loss 1.76481, acc 0.351562\n",
      "2018-10-22T23:27:08.772836: step 475, loss 1.75336, acc 0.375\n",
      "2018-10-22T23:27:09.224628: step 476, loss 1.83042, acc 0.25\n",
      "2018-10-22T23:27:09.750222: step 477, loss 1.75717, acc 0.28125\n",
      "2018-10-22T23:27:10.235925: step 478, loss 1.75872, acc 0.328125\n",
      "2018-10-22T23:27:10.737583: step 479, loss 1.68734, acc 0.351562\n",
      "2018-10-22T23:27:11.265172: step 480, loss 1.78505, acc 0.265625\n",
      "2018-10-22T23:27:11.759157: step 481, loss 1.77408, acc 0.304688\n",
      "2018-10-22T23:27:12.250842: step 482, loss 1.82704, acc 0.304688\n",
      "2018-10-22T23:27:12.679695: step 483, loss 1.74237, acc 0.265625\n",
      "2018-10-22T23:27:13.188336: step 484, loss 1.77056, acc 0.328125\n",
      "2018-10-22T23:27:13.669049: step 485, loss 1.77142, acc 0.296875\n",
      "2018-10-22T23:27:14.148767: step 486, loss 1.93935, acc 0.210938\n",
      "2018-10-22T23:27:14.577371: step 487, loss 1.79473, acc 0.3125\n",
      "2018-10-22T23:27:15.038140: step 488, loss 1.86236, acc 0.257812\n",
      "2018-10-22T23:27:15.475970: step 489, loss 1.78695, acc 0.304688\n",
      "2018-10-22T23:27:15.937734: step 490, loss 1.80494, acc 0.304688\n",
      "2018-10-22T23:27:16.461334: step 491, loss 1.81532, acc 0.242188\n",
      "2018-10-22T23:27:16.946038: step 492, loss 1.84769, acc 0.25\n",
      "2018-10-22T23:27:17.410795: step 493, loss 1.7764, acc 0.273438\n",
      "2018-10-22T23:27:17.868571: step 494, loss 1.8054, acc 0.304688\n",
      "2018-10-22T23:27:18.335644: step 495, loss 1.77571, acc 0.296875\n",
      "2018-10-22T23:27:18.821706: step 496, loss 1.87141, acc 0.304688\n",
      "2018-10-22T23:27:19.277486: step 497, loss 1.75035, acc 0.320312\n",
      "2018-10-22T23:27:19.805078: step 498, loss 1.8625, acc 0.265625\n",
      "2018-10-22T23:27:20.463876: step 499, loss 1.81156, acc 0.320312\n",
      "2018-10-22T23:27:21.055295: step 500, loss 1.74567, acc 0.257812\n",
      "\n",
      "Evaluation:\n",
      "Number of batches in dev set is 62\n",
      "batch 1 in dev >> 2018-10-22T23:27:22.966185: loss 1.80572, acc 0.298\n",
      "batch 2 in dev >> 2018-10-22T23:27:24.653674: loss 1.79829, acc 0.304\n",
      "batch 3 in dev >> 2018-10-22T23:27:26.540629: loss 1.79435, acc 0.266\n",
      "batch 4 in dev >> 2018-10-22T23:27:28.347796: loss 1.76125, acc 0.342\n",
      "batch 5 in dev >> 2018-10-22T23:27:29.873090: loss 1.74656, acc 0.318\n",
      "batch 6 in dev >> 2018-10-22T23:27:31.923226: loss 1.77155, acc 0.294\n",
      "batch 7 in dev >> 2018-10-22T23:27:33.531925: loss 1.77604, acc 0.332\n",
      "batch 8 in dev >> 2018-10-22T23:27:35.599888: loss 1.73398, acc 0.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 9 in dev >> 2018-10-22T23:27:37.155729: loss 1.77652, acc 0.268\n",
      "batch 10 in dev >> 2018-10-22T23:27:38.689626: loss 1.81338, acc 0.274\n",
      "batch 11 in dev >> 2018-10-22T23:27:40.251450: loss 1.76451, acc 0.316\n",
      "batch 12 in dev >> 2018-10-22T23:27:42.244123: loss 1.75273, acc 0.32\n",
      "batch 13 in dev >> 2018-10-22T23:27:43.966516: loss 1.76606, acc 0.286\n",
      "batch 14 in dev >> 2018-10-22T23:27:45.684922: loss 1.76849, acc 0.328\n",
      "batch 15 in dev >> 2018-10-22T23:27:47.518020: loss 1.7629, acc 0.316\n",
      "batch 16 in dev >> 2018-10-22T23:27:49.341147: loss 1.75598, acc 0.292\n",
      "batch 17 in dev >> 2018-10-22T23:27:51.113408: loss 1.77303, acc 0.31\n",
      "batch 18 in dev >> 2018-10-22T23:27:52.846773: loss 1.80173, acc 0.3\n",
      "batch 19 in dev >> 2018-10-22T23:27:54.487386: loss 1.80619, acc 0.29\n",
      "batch 20 in dev >> 2018-10-22T23:27:56.068159: loss 1.76893, acc 0.304\n",
      "batch 21 in dev >> 2018-10-22T23:27:57.729717: loss 1.75142, acc 0.316\n",
      "batch 22 in dev >> 2018-10-22T23:27:59.455104: loss 1.73336, acc 0.346\n",
      "batch 23 in dev >> 2018-10-22T23:28:01.497642: loss 1.78804, acc 0.29\n",
      "batch 24 in dev >> 2018-10-22T23:28:03.355675: loss 1.72455, acc 0.366\n",
      "batch 25 in dev >> 2018-10-22T23:28:04.983322: loss 1.77799, acc 0.306\n",
      "batch 26 in dev >> 2018-10-22T23:28:06.626929: loss 1.74938, acc 0.314\n",
      "batch 27 in dev >> 2018-10-22T23:28:08.196730: loss 1.77515, acc 0.32\n",
      "batch 28 in dev >> 2018-10-22T23:28:09.855296: loss 1.79532, acc 0.278\n",
      "batch 29 in dev >> 2018-10-22T23:28:11.452026: loss 1.81448, acc 0.308\n",
      "batch 30 in dev >> 2018-10-22T23:28:13.119567: loss 1.75571, acc 0.304\n",
      "batch 31 in dev >> 2018-10-22T23:28:14.647483: loss 1.76584, acc 0.312\n",
      "batch 32 in dev >> 2018-10-22T23:28:16.256181: loss 1.76214, acc 0.316\n",
      "batch 33 in dev >> 2018-10-22T23:28:17.955638: loss 1.81418, acc 0.298\n",
      "batch 34 in dev >> 2018-10-22T23:28:19.652101: loss 1.77428, acc 0.282\n",
      "batch 35 in dev >> 2018-10-22T23:28:21.468751: loss 1.79075, acc 0.29\n",
      "batch 36 in dev >> 2018-10-22T23:28:23.787753: loss 1.79575, acc 0.338\n",
      "batch 37 in dev >> 2018-10-22T23:28:25.713735: loss 1.77274, acc 0.304\n",
      "batch 38 in dev >> 2018-10-22T23:28:28.125288: loss 1.71969, acc 0.342\n",
      "batch 39 in dev >> 2018-10-22T23:28:30.877928: loss 1.78713, acc 0.292\n",
      "batch 40 in dev >> 2018-10-22T23:28:33.117938: loss 1.76717, acc 0.314\n",
      "batch 41 in dev >> 2018-10-22T23:28:34.775508: loss 1.76959, acc 0.318\n",
      "batch 42 in dev >> 2018-10-22T23:28:36.299431: loss 1.80377, acc 0.32\n",
      "batch 43 in dev >> 2018-10-22T23:28:38.083660: loss 1.79356, acc 0.266\n",
      "batch 44 in dev >> 2018-10-22T23:28:39.662440: loss 1.80532, acc 0.312\n",
      "batch 45 in dev >> 2018-10-22T23:28:41.260167: loss 1.77667, acc 0.298\n",
      "batch 46 in dev >> 2018-10-22T23:28:42.818002: loss 1.78788, acc 0.298\n",
      "batch 47 in dev >> 2018-10-22T23:28:44.596249: loss 1.75013, acc 0.31\n",
      "batch 48 in dev >> 2018-10-22T23:28:46.293709: loss 1.80501, acc 0.282\n",
      "batch 49 in dev >> 2018-10-22T23:28:47.849548: loss 1.79812, acc 0.302\n",
      "batch 50 in dev >> 2018-10-22T23:28:49.528061: loss 1.79402, acc 0.296\n",
      "batch 51 in dev >> 2018-10-22T23:28:51.022066: loss 1.8115, acc 0.272\n",
      "batch 52 in dev >> 2018-10-22T23:28:52.876109: loss 1.75687, acc 0.276\n",
      "batch 53 in dev >> 2018-10-22T23:28:54.495778: loss 1.71528, acc 0.356\n",
      "batch 54 in dev >> 2018-10-22T23:28:56.308930: loss 1.76115, acc 0.29\n",
      "batch 55 in dev >> 2018-10-22T23:28:58.332520: loss 1.80505, acc 0.264\n",
      "batch 56 in dev >> 2018-10-22T23:29:00.085832: loss 1.77075, acc 0.284\n",
      "batch 57 in dev >> 2018-10-22T23:29:01.929901: loss 1.82312, acc 0.27\n",
      "batch 58 in dev >> 2018-10-22T23:29:03.618386: loss 1.74564, acc 0.332\n",
      "batch 59 in dev >> 2018-10-22T23:29:05.235063: loss 1.75507, acc 0.324\n",
      "batch 60 in dev >> 2018-10-22T23:29:06.949479: loss 1.77318, acc 0.306\n",
      "batch 61 in dev >> 2018-10-22T23:29:08.831448: loss 1.77931, acc 0.33\n",
      "batch 62 in dev >> 2018-10-22T23:29:10.475053: loss 1.82216, acc 0.298\n",
      "\n",
      "Mean accuracy=0.30529032359200137\n",
      "Mean loss=1.776071471552695\n",
      "\n",
      "2018-10-22T23:29:10.977709: step 501, loss 1.84382, acc 0.234375\n",
      "2018-10-22T23:29:11.406562: step 502, loss 1.70776, acc 0.375\n",
      "2018-10-22T23:29:11.874311: step 503, loss 1.68787, acc 0.367188\n",
      "2018-10-22T23:29:12.313138: step 504, loss 1.83386, acc 0.234375\n",
      "2018-10-22T23:29:12.816792: step 505, loss 1.77254, acc 0.25\n",
      "2018-10-22T23:29:13.271575: step 506, loss 1.81906, acc 0.289062\n",
      "2018-10-22T23:29:13.780215: step 507, loss 1.74114, acc 0.320312\n",
      "2018-10-22T23:29:14.245970: step 508, loss 1.86321, acc 0.242188\n",
      "2018-10-22T23:29:14.746631: step 509, loss 1.75898, acc 0.273438\n",
      "2018-10-22T23:29:15.194435: step 510, loss 1.73436, acc 0.34375\n",
      "2018-10-22T23:29:15.742968: step 511, loss 1.77975, acc 0.273438\n",
      "2018-10-22T23:29:16.478003: step 512, loss 1.80246, acc 0.234375\n",
      "2018-10-22T23:29:16.976702: step 513, loss 1.78058, acc 0.320312\n",
      "2018-10-22T23:29:17.447443: step 514, loss 1.8409, acc 0.234375\n",
      "2018-10-22T23:29:17.947108: step 515, loss 1.8903, acc 0.242188\n",
      "2018-10-22T23:29:18.382942: step 516, loss 1.80061, acc 0.289062\n",
      "2018-10-22T23:29:18.823762: step 517, loss 1.75523, acc 0.328125\n",
      "2018-10-22T23:29:19.244638: step 518, loss 1.71055, acc 0.328125\n",
      "2018-10-22T23:29:19.769235: step 519, loss 1.73938, acc 0.273438\n",
      "2018-10-22T23:29:20.231000: step 520, loss 1.78148, acc 0.328125\n",
      "2018-10-22T23:29:20.658856: step 521, loss 1.66945, acc 0.390625\n",
      "2018-10-22T23:29:21.103667: step 522, loss 1.76382, acc 0.273438\n",
      "2018-10-22T23:29:21.599342: step 523, loss 1.75923, acc 0.335938\n",
      "2018-10-22T23:29:22.117955: step 524, loss 1.8591, acc 0.296875\n",
      "2018-10-22T23:29:22.698404: step 525, loss 1.80262, acc 0.257812\n",
      "2018-10-22T23:29:23.197070: step 526, loss 1.838, acc 0.25\n",
      "2018-10-22T23:29:23.707704: step 527, loss 1.84447, acc 0.3125\n",
      "2018-10-22T23:29:24.162488: step 528, loss 1.77763, acc 0.296875\n",
      "2018-10-22T23:29:24.645198: step 529, loss 1.75987, acc 0.289062\n",
      "2018-10-22T23:29:25.152841: step 530, loss 1.79419, acc 0.28125\n",
      "2018-10-22T23:29:25.658488: step 531, loss 1.77125, acc 0.296875\n",
      "2018-10-22T23:29:26.199043: step 532, loss 1.75708, acc 0.296875\n",
      "2018-10-22T23:29:26.682750: step 533, loss 1.80112, acc 0.242188\n",
      "2018-10-22T23:29:27.227294: step 534, loss 1.83728, acc 0.234375\n",
      "2018-10-22T23:29:27.693050: step 535, loss 1.82089, acc 0.28125\n",
      "2018-10-22T23:29:28.134867: step 536, loss 1.77216, acc 0.320312\n",
      "2018-10-22T23:29:28.558733: step 537, loss 1.78458, acc 0.289062\n",
      "2018-10-22T23:29:28.997560: step 538, loss 1.8369, acc 0.28125\n",
      "2018-10-22T23:29:29.445364: step 539, loss 1.71246, acc 0.335938\n",
      "2018-10-22T23:29:29.894163: step 540, loss 1.75266, acc 0.3125\n",
      "2018-10-22T23:29:30.384850: step 541, loss 1.82633, acc 0.289062\n",
      "2018-10-22T23:29:30.836643: step 542, loss 1.75801, acc 0.320312\n",
      "2018-10-22T23:29:31.250537: step 543, loss 1.82503, acc 0.265625\n",
      "2018-10-22T23:29:31.696345: step 544, loss 1.84013, acc 0.21875\n",
      "2018-10-22T23:29:32.142152: step 545, loss 1.75256, acc 0.34375\n",
      "2018-10-22T23:29:32.587961: step 546, loss 1.76168, acc 0.320312\n",
      "2018-10-22T23:29:32.997864: step 547, loss 1.82298, acc 0.328125\n",
      "2018-10-22T23:29:33.464617: step 548, loss 1.76614, acc 0.296875\n",
      "2018-10-22T23:29:33.955304: step 549, loss 1.74911, acc 0.351562\n",
      "2018-10-22T23:29:34.417069: step 550, loss 1.67366, acc 0.414062\n",
      "2018-10-22T23:29:34.894793: step 551, loss 1.7558, acc 0.265625\n",
      "2018-10-22T23:29:35.313672: step 552, loss 1.87599, acc 0.203125\n",
      "2018-10-22T23:29:35.740531: step 553, loss 1.81373, acc 0.296875\n",
      "2018-10-22T23:29:36.209658: step 554, loss 1.81555, acc 0.320312\n",
      "2018-10-22T23:29:36.673201: step 555, loss 1.83876, acc 0.3125\n",
      "2018-10-22T23:29:37.177852: step 556, loss 1.76737, acc 0.289062\n",
      "2018-10-22T23:29:37.632635: step 557, loss 1.76015, acc 0.320312\n",
      "2018-10-22T23:29:38.090412: step 558, loss 1.74094, acc 0.289062\n",
      "2018-10-22T23:29:38.626977: step 559, loss 1.74445, acc 0.335938\n",
      "2018-10-22T23:29:39.029900: step 560, loss 1.77268, acc 0.28125\n",
      "2018-10-22T23:29:39.484683: step 561, loss 1.77493, acc 0.320312\n",
      "2018-10-22T23:29:39.921516: step 562, loss 1.81257, acc 0.257812\n",
      "2018-10-22T23:29:40.332416: step 563, loss 1.83524, acc 0.265625\n",
      "2018-10-22T23:29:40.877959: step 564, loss 1.87538, acc 0.265625\n",
      "2018-10-22T23:29:41.441452: step 565, loss 1.81635, acc 0.265625\n",
      "2018-10-22T23:29:41.977486: step 566, loss 1.79809, acc 0.273438\n",
      "2018-10-22T23:29:42.420573: step 567, loss 1.74852, acc 0.351562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:29:42.961127: step 568, loss 1.78579, acc 0.273438\n",
      "2018-10-22T23:29:43.548556: step 569, loss 1.76035, acc 0.289062\n",
      "2018-10-22T23:29:44.270626: step 570, loss 1.80826, acc 0.320312\n",
      "2018-10-22T23:29:44.945820: step 571, loss 1.8674, acc 0.203125\n",
      "2018-10-22T23:29:45.744685: step 572, loss 1.7575, acc 0.257812\n",
      "2018-10-22T23:29:46.261099: step 573, loss 1.75382, acc 0.273438\n",
      "2018-10-22T23:29:46.739581: step 574, loss 1.72061, acc 0.351562\n",
      "2018-10-22T23:29:47.210323: step 575, loss 1.75986, acc 0.296875\n",
      "2018-10-22T23:29:47.674082: step 576, loss 1.78133, acc 0.320312\n",
      "2018-10-22T23:29:48.184717: step 577, loss 1.84782, acc 0.304688\n",
      "2018-10-22T23:29:48.596618: step 578, loss 1.74287, acc 0.320312\n",
      "2018-10-22T23:29:49.029458: step 579, loss 1.78956, acc 0.242188\n",
      "2018-10-22T23:29:49.600930: step 580, loss 1.72961, acc 0.28125\n",
      "2018-10-22T23:29:50.144479: step 581, loss 1.83578, acc 0.265625\n",
      "2018-10-22T23:29:50.626189: step 582, loss 1.82692, acc 0.234375\n",
      "2018-10-22T23:29:51.125853: step 583, loss 1.74241, acc 0.289062\n",
      "2018-10-22T23:29:51.599588: step 584, loss 1.82035, acc 0.289062\n",
      "2018-10-22T23:29:52.159091: step 585, loss 1.79191, acc 0.351562\n",
      "2018-10-22T23:29:52.680695: step 586, loss 1.86876, acc 0.242188\n",
      "2018-10-22T23:29:53.156423: step 587, loss 1.82272, acc 0.25\n",
      "2018-10-22T23:29:53.662072: step 588, loss 1.86159, acc 0.28125\n",
      "2018-10-22T23:29:54.125831: step 589, loss 1.68606, acc 0.382812\n",
      "2018-10-22T23:29:54.635469: step 590, loss 1.77169, acc 0.328125\n",
      "2018-10-22T23:29:55.115186: step 591, loss 1.80327, acc 0.265625\n",
      "2018-10-22T23:29:55.578947: step 592, loss 1.8308, acc 0.210938\n",
      "2018-10-22T23:29:56.351881: step 593, loss 1.79141, acc 0.265625\n",
      "2018-10-22T23:29:56.995667: step 594, loss 1.78177, acc 0.351562\n",
      "2018-10-22T23:29:57.509269: step 595, loss 1.71046, acc 0.351562\n",
      "2018-10-22T23:29:58.133601: step 596, loss 1.70625, acc 0.3125\n",
      "2018-10-22T23:29:58.696096: step 597, loss 1.88618, acc 0.25\n",
      "2018-10-22T23:29:59.259589: step 598, loss 1.78867, acc 0.234375\n",
      "2018-10-22T23:29:59.761247: step 599, loss 1.72246, acc 0.367188\n",
      "2018-10-22T23:30:00.288837: step 600, loss 1.81772, acc 0.335938\n",
      "2018-10-22T23:30:00.828394: step 601, loss 1.78053, acc 0.289062\n",
      "2018-10-22T23:30:01.293151: step 602, loss 1.8078, acc 0.273438\n",
      "2018-10-22T23:30:01.881579: step 603, loss 1.76298, acc 0.289062\n",
      "2018-10-22T23:30:02.590683: step 604, loss 1.79047, acc 0.3125\n",
      "2018-10-22T23:30:03.232966: step 605, loss 1.74901, acc 0.328125\n",
      "2018-10-22T23:30:03.844331: step 606, loss 1.74304, acc 0.328125\n",
      "2018-10-22T23:30:04.303103: step 607, loss 1.69783, acc 0.375\n",
      "2018-10-22T23:30:04.684085: step 608, loss 1.73954, acc 0.359375\n",
      "2018-10-22T23:30:05.099973: step 609, loss 1.73018, acc 0.375\n",
      "2018-10-22T23:30:05.589663: step 610, loss 1.84472, acc 0.25\n",
      "2018-10-22T23:30:06.033477: step 611, loss 1.75033, acc 0.328125\n",
      "2018-10-22T23:30:06.519178: step 612, loss 1.81623, acc 0.273438\n",
      "2018-10-22T23:30:06.973961: step 613, loss 1.76473, acc 0.3125\n",
      "2018-10-22T23:30:07.409797: step 614, loss 1.82223, acc 0.273438\n",
      "2018-10-22T23:30:07.868570: step 615, loss 1.78282, acc 0.296875\n",
      "2018-10-22T23:30:08.352277: step 616, loss 1.75172, acc 0.335938\n",
      "2018-10-22T23:30:08.843962: step 617, loss 1.77652, acc 0.273438\n",
      "2018-10-22T23:30:09.368560: step 618, loss 1.77452, acc 0.289062\n",
      "2018-10-22T23:30:09.836309: step 619, loss 1.77627, acc 0.3125\n",
      "2018-10-22T23:30:10.284112: step 620, loss 1.78512, acc 0.304688\n",
      "2018-10-22T23:30:10.800730: step 621, loss 1.82271, acc 0.210938\n",
      "2018-10-22T23:30:11.296406: step 622, loss 1.86966, acc 0.242188\n",
      "2018-10-22T23:30:11.726256: step 623, loss 1.74874, acc 0.351562\n",
      "2018-10-22T23:30:12.195055: step 624, loss 1.71911, acc 0.304688\n",
      "2018-10-22T23:30:12.702008: step 625, loss 1.78362, acc 0.273438\n",
      "2018-10-22T23:30:13.165768: step 626, loss 1.75548, acc 0.28125\n",
      "2018-10-22T23:30:13.617561: step 627, loss 1.86295, acc 0.257812\n",
      "2018-10-22T23:30:14.084313: step 628, loss 1.77137, acc 0.28125\n",
      "2018-10-22T23:30:14.587966: step 629, loss 1.89629, acc 0.25\n",
      "2018-10-22T23:30:15.069677: step 630, loss 1.81195, acc 0.28125\n",
      "2018-10-22T23:30:15.551390: step 631, loss 1.7783, acc 0.289062\n",
      "2018-10-22T23:30:16.102916: step 632, loss 1.7122, acc 0.335938\n",
      "2018-10-22T23:30:16.661422: step 633, loss 1.81502, acc 0.265625\n",
      "2018-10-22T23:30:17.126179: step 634, loss 1.7488, acc 0.320312\n",
      "2018-10-22T23:30:17.585950: step 635, loss 1.86192, acc 0.226562\n",
      "2018-10-22T23:30:18.040734: step 636, loss 1.78861, acc 0.28125\n",
      "2018-10-22T23:30:18.451636: step 637, loss 1.76489, acc 0.304688\n",
      "2018-10-22T23:30:18.899439: step 638, loss 1.70332, acc 0.34375\n",
      "2018-10-22T23:30:19.400100: step 639, loss 1.73998, acc 0.367188\n",
      "2018-10-22T23:30:20.118180: step 640, loss 1.73099, acc 0.3125\n",
      "2018-10-22T23:30:20.834265: step 641, loss 1.84902, acc 0.242188\n",
      "2018-10-22T23:30:21.361854: step 642, loss 1.77406, acc 0.257812\n",
      "2018-10-22T23:30:21.927341: step 643, loss 1.827, acc 0.289062\n",
      "2018-10-22T23:30:22.336248: step 644, loss 1.74079, acc 0.328125\n",
      "2018-10-22T23:30:22.794026: step 645, loss 1.65128, acc 0.414062\n",
      "2018-10-22T23:30:23.290696: step 646, loss 1.69054, acc 0.359375\n",
      "2018-10-22T23:30:23.718553: step 647, loss 1.75696, acc 0.320312\n",
      "2018-10-22T23:30:24.127459: step 648, loss 1.68229, acc 0.328125\n",
      "2018-10-22T23:30:24.587230: step 649, loss 1.77954, acc 0.28125\n",
      "2018-10-22T23:30:25.020073: step 650, loss 1.75725, acc 0.28125\n",
      "2018-10-22T23:30:25.450920: step 651, loss 1.83511, acc 0.328125\n",
      "2018-10-22T23:30:25.949587: step 652, loss 1.76666, acc 0.289062\n",
      "2018-10-22T23:30:26.550979: step 653, loss 1.70996, acc 0.289062\n",
      "2018-10-22T23:30:27.010749: step 654, loss 1.77417, acc 0.289062\n",
      "2018-10-22T23:30:27.501438: step 655, loss 1.75474, acc 0.382812\n",
      "2018-10-22T23:30:28.031022: step 656, loss 1.77598, acc 0.304688\n",
      "2018-10-22T23:30:28.562601: step 657, loss 1.86806, acc 0.28125\n",
      "2018-10-22T23:30:29.062265: step 658, loss 1.67947, acc 0.359375\n",
      "2018-10-22T23:30:29.546969: step 659, loss 1.82599, acc 0.335938\n",
      "2018-10-22T23:30:30.075555: step 660, loss 1.68127, acc 0.390625\n",
      "2018-10-22T23:30:30.605139: step 661, loss 1.83247, acc 0.234375\n",
      "2018-10-22T23:30:31.092835: step 662, loss 1.77748, acc 0.304688\n",
      "2018-10-22T23:30:31.558590: step 663, loss 1.7601, acc 0.304688\n",
      "2018-10-22T23:30:32.113109: step 664, loss 1.74945, acc 0.28125\n",
      "2018-10-22T23:30:32.599807: step 665, loss 1.79762, acc 0.351562\n",
      "2018-10-22T23:30:33.079524: step 666, loss 1.83538, acc 0.273438\n",
      "2018-10-22T23:30:33.668948: step 667, loss 1.7235, acc 0.320312\n",
      "2018-10-22T23:30:34.097801: step 668, loss 1.82287, acc 0.265625\n",
      "2018-10-22T23:30:34.536627: step 669, loss 1.76477, acc 0.273438\n",
      "2018-10-22T23:30:35.010361: step 670, loss 1.81395, acc 0.257812\n",
      "2018-10-22T23:30:35.479108: step 671, loss 1.80756, acc 0.265625\n",
      "2018-10-22T23:30:35.950846: step 672, loss 1.8065, acc 0.289062\n",
      "2018-10-22T23:30:36.486414: step 673, loss 1.7022, acc 0.398438\n",
      "2018-10-22T23:30:37.001039: step 674, loss 1.83094, acc 0.257812\n",
      "2018-10-22T23:30:37.423907: step 675, loss 1.80743, acc 0.359375\n",
      "2018-10-22T23:30:37.880686: step 676, loss 1.66629, acc 0.414062\n",
      "2018-10-22T23:30:38.329486: step 677, loss 1.76441, acc 0.289062\n",
      "2018-10-22T23:30:38.747369: step 678, loss 1.72975, acc 0.257812\n",
      "2018-10-22T23:30:39.180213: step 679, loss 1.78397, acc 0.328125\n",
      "2018-10-22T23:30:39.721764: step 680, loss 1.82127, acc 0.265625\n",
      "2018-10-22T23:30:40.238382: step 681, loss 1.80715, acc 0.3125\n",
      "2018-10-22T23:30:40.682195: step 682, loss 1.86277, acc 0.304688\n",
      "2018-10-22T23:30:41.134985: step 683, loss 1.73093, acc 0.296875\n",
      "2018-10-22T23:30:41.580793: step 684, loss 1.79039, acc 0.25\n",
      "2018-10-22T23:30:42.063504: step 685, loss 1.83045, acc 0.320312\n",
      "2018-10-22T23:30:42.511305: step 686, loss 1.79987, acc 0.328125\n",
      "2018-10-22T23:30:42.934175: step 687, loss 1.81034, acc 0.25\n",
      "2018-10-22T23:30:43.368014: step 688, loss 1.8384, acc 0.257812\n",
      "2018-10-22T23:30:43.932504: step 689, loss 1.81612, acc 0.296875\n",
      "2018-10-22T23:30:44.383300: step 690, loss 1.80496, acc 0.273438\n",
      "2018-10-22T23:30:44.870996: step 691, loss 1.84015, acc 0.335938\n",
      "2018-10-22T23:30:45.394595: step 692, loss 1.64113, acc 0.398438\n",
      "2018-10-22T23:30:45.849379: step 693, loss 1.7662, acc 0.28125\n",
      "2018-10-22T23:30:46.260280: step 694, loss 1.81948, acc 0.25\n",
      "2018-10-22T23:30:46.718057: step 695, loss 1.77669, acc 0.242188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:30:47.192787: step 696, loss 1.81608, acc 0.257812\n",
      "2018-10-22T23:30:47.637599: step 697, loss 1.71449, acc 0.34375\n",
      "2018-10-22T23:30:48.103353: step 698, loss 1.81937, acc 0.289062\n",
      "2018-10-22T23:30:48.583071: step 699, loss 1.73806, acc 0.304688\n",
      "2018-10-22T23:30:49.009928: step 700, loss 1.83366, acc 0.289062\n",
      "2018-10-22T23:30:49.436787: step 701, loss 1.72078, acc 0.328125\n",
      "2018-10-22T23:30:49.853673: step 702, loss 1.80475, acc 0.304688\n",
      "2018-10-22T23:30:50.293496: step 703, loss 1.81878, acc 0.296875\n",
      "2018-10-22T23:30:50.832057: step 704, loss 1.78565, acc 0.351562\n",
      "2018-10-22T23:30:51.281856: step 705, loss 1.67759, acc 0.398438\n",
      "2018-10-22T23:30:51.815428: step 706, loss 1.76848, acc 0.273438\n",
      "2018-10-22T23:30:52.318084: step 707, loss 1.75691, acc 0.351562\n",
      "2018-10-22T23:30:52.816751: step 708, loss 1.72383, acc 0.398438\n",
      "2018-10-22T23:30:53.254579: step 709, loss 1.6899, acc 0.34375\n",
      "2018-10-22T23:30:53.690414: step 710, loss 1.72209, acc 0.3125\n",
      "2018-10-22T23:30:54.252910: step 711, loss 1.70614, acc 0.335938\n",
      "2018-10-22T23:30:54.771523: step 712, loss 1.7594, acc 0.320312\n",
      "2018-10-22T23:30:55.269193: step 713, loss 1.836, acc 0.25\n",
      "2018-10-22T23:30:55.727966: step 714, loss 1.72829, acc 0.304688\n",
      "2018-10-22T23:30:56.276498: step 715, loss 1.64065, acc 0.429688\n",
      "2018-10-22T23:30:56.745246: step 716, loss 1.7412, acc 0.289062\n",
      "2018-10-22T23:30:57.229950: step 717, loss 1.69869, acc 0.359375\n",
      "2018-10-22T23:30:57.955012: step 718, loss 1.84821, acc 0.21875\n",
      "2018-10-22T23:30:58.435727: step 719, loss 1.8254, acc 0.3125\n",
      "2018-10-22T23:30:58.871561: step 720, loss 1.77974, acc 0.289062\n",
      "2018-10-22T23:30:59.362339: step 721, loss 1.8052, acc 0.304688\n",
      "2018-10-22T23:30:59.839359: step 722, loss 1.72612, acc 0.351562\n",
      "2018-10-22T23:31:00.315087: step 723, loss 1.78378, acc 0.25\n",
      "2018-10-22T23:31:00.781839: step 724, loss 1.77465, acc 0.320312\n",
      "2018-10-22T23:31:01.283496: step 725, loss 1.71183, acc 0.3125\n",
      "2018-10-22T23:31:01.742270: step 726, loss 1.78216, acc 0.273438\n",
      "2018-10-22T23:31:02.148186: step 727, loss 1.79225, acc 0.273438\n",
      "2018-10-22T23:31:02.577055: step 728, loss 1.83266, acc 0.289062\n",
      "2018-10-22T23:31:03.087689: step 729, loss 1.75417, acc 0.328125\n",
      "2018-10-22T23:31:03.633232: step 730, loss 1.68009, acc 0.304688\n",
      "2018-10-22T23:31:04.177774: step 731, loss 1.79145, acc 0.320312\n",
      "2018-10-22T23:31:04.751241: step 732, loss 1.84976, acc 0.226562\n",
      "2018-10-22T23:31:05.365599: step 733, loss 1.73904, acc 0.3125\n",
      "2018-10-22T23:31:05.864265: step 734, loss 1.74165, acc 0.3125\n",
      "2018-10-22T23:31:06.402825: step 735, loss 1.80856, acc 0.304688\n",
      "2018-10-22T23:31:07.015189: step 736, loss 1.79669, acc 0.257812\n",
      "2018-10-22T23:31:07.563721: step 737, loss 1.83186, acc 0.273438\n",
      "2018-10-22T23:31:08.251882: step 738, loss 1.78791, acc 0.28125\n",
      "2018-10-22T23:31:08.795427: step 739, loss 1.71935, acc 0.351562\n",
      "2018-10-22T23:31:09.302074: step 740, loss 1.77127, acc 0.25\n",
      "2018-10-22T23:31:09.848613: step 741, loss 1.76092, acc 0.289062\n",
      "2018-10-22T23:31:10.466959: step 742, loss 1.76913, acc 0.265625\n",
      "2018-10-22T23:31:11.019481: step 743, loss 1.74709, acc 0.320312\n",
      "2018-10-22T23:31:11.535102: step 744, loss 1.90196, acc 0.28125\n",
      "2018-10-22T23:31:12.183370: step 745, loss 1.77636, acc 0.34375\n",
      "2018-10-22T23:31:12.764815: step 746, loss 1.69248, acc 0.3125\n",
      "2018-10-22T23:31:13.204638: step 747, loss 1.84645, acc 0.304688\n",
      "2018-10-22T23:31:13.622521: step 748, loss 1.81333, acc 0.289062\n",
      "2018-10-22T23:31:14.097251: step 749, loss 1.74254, acc 0.351562\n",
      "2018-10-22T23:31:14.574974: step 750, loss 1.67924, acc 0.34375\n",
      "2018-10-22T23:31:15.048709: step 751, loss 1.77909, acc 0.296875\n",
      "2018-10-22T23:31:15.567322: step 752, loss 1.73561, acc 0.359375\n",
      "2018-10-22T23:31:16.021108: step 753, loss 1.78303, acc 0.320312\n",
      "2018-10-22T23:31:16.465920: step 754, loss 1.78445, acc 0.25\n",
      "2018-10-22T23:31:16.974558: step 755, loss 1.71406, acc 0.304688\n",
      "2018-10-22T23:31:17.416378: step 756, loss 1.77446, acc 0.34375\n",
      "2018-10-22T23:31:17.880137: step 757, loss 1.84767, acc 0.25\n",
      "2018-10-22T23:31:18.363844: step 758, loss 1.80711, acc 0.289062\n",
      "2018-10-22T23:31:18.871487: step 759, loss 1.72129, acc 0.335938\n",
      "2018-10-22T23:31:19.263439: step 760, loss 1.73527, acc 0.335938\n",
      "2018-10-22T23:31:19.690297: step 761, loss 1.71923, acc 0.359375\n",
      "2018-10-22T23:31:20.174004: step 762, loss 1.73932, acc 0.320312\n",
      "2018-10-22T23:31:20.640757: step 763, loss 1.82139, acc 0.3125\n",
      "2018-10-22T23:31:21.088559: step 764, loss 1.73766, acc 0.3125\n",
      "2018-10-22T23:31:21.527385: step 765, loss 1.76439, acc 0.382812\n",
      "2018-10-22T23:31:22.013086: step 766, loss 1.81313, acc 0.265625\n",
      "2018-10-22T23:31:22.606500: step 767, loss 1.74249, acc 0.335938\n",
      "2018-10-22T23:31:23.098186: step 768, loss 1.97966, acc 0.21875\n",
      "2018-10-22T23:31:23.550975: step 769, loss 1.85839, acc 0.234375\n",
      "2018-10-22T23:31:24.034682: step 770, loss 1.74335, acc 0.289062\n",
      "2018-10-22T23:31:24.516394: step 771, loss 1.72536, acc 0.328125\n",
      "2018-10-22T23:31:24.972174: step 772, loss 1.69932, acc 0.34375\n",
      "2018-10-22T23:31:25.468846: step 773, loss 1.81361, acc 0.304688\n",
      "2018-10-22T23:31:26.044308: step 774, loss 1.84214, acc 0.25\n",
      "2018-10-22T23:31:26.553945: step 775, loss 1.81372, acc 0.296875\n",
      "2018-10-22T23:31:26.991774: step 776, loss 1.72029, acc 0.335938\n",
      "2018-10-22T23:31:27.500415: step 777, loss 1.77252, acc 0.265625\n",
      "2018-10-22T23:31:27.988111: step 778, loss 1.71759, acc 0.335938\n",
      "2018-10-22T23:31:28.456858: step 779, loss 1.77309, acc 0.351562\n",
      "2018-10-22T23:31:28.896681: step 780, loss 1.68301, acc 0.320312\n",
      "2018-10-22T23:31:29.396346: step 781, loss 1.76722, acc 0.367188\n",
      "2018-10-22T23:31:29.833178: step 782, loss 1.71262, acc 0.304688\n",
      "2018-10-22T23:31:30.261033: step 783, loss 1.82868, acc 0.242188\n",
      "2018-10-22T23:31:30.759700: step 784, loss 1.76415, acc 0.328125\n",
      "2018-10-22T23:31:31.202516: step 785, loss 1.81533, acc 0.273438\n",
      "2018-10-22T23:31:31.679241: step 786, loss 1.76721, acc 0.265625\n",
      "2018-10-22T23:31:32.102111: step 787, loss 1.77385, acc 0.34375\n",
      "2018-10-22T23:31:32.603769: step 788, loss 1.74731, acc 0.304688\n",
      "2018-10-22T23:31:33.055561: step 789, loss 1.69712, acc 0.351562\n",
      "2018-10-22T23:31:33.564201: step 790, loss 1.76034, acc 0.273438\n",
      "2018-10-22T23:31:34.003028: step 791, loss 1.80115, acc 0.289062\n",
      "2018-10-22T23:31:34.462799: step 792, loss 1.78373, acc 0.273438\n",
      "2018-10-22T23:31:34.905615: step 793, loss 1.77176, acc 0.304688\n",
      "2018-10-22T23:31:35.322500: step 794, loss 1.80186, acc 0.296875\n",
      "2018-10-22T23:31:35.769305: step 795, loss 1.80364, acc 0.28125\n",
      "2018-10-22T23:31:36.296894: step 796, loss 1.81168, acc 0.320312\n",
      "2018-10-22T23:31:36.737716: step 797, loss 1.68037, acc 0.390625\n",
      "2018-10-22T23:31:37.160845: step 798, loss 1.76725, acc 0.335938\n",
      "2018-10-22T23:31:37.616627: step 799, loss 1.86846, acc 0.257812\n",
      "2018-10-22T23:31:38.102582: step 800, loss 1.79384, acc 0.335938\n",
      "2018-10-22T23:31:38.549388: step 801, loss 1.7811, acc 0.28125\n",
      "2018-10-22T23:31:39.003174: step 802, loss 1.71984, acc 0.429688\n",
      "2018-10-22T23:31:39.487879: step 803, loss 1.80671, acc 0.296875\n",
      "2018-10-22T23:31:39.956625: step 804, loss 1.79451, acc 0.273438\n",
      "2018-10-22T23:31:40.378497: step 805, loss 1.76007, acc 0.328125\n",
      "2018-10-22T23:31:40.827297: step 806, loss 1.68886, acc 0.390625\n",
      "2018-10-22T23:31:41.371842: step 807, loss 1.80568, acc 0.304688\n",
      "2018-10-22T23:31:41.863527: step 808, loss 1.86916, acc 0.28125\n",
      "2018-10-22T23:31:42.340251: step 809, loss 1.75644, acc 0.328125\n",
      "2018-10-22T23:31:42.890779: step 810, loss 1.73884, acc 0.328125\n",
      "2018-10-22T23:31:43.467238: step 811, loss 1.78242, acc 0.273438\n",
      "2018-10-22T23:31:44.040706: step 812, loss 1.64399, acc 0.351562\n",
      "2018-10-22T23:31:44.512443: step 813, loss 1.77544, acc 0.335938\n",
      "2018-10-22T23:31:45.035047: step 814, loss 1.72651, acc 0.351562\n",
      "2018-10-22T23:31:45.527729: step 815, loss 1.80933, acc 0.304688\n",
      "2018-10-22T23:31:45.964561: step 816, loss 1.71375, acc 0.351562\n",
      "2018-10-22T23:31:46.498134: step 817, loss 1.7751, acc 0.335938\n",
      "2018-10-22T23:31:46.981841: step 818, loss 1.84657, acc 0.265625\n",
      "2018-10-22T23:31:47.486492: step 819, loss 1.72968, acc 0.34375\n",
      "2018-10-22T23:31:48.042006: step 820, loss 1.75334, acc 0.296875\n",
      "2018-10-22T23:31:48.534689: step 821, loss 1.72575, acc 0.367188\n",
      "2018-10-22T23:31:49.102173: step 822, loss 1.79041, acc 0.335938\n",
      "2018-10-22T23:31:49.623777: step 823, loss 1.72914, acc 0.34375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:31:50.185277: step 824, loss 1.85842, acc 0.257812\n",
      "2018-10-22T23:31:50.686934: step 825, loss 1.72056, acc 0.328125\n",
      "2018-10-22T23:31:51.121772: step 826, loss 1.81606, acc 0.335938\n",
      "2018-10-22T23:31:51.600492: step 827, loss 1.77159, acc 0.296875\n",
      "2018-10-22T23:31:52.092177: step 828, loss 1.74089, acc 0.320312\n",
      "2018-10-22T23:31:52.599820: step 829, loss 1.83649, acc 0.257812\n",
      "2018-10-22T23:31:53.087515: step 830, loss 1.74867, acc 0.359375\n",
      "2018-10-22T23:31:53.667964: step 831, loss 1.74115, acc 0.296875\n",
      "2018-10-22T23:31:54.107787: step 832, loss 1.70968, acc 0.367188\n",
      "2018-10-22T23:31:54.617682: step 833, loss 1.79528, acc 0.3125\n",
      "2018-10-22T23:31:55.052847: step 834, loss 1.78753, acc 0.304688\n",
      "2018-10-22T23:31:55.518602: step 835, loss 1.82329, acc 0.257812\n",
      "2018-10-22T23:31:56.023252: step 836, loss 1.76582, acc 0.296875\n",
      "2018-10-22T23:31:56.550842: step 837, loss 1.78074, acc 0.335938\n",
      "2018-10-22T23:31:57.048512: step 838, loss 1.74401, acc 0.296875\n",
      "2018-10-22T23:31:57.505289: step 839, loss 1.77048, acc 0.304688\n",
      "2018-10-22T23:31:57.940127: step 840, loss 1.87062, acc 0.210938\n",
      "2018-10-22T23:31:58.416854: step 841, loss 1.78812, acc 0.304688\n",
      "2018-10-22T23:31:58.954416: step 842, loss 1.73389, acc 0.359375\n",
      "2018-10-22T23:31:59.483002: step 843, loss 1.8169, acc 0.25\n",
      "2018-10-22T23:31:59.971694: step 844, loss 1.80949, acc 0.382812\n",
      "2018-10-22T23:32:00.581066: step 845, loss 1.75076, acc 0.335938\n",
      "2018-10-22T23:32:01.070756: step 846, loss 1.83144, acc 0.265625\n",
      "2018-10-22T23:32:01.522549: step 847, loss 1.77337, acc 0.328125\n",
      "2018-10-22T23:32:01.990298: step 848, loss 1.77629, acc 0.320312\n",
      "2018-10-22T23:32:02.444084: step 849, loss 1.82729, acc 0.28125\n",
      "2018-10-22T23:32:02.891265: step 850, loss 1.79215, acc 0.3125\n",
      "2018-10-22T23:32:03.382951: step 851, loss 1.82548, acc 0.3125\n",
      "2018-10-22T23:32:03.823771: step 852, loss 1.77353, acc 0.296875\n",
      "2018-10-22T23:32:04.333408: step 853, loss 1.83419, acc 0.273438\n",
      "2018-10-22T23:32:04.825094: step 854, loss 1.64838, acc 0.40625\n",
      "2018-10-22T23:32:05.283868: step 855, loss 1.82149, acc 0.273438\n",
      "2018-10-22T23:32:05.739649: step 856, loss 1.762, acc 0.328125\n",
      "2018-10-22T23:32:06.170496: step 857, loss 1.76509, acc 0.304688\n",
      "2018-10-22T23:32:06.706065: step 858, loss 1.87599, acc 0.265625\n",
      "2018-10-22T23:32:07.138908: step 859, loss 1.77081, acc 0.328125\n",
      "2018-10-22T23:32:07.685447: step 860, loss 1.74548, acc 0.335938\n",
      "2018-10-22T23:32:08.107318: step 861, loss 1.79546, acc 0.296875\n",
      "2018-10-22T23:32:08.585042: step 862, loss 1.71756, acc 0.3125\n",
      "2018-10-22T23:32:09.117617: step 863, loss 1.74944, acc 0.328125\n",
      "2018-10-22T23:32:09.609302: step 864, loss 1.78675, acc 0.304688\n",
      "2018-10-22T23:32:10.132902: step 865, loss 1.75661, acc 0.390625\n",
      "2018-10-22T23:32:10.626583: step 866, loss 1.78143, acc 0.289062\n",
      "2018-10-22T23:32:11.152177: step 867, loss 1.63916, acc 0.429688\n",
      "2018-10-22T23:32:11.624913: step 868, loss 1.72355, acc 0.34375\n",
      "2018-10-22T23:32:12.071718: step 869, loss 1.7495, acc 0.382812\n",
      "2018-10-22T23:32:12.526502: step 870, loss 1.69578, acc 0.34375\n",
      "2018-10-22T23:32:12.976299: step 871, loss 1.77041, acc 0.28125\n",
      "2018-10-22T23:32:13.439062: step 872, loss 1.81344, acc 0.273438\n",
      "2018-10-22T23:32:13.915787: step 873, loss 1.68545, acc 0.367188\n",
      "2018-10-22T23:32:14.485265: step 874, loss 1.81624, acc 0.328125\n",
      "2018-10-22T23:32:15.077681: step 875, loss 1.86466, acc 0.21875\n",
      "2018-10-22T23:32:15.548422: step 876, loss 1.77915, acc 0.328125\n",
      "2018-10-22T23:32:15.991238: step 877, loss 1.8298, acc 0.273438\n",
      "2018-10-22T23:32:16.458987: step 878, loss 1.73266, acc 0.34375\n",
      "2018-10-22T23:32:16.923745: step 879, loss 1.7571, acc 0.304688\n",
      "2018-10-22T23:32:17.373543: step 880, loss 1.67034, acc 0.351562\n",
      "2018-10-22T23:32:17.853260: step 881, loss 1.69279, acc 0.3125\n",
      "2018-10-22T23:32:18.341954: step 882, loss 1.7872, acc 0.296875\n",
      "2018-10-22T23:32:18.867548: step 883, loss 1.80738, acc 0.265625\n",
      "2018-10-22T23:32:19.309366: step 884, loss 1.72644, acc 0.351562\n",
      "2018-10-22T23:32:19.841942: step 885, loss 1.80484, acc 0.3125\n",
      "2018-10-22T23:32:20.299720: step 886, loss 1.91843, acc 0.257812\n",
      "2018-10-22T23:32:20.818332: step 887, loss 1.74833, acc 0.296875\n",
      "2018-10-22T23:32:21.301042: step 888, loss 1.75282, acc 0.304688\n",
      "2018-10-22T23:32:21.776769: step 889, loss 1.75291, acc 0.34375\n",
      "2018-10-22T23:32:22.235915: step 890, loss 1.71361, acc 0.359375\n",
      "2018-10-22T23:32:22.703664: step 891, loss 1.73397, acc 0.34375\n",
      "2018-10-22T23:32:23.188368: step 892, loss 1.79647, acc 0.265625\n",
      "2018-10-22T23:32:23.624203: step 893, loss 1.68664, acc 0.367188\n",
      "2018-10-22T23:32:24.118881: step 894, loss 1.7289, acc 0.296875\n",
      "2018-10-22T23:32:24.607572: step 895, loss 1.82734, acc 0.3125\n",
      "2018-10-22T23:32:25.125189: step 896, loss 1.72053, acc 0.335938\n",
      "2018-10-22T23:32:25.591941: step 897, loss 1.73318, acc 0.328125\n",
      "2018-10-22T23:32:26.047723: step 898, loss 1.70914, acc 0.382812\n",
      "2018-10-22T23:32:26.525445: step 899, loss 1.80105, acc 0.289062\n",
      "2018-10-22T23:32:26.986214: step 900, loss 1.80639, acc 0.273438\n",
      "2018-10-22T23:32:27.478895: step 901, loss 1.78151, acc 0.320312\n",
      "2018-10-22T23:32:28.010475: step 902, loss 1.7258, acc 0.289062\n",
      "2018-10-22T23:32:28.619846: step 903, loss 1.67873, acc 0.328125\n",
      "2018-10-22T23:32:29.088592: step 904, loss 1.72308, acc 0.359375\n",
      "2018-10-22T23:32:29.533403: step 905, loss 1.80948, acc 0.265625\n",
      "2018-10-22T23:32:29.984197: step 906, loss 1.80434, acc 0.273438\n",
      "2018-10-22T23:32:30.479872: step 907, loss 1.68347, acc 0.335938\n",
      "2018-10-22T23:32:30.988513: step 908, loss 1.71438, acc 0.34375\n",
      "2018-10-22T23:32:31.617829: step 909, loss 1.67683, acc 0.398438\n",
      "2018-10-22T23:32:32.233184: step 910, loss 1.81248, acc 0.3125\n",
      "2018-10-22T23:32:32.676997: step 911, loss 1.75762, acc 0.296875\n",
      "2018-10-22T23:32:33.143749: step 912, loss 1.7249, acc 0.3125\n",
      "2018-10-22T23:32:33.591552: step 913, loss 1.72263, acc 0.335938\n",
      "2018-10-22T23:32:34.175989: step 914, loss 1.76108, acc 0.328125\n",
      "2018-10-22T23:32:34.687621: step 915, loss 1.67134, acc 0.40625\n",
      "2018-10-22T23:32:35.113447: step 916, loss 1.7583, acc 0.296875\n",
      "2018-10-22T23:32:35.625080: step 917, loss 1.79905, acc 0.296875\n",
      "2018-10-22T23:32:36.136712: step 918, loss 1.79253, acc 0.3125\n",
      "2018-10-22T23:32:36.623411: step 919, loss 1.7353, acc 0.320312\n",
      "2018-10-22T23:32:37.095149: step 920, loss 1.82632, acc 0.289062\n",
      "2018-10-22T23:32:37.523004: step 921, loss 1.81472, acc 0.3125\n",
      "2018-10-22T23:32:37.947869: step 922, loss 1.77757, acc 0.304688\n",
      "2018-10-22T23:32:38.408637: step 923, loss 1.86878, acc 0.242188\n",
      "2018-10-22T23:32:38.909298: step 924, loss 1.84941, acc 0.257812\n",
      "2018-10-22T23:32:39.406967: step 925, loss 1.75195, acc 0.3125\n",
      "2018-10-22T23:32:39.875714: step 926, loss 1.76651, acc 0.3125\n",
      "2018-10-22T23:32:40.299581: step 927, loss 1.7624, acc 0.351562\n",
      "2018-10-22T23:32:40.817197: step 928, loss 1.78746, acc 0.273438\n",
      "2018-10-22T23:32:41.285944: step 929, loss 1.76123, acc 0.320312\n",
      "2018-10-22T23:32:41.720782: step 930, loss 1.80961, acc 0.289062\n",
      "2018-10-22T23:32:42.251364: step 931, loss 1.75391, acc 0.273438\n",
      "2018-10-22T23:32:42.888659: step 932, loss 1.79375, acc 0.3125\n",
      "2018-10-22T23:32:43.472135: step 933, loss 1.75559, acc 0.34375\n",
      "2018-10-22T23:32:44.121403: step 934, loss 1.75505, acc 0.351562\n",
      "2018-10-22T23:32:44.705839: step 935, loss 1.79546, acc 0.273438\n",
      "2018-10-22T23:32:45.182564: step 936, loss 1.77849, acc 0.3125\n",
      "2018-10-22T23:32:45.625381: step 937, loss 1.7458, acc 0.398438\n",
      "2018-10-22T23:32:46.159952: step 938, loss 1.74443, acc 0.375\n",
      "2018-10-22T23:32:46.676571: step 939, loss 1.79343, acc 0.296875\n",
      "2018-10-22T23:32:47.147312: step 940, loss 1.67384, acc 0.335938\n",
      "2018-10-22T23:32:47.609077: step 941, loss 1.76213, acc 0.375\n",
      "2018-10-22T23:32:48.075830: step 942, loss 1.752, acc 0.34375\n",
      "2018-10-22T23:32:48.492714: step 943, loss 1.6657, acc 0.375\n",
      "2018-10-22T23:32:49.005344: step 944, loss 1.85552, acc 0.296875\n",
      "2018-10-22T23:32:49.559862: step 945, loss 1.73786, acc 0.304688\n",
      "2018-10-22T23:32:50.071495: step 946, loss 1.75577, acc 0.304688\n",
      "2018-10-22T23:32:50.541237: step 947, loss 1.77819, acc 0.320312\n",
      "2018-10-22T23:32:51.065834: step 948, loss 1.70178, acc 0.351562\n",
      "2018-10-22T23:32:51.663237: step 949, loss 1.69436, acc 0.351562\n",
      "2018-10-22T23:32:52.338432: step 950, loss 1.79334, acc 0.304688\n",
      "2018-10-22T23:32:52.877990: step 951, loss 1.80141, acc 0.296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:32:53.315818: step 952, loss 1.72225, acc 0.328125\n",
      "2018-10-22T23:32:53.738688: step 953, loss 1.82018, acc 0.28125\n",
      "2018-10-22T23:32:54.292207: step 954, loss 1.76889, acc 0.273438\n",
      "2018-10-22T23:32:54.786885: step 955, loss 1.73361, acc 0.3125\n",
      "2018-10-22T23:32:55.329209: step 956, loss 1.78368, acc 0.335938\n",
      "2018-10-22T23:32:55.776015: step 957, loss 1.82048, acc 0.28125\n",
      "2018-10-22T23:32:56.471157: step 958, loss 1.7345, acc 0.34375\n",
      "2018-10-22T23:32:57.069557: step 959, loss 1.77575, acc 0.304688\n",
      "2018-10-22T23:32:57.642027: step 960, loss 1.70172, acc 0.34375\n",
      "2018-10-22T23:32:58.252394: step 961, loss 1.74447, acc 0.335938\n",
      "2018-10-22T23:32:58.895674: step 962, loss 1.7414, acc 0.320312\n",
      "2018-10-22T23:32:59.612757: step 963, loss 1.75745, acc 0.335938\n",
      "2018-10-22T23:33:00.221130: step 964, loss 1.9145, acc 0.179688\n",
      "2018-10-22T23:33:00.750715: step 965, loss 1.84617, acc 0.328125\n",
      "2018-10-22T23:33:01.323184: step 966, loss 1.76258, acc 0.289062\n",
      "2018-10-22T23:33:02.119056: step 967, loss 1.80006, acc 0.296875\n",
      "2018-10-22T23:33:02.845114: step 968, loss 1.80885, acc 0.257812\n",
      "2018-10-22T23:33:03.558207: step 969, loss 1.75151, acc 0.351562\n",
      "2018-10-22T23:33:04.077818: step 970, loss 1.74956, acc 0.289062\n",
      "2018-10-22T23:33:04.501686: step 971, loss 1.72374, acc 0.335938\n",
      "2018-10-22T23:33:05.005339: step 972, loss 1.69748, acc 0.351562\n",
      "2018-10-22T23:33:05.501012: step 973, loss 1.74438, acc 0.273438\n",
      "2018-10-22T23:33:06.066501: step 974, loss 1.79918, acc 0.296875\n",
      "2018-10-22T23:33:06.817869: step 975, loss 1.81987, acc 0.289062\n",
      "2018-10-22T23:33:07.334550: step 976, loss 1.80847, acc 0.296875\n",
      "2018-10-22T23:33:07.843189: step 977, loss 1.83266, acc 0.242188\n",
      "2018-10-22T23:33:08.326896: step 978, loss 1.76156, acc 0.351562\n",
      "2018-10-22T23:33:08.809605: step 979, loss 1.69834, acc 0.335938\n",
      "2018-10-22T23:33:09.308272: step 980, loss 1.72863, acc 0.351562\n",
      "2018-10-22T23:33:09.716182: step 981, loss 1.7529, acc 0.320312\n",
      "2018-10-22T23:33:10.158000: step 982, loss 1.81145, acc 0.257812\n",
      "2018-10-22T23:33:10.682599: step 983, loss 1.73727, acc 0.351562\n",
      "2018-10-22T23:33:11.190240: step 984, loss 1.77866, acc 0.265625\n",
      "2018-10-22T23:33:11.638044: step 985, loss 1.78487, acc 0.3125\n",
      "2018-10-22T23:33:12.107788: step 986, loss 1.70853, acc 0.375\n",
      "2018-10-22T23:33:12.572545: step 987, loss 1.81799, acc 0.242188\n",
      "2018-10-22T23:33:13.084176: step 988, loss 1.843, acc 0.273438\n",
      "2018-10-22T23:33:13.502060: step 989, loss 1.69816, acc 0.359375\n",
      "2018-10-22T23:33:13.924929: step 990, loss 1.7177, acc 0.34375\n",
      "2018-10-22T23:33:14.411627: step 991, loss 1.77328, acc 0.265625\n",
      "2018-10-22T23:33:14.891346: step 992, loss 1.75896, acc 0.3125\n",
      "2018-10-22T23:33:15.350118: step 993, loss 1.8431, acc 0.257812\n",
      "2018-10-22T23:33:15.799915: step 994, loss 1.70511, acc 0.28125\n",
      "2018-10-22T23:33:16.226774: step 995, loss 1.75572, acc 0.328125\n",
      "2018-10-22T23:33:16.757355: step 996, loss 1.83036, acc 0.3125\n",
      "2018-10-22T23:33:17.318117: step 997, loss 1.68675, acc 0.335938\n",
      "2018-10-22T23:33:17.925772: step 998, loss 1.8248, acc 0.234375\n",
      "2018-10-22T23:33:18.372576: step 999, loss 1.74005, acc 0.34375\n",
      "2018-10-22T23:33:18.802428: step 1000, loss 1.77213, acc 0.367188\n",
      "\n",
      "Evaluation:\n",
      "Number of batches in dev set is 62\n",
      "batch 1 in dev >> 2018-10-22T23:33:20.606603: loss 1.78735, acc 0.292\n",
      "batch 2 in dev >> 2018-10-22T23:33:23.158780: loss 1.77572, acc 0.304\n",
      "batch 3 in dev >> 2018-10-22T23:33:24.869206: loss 1.76193, acc 0.288\n",
      "batch 4 in dev >> 2018-10-22T23:33:26.597584: loss 1.74052, acc 0.338\n",
      "batch 5 in dev >> 2018-10-22T23:33:28.288065: loss 1.73541, acc 0.336\n",
      "batch 6 in dev >> 2018-10-22T23:33:30.280736: loss 1.75016, acc 0.322\n",
      "batch 7 in dev >> 2018-10-22T23:33:32.023849: loss 1.77025, acc 0.304\n",
      "batch 8 in dev >> 2018-10-22T23:33:34.102292: loss 1.72362, acc 0.316\n",
      "batch 9 in dev >> 2018-10-22T23:33:35.740911: loss 1.76548, acc 0.288\n",
      "batch 10 in dev >> 2018-10-22T23:33:37.472283: loss 1.79535, acc 0.266\n",
      "batch 11 in dev >> 2018-10-22T23:33:39.359236: loss 1.73989, acc 0.342\n",
      "batch 12 in dev >> 2018-10-22T23:33:41.467598: loss 1.74014, acc 0.338\n",
      "batch 13 in dev >> 2018-10-22T23:33:43.200964: loss 1.75791, acc 0.29\n",
      "batch 14 in dev >> 2018-10-22T23:33:44.901418: loss 1.75556, acc 0.332\n",
      "batch 15 in dev >> 2018-10-22T23:33:46.650740: loss 1.75005, acc 0.306\n",
      "batch 16 in dev >> 2018-10-22T23:33:48.419013: loss 1.74349, acc 0.304\n",
      "batch 17 in dev >> 2018-10-22T23:33:50.225184: loss 1.762, acc 0.296\n",
      "batch 18 in dev >> 2018-10-22T23:33:52.097177: loss 1.7997, acc 0.29\n",
      "batch 19 in dev >> 2018-10-22T23:33:53.872470: loss 1.78678, acc 0.29\n",
      "batch 20 in dev >> 2018-10-22T23:33:55.535063: loss 1.752, acc 0.332\n",
      "batch 21 in dev >> 2018-10-22T23:33:57.328268: loss 1.74071, acc 0.31\n",
      "batch 22 in dev >> 2018-10-22T23:33:59.249132: loss 1.7205, acc 0.32\n",
      "batch 23 in dev >> 2018-10-22T23:34:02.481495: loss 1.77196, acc 0.292\n",
      "batch 24 in dev >> 2018-10-22T23:34:05.167325: loss 1.69209, acc 0.378\n",
      "batch 25 in dev >> 2018-10-22T23:34:07.450222: loss 1.75027, acc 0.322\n",
      "batch 26 in dev >> 2018-10-22T23:34:09.772012: loss 1.72923, acc 0.328\n",
      "batch 27 in dev >> 2018-10-22T23:34:11.843476: loss 1.76433, acc 0.316\n",
      "batch 28 in dev >> 2018-10-22T23:34:14.399640: loss 1.77021, acc 0.286\n",
      "batch 29 in dev >> 2018-10-22T23:34:16.254680: loss 1.815, acc 0.278\n",
      "batch 30 in dev >> 2018-10-22T23:34:18.858717: loss 1.74007, acc 0.342\n",
      "batch 31 in dev >> 2018-10-22T23:34:20.508307: loss 1.74886, acc 0.328\n",
      "batch 32 in dev >> 2018-10-22T23:34:22.143935: loss 1.75839, acc 0.292\n",
      "batch 33 in dev >> 2018-10-22T23:34:24.114664: loss 1.79915, acc 0.272\n",
      "batch 34 in dev >> 2018-10-22T23:34:25.819107: loss 1.75387, acc 0.294\n",
      "batch 35 in dev >> 2018-10-22T23:34:27.975341: loss 1.78034, acc 0.302\n",
      "batch 36 in dev >> 2018-10-22T23:34:29.917149: loss 1.78263, acc 0.32\n",
      "batch 37 in dev >> 2018-10-22T23:34:32.475310: loss 1.75857, acc 0.302\n",
      "batch 38 in dev >> 2018-10-22T23:34:34.153821: loss 1.69849, acc 0.336\n",
      "batch 39 in dev >> 2018-10-22T23:34:35.684728: loss 1.77369, acc 0.296\n",
      "batch 40 in dev >> 2018-10-22T23:34:37.322349: loss 1.73323, acc 0.33\n",
      "batch 41 in dev >> 2018-10-22T23:34:39.096606: loss 1.75467, acc 0.316\n",
      "batch 42 in dev >> 2018-10-22T23:34:40.903774: loss 1.77948, acc 0.312\n",
      "batch 43 in dev >> 2018-10-22T23:34:42.612205: loss 1.77386, acc 0.294\n",
      "batch 44 in dev >> 2018-10-22T23:34:44.375491: loss 1.77429, acc 0.318\n",
      "batch 45 in dev >> 2018-10-22T23:34:45.961251: loss 1.76145, acc 0.316\n",
      "batch 46 in dev >> 2018-10-22T23:34:47.591890: loss 1.77597, acc 0.292\n",
      "batch 47 in dev >> 2018-10-22T23:34:49.188621: loss 1.73547, acc 0.304\n",
      "batch 48 in dev >> 2018-10-22T23:34:50.835219: loss 1.77624, acc 0.312\n",
      "batch 49 in dev >> 2018-10-22T23:34:52.525699: loss 1.77751, acc 0.312\n",
      "batch 50 in dev >> 2018-10-22T23:34:54.212189: loss 1.78211, acc 0.31\n",
      "batch 51 in dev >> 2018-10-22T23:34:56.002403: loss 1.79285, acc 0.268\n",
      "batch 52 in dev >> 2018-10-22T23:34:57.833507: loss 1.72915, acc 0.302\n",
      "batch 53 in dev >> 2018-10-22T23:34:59.541938: loss 1.68155, acc 0.364\n",
      "batch 54 in dev >> 2018-10-22T23:35:01.294254: loss 1.75208, acc 0.286\n",
      "batch 55 in dev >> 2018-10-22T23:35:02.936861: loss 1.77374, acc 0.29\n",
      "batch 56 in dev >> 2018-10-22T23:35:04.548553: loss 1.75227, acc 0.322\n",
      "batch 57 in dev >> 2018-10-22T23:35:06.235042: loss 1.81491, acc 0.278\n",
      "batch 58 in dev >> 2018-10-22T23:35:07.837757: loss 1.72154, acc 0.348\n",
      "batch 59 in dev >> 2018-10-22T23:35:09.516269: loss 1.73138, acc 0.332\n",
      "batch 60 in dev >> 2018-10-22T23:35:11.044184: loss 1.73831, acc 0.338\n",
      "batch 61 in dev >> 2018-10-22T23:35:12.850354: loss 1.76742, acc 0.296\n",
      "batch 62 in dev >> 2018-10-22T23:35:14.602669: loss 1.80443, acc 0.29\n",
      "\n",
      "Mean accuracy=0.30996774040883585\n",
      "Mean loss=1.758476649561236\n",
      "\n",
      "2018-10-22T23:35:15.148211: step 1001, loss 1.81123, acc 0.289062\n",
      "2018-10-22T23:35:15.745713: step 1002, loss 1.79441, acc 0.195312\n",
      "2018-10-22T23:35:16.267318: step 1003, loss 1.75819, acc 0.328125\n",
      "2018-10-22T23:35:17.129015: step 1004, loss 1.72257, acc 0.304688\n",
      "2018-10-22T23:35:17.729197: step 1005, loss 1.79737, acc 0.304688\n",
      "2018-10-22T23:35:18.282716: step 1006, loss 1.80094, acc 0.296875\n",
      "2018-10-22T23:35:18.895079: step 1007, loss 1.77716, acc 0.3125\n",
      "2018-10-22T23:35:19.478519: step 1008, loss 1.75145, acc 0.34375\n",
      "2018-10-22T23:35:20.020070: step 1009, loss 1.73873, acc 0.359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:35:20.527713: step 1010, loss 1.74439, acc 0.34375\n",
      "2018-10-22T23:35:21.014411: step 1011, loss 1.73489, acc 0.3125\n",
      "2018-10-22T23:35:21.518066: step 1012, loss 1.79153, acc 0.320312\n",
      "2018-10-22T23:35:22.108486: step 1013, loss 1.78098, acc 0.257812\n",
      "2018-10-22T23:35:22.799639: step 1014, loss 1.75196, acc 0.359375\n",
      "2018-10-22T23:35:23.307280: step 1015, loss 1.769, acc 0.328125\n",
      "2018-10-22T23:35:23.761074: step 1016, loss 1.78373, acc 0.3125\n",
      "2018-10-22T23:35:24.204496: step 1017, loss 1.77494, acc 0.296875\n",
      "2018-10-22T23:35:24.628362: step 1018, loss 1.78989, acc 0.328125\n",
      "2018-10-22T23:35:25.106084: step 1019, loss 1.77593, acc 0.328125\n",
      "2018-10-22T23:35:25.556880: step 1020, loss 1.86059, acc 0.296875\n",
      "2018-10-22T23:35:26.071504: step 1021, loss 1.79206, acc 0.320312\n",
      "2018-10-22T23:35:26.506340: step 1022, loss 1.79891, acc 0.289062\n",
      "2018-10-22T23:35:26.948159: step 1023, loss 1.6997, acc 0.351562\n",
      "2018-10-22T23:35:27.371028: step 1024, loss 1.76131, acc 0.257812\n",
      "2018-10-22T23:35:27.882660: step 1025, loss 1.86538, acc 0.242188\n",
      "2018-10-22T23:35:28.365370: step 1026, loss 1.84471, acc 0.3125\n",
      "2018-10-22T23:35:28.824435: step 1027, loss 1.81391, acc 0.296875\n",
      "2018-10-22T23:35:29.313129: step 1028, loss 1.82972, acc 0.3125\n",
      "2018-10-22T23:35:29.883602: step 1029, loss 1.75841, acc 0.34375\n",
      "2018-10-22T23:35:30.381272: step 1030, loss 1.77207, acc 0.320312\n",
      "2018-10-22T23:35:30.890908: step 1031, loss 1.70654, acc 0.304688\n",
      "2018-10-22T23:35:31.395560: step 1032, loss 1.84274, acc 0.273438\n",
      "2018-10-22T23:35:31.882259: step 1033, loss 1.79207, acc 0.289062\n",
      "2018-10-22T23:35:32.352002: step 1034, loss 1.71447, acc 0.382812\n",
      "2018-10-22T23:35:32.875603: step 1035, loss 1.77129, acc 0.320312\n",
      "2018-10-22T23:35:33.396213: step 1036, loss 1.73871, acc 0.34375\n",
      "2018-10-22T23:35:33.910834: step 1037, loss 1.76559, acc 0.304688\n",
      "2018-10-22T23:35:34.337693: step 1038, loss 1.74027, acc 0.351562\n",
      "2018-10-22T23:35:34.775523: step 1039, loss 1.77829, acc 0.25\n",
      "2018-10-22T23:35:35.197395: step 1040, loss 1.72191, acc 0.3125\n",
      "2018-10-22T23:35:35.618270: step 1041, loss 1.78263, acc 0.320312\n",
      "2018-10-22T23:35:36.019198: step 1042, loss 1.78795, acc 0.335938\n",
      "2018-10-22T23:35:36.494926: step 1043, loss 1.72335, acc 0.335938\n",
      "2018-10-22T23:35:36.994590: step 1044, loss 1.87251, acc 0.28125\n",
      "2018-10-22T23:35:37.451368: step 1045, loss 1.74448, acc 0.320312\n",
      "2018-10-22T23:35:37.895183: step 1046, loss 1.81568, acc 0.257812\n",
      "2018-10-22T23:35:38.334007: step 1047, loss 1.8044, acc 0.296875\n",
      "2018-10-22T23:35:38.754883: step 1048, loss 1.78389, acc 0.28125\n",
      "2018-10-22T23:35:39.146834: step 1049, loss 1.79653, acc 0.265625\n",
      "2018-10-22T23:35:39.708334: step 1050, loss 1.7276, acc 0.34375\n",
      "2018-10-22T23:35:40.396494: step 1051, loss 1.75433, acc 0.28125\n",
      "2018-10-22T23:35:41.011848: step 1052, loss 1.69588, acc 0.367188\n",
      "2018-10-22T23:35:41.599277: step 1053, loss 1.77584, acc 0.390625\n",
      "2018-10-22T23:35:42.183716: step 1054, loss 1.76547, acc 0.28125\n",
      "2018-10-22T23:35:42.746211: step 1055, loss 1.68006, acc 0.382812\n",
      "2018-10-22T23:35:43.268814: step 1056, loss 1.79023, acc 0.335938\n",
      "2018-10-22T23:35:43.968941: step 1057, loss 1.78491, acc 0.242188\n",
      "2018-10-22T23:35:44.447662: step 1058, loss 1.84148, acc 0.273438\n",
      "2018-10-22T23:35:44.891475: step 1059, loss 1.77722, acc 0.320312\n",
      "2018-10-22T23:35:45.316339: step 1060, loss 1.80142, acc 0.320312\n",
      "2018-10-22T23:35:45.798051: step 1061, loss 1.71577, acc 0.34375\n",
      "2018-10-22T23:35:46.343592: step 1062, loss 1.71379, acc 0.34375\n",
      "2018-10-22T23:35:46.927032: step 1063, loss 1.72467, acc 0.320312\n",
      "2018-10-22T23:35:47.532414: step 1064, loss 1.85639, acc 0.242188\n",
      "2018-10-22T23:35:48.049032: step 1065, loss 1.70408, acc 0.390625\n",
      "2018-10-22T23:35:48.509800: step 1066, loss 1.67687, acc 0.429688\n",
      "2018-10-22T23:35:49.069304: step 1067, loss 1.71351, acc 0.351562\n",
      "2018-10-22T23:35:49.593902: step 1068, loss 1.77887, acc 0.296875\n",
      "2018-10-22T23:35:50.084590: step 1069, loss 1.72795, acc 0.34375\n",
      "2018-10-22T23:35:50.510451: step 1070, loss 1.68604, acc 0.351562\n",
      "2018-10-22T23:35:51.024078: step 1071, loss 1.81706, acc 0.3125\n",
      "2018-10-22T23:35:51.501801: step 1072, loss 1.70449, acc 0.328125\n",
      "2018-10-22T23:35:51.995481: step 1073, loss 1.73547, acc 0.296875\n",
      "2018-10-22T23:35:52.465224: step 1074, loss 1.68869, acc 0.421875\n",
      "2018-10-22T23:35:53.009768: step 1075, loss 1.77104, acc 0.367188\n",
      "2018-10-22T23:35:53.508434: step 1076, loss 1.75924, acc 0.34375\n",
      "2018-10-22T23:35:54.016078: step 1077, loss 1.76587, acc 0.3125\n",
      "2018-10-22T23:35:54.468867: step 1078, loss 1.70586, acc 0.375\n",
      "2018-10-22T23:35:54.937613: step 1079, loss 1.80212, acc 0.320312\n",
      "2018-10-22T23:35:55.397385: step 1080, loss 1.68886, acc 0.320312\n",
      "2018-10-22T23:35:55.924974: step 1081, loss 1.72466, acc 0.3125\n",
      "2018-10-22T23:35:56.424637: step 1082, loss 1.70245, acc 0.335938\n",
      "2018-10-22T23:35:56.912335: step 1083, loss 1.82849, acc 0.296875\n",
      "2018-10-22T23:35:57.368117: step 1084, loss 1.70396, acc 0.328125\n",
      "2018-10-22T23:35:57.880745: step 1085, loss 1.74554, acc 0.351562\n",
      "2018-10-22T23:35:58.302616: step 1086, loss 1.80042, acc 0.328125\n",
      "2018-10-22T23:35:58.718505: step 1087, loss 1.71282, acc 0.359375\n",
      "2018-10-22T23:35:59.190242: step 1088, loss 1.82277, acc 0.296875\n",
      "2018-10-22T23:35:59.691901: step 1089, loss 1.79635, acc 0.3125\n",
      "2018-10-22T23:36:00.135715: step 1090, loss 1.86047, acc 0.328125\n",
      "2018-10-22T23:36:00.584516: step 1091, loss 1.74687, acc 0.304688\n",
      "2018-10-22T23:36:01.070217: step 1092, loss 1.74274, acc 0.335938\n",
      "2018-10-22T23:36:01.486104: step 1093, loss 1.7348, acc 0.367188\n",
      "2018-10-22T23:36:01.931913: step 1094, loss 1.79745, acc 0.289062\n",
      "2018-10-22T23:36:02.411629: step 1095, loss 1.87388, acc 0.28125\n",
      "2018-10-22T23:36:02.894183: step 1096, loss 1.8259, acc 0.257812\n",
      "2018-10-22T23:36:03.336003: step 1097, loss 1.72092, acc 0.296875\n",
      "2018-10-22T23:36:03.832675: step 1098, loss 1.80665, acc 0.320312\n",
      "2018-10-22T23:36:04.318375: step 1099, loss 1.76998, acc 0.289062\n",
      "2018-10-22T23:36:04.816046: step 1100, loss 1.81505, acc 0.28125\n",
      "2018-10-22T23:36:05.247890: step 1101, loss 1.766, acc 0.257812\n",
      "2018-10-22T23:36:05.701677: step 1102, loss 1.71168, acc 0.367188\n",
      "2018-10-22T23:36:06.167432: step 1103, loss 1.80589, acc 0.304688\n",
      "2018-10-22T23:36:06.620221: step 1104, loss 1.83892, acc 0.3125\n",
      "2018-10-22T23:36:07.077997: step 1105, loss 1.71615, acc 0.382812\n",
      "2018-10-22T23:36:07.547742: step 1106, loss 1.74443, acc 0.304688\n",
      "2018-10-22T23:36:08.063364: step 1107, loss 1.75196, acc 0.351562\n",
      "2018-10-22T23:36:08.466285: step 1108, loss 1.69939, acc 0.359375\n",
      "2018-10-22T23:36:08.908104: step 1109, loss 1.7688, acc 0.328125\n",
      "2018-10-22T23:36:09.351917: step 1110, loss 1.64857, acc 0.40625\n",
      "2018-10-22T23:36:09.854574: step 1111, loss 1.81111, acc 0.289062\n",
      "2018-10-22T23:36:10.342269: step 1112, loss 1.78247, acc 0.3125\n",
      "2018-10-22T23:36:10.873847: step 1113, loss 1.70125, acc 0.351562\n",
      "2018-10-22T23:36:11.465266: step 1114, loss 1.67837, acc 0.351562\n",
      "2018-10-22T23:36:11.993853: step 1115, loss 1.79377, acc 0.304688\n",
      "2018-10-22T23:36:12.580285: step 1116, loss 1.72764, acc 0.359375\n",
      "2018-10-22T23:36:13.148765: step 1117, loss 1.69692, acc 0.398438\n",
      "2018-10-22T23:36:13.765117: step 1118, loss 1.78741, acc 0.304688\n",
      "2018-10-22T23:36:14.324621: step 1119, loss 1.72833, acc 0.289062\n",
      "2018-10-22T23:36:14.965907: step 1120, loss 1.74382, acc 0.304688\n",
      "2018-10-22T23:36:15.547352: step 1121, loss 1.8403, acc 0.328125\n",
      "2018-10-22T23:36:16.068957: step 1122, loss 1.74992, acc 0.328125\n",
      "2018-10-22T23:36:16.571614: step 1123, loss 1.75766, acc 0.296875\n",
      "2018-10-22T23:36:17.082248: step 1124, loss 1.70961, acc 0.390625\n",
      "2018-10-22T23:36:17.619810: step 1125, loss 1.70723, acc 0.382812\n",
      "2018-10-22T23:36:18.182306: step 1126, loss 1.71126, acc 0.335938\n",
      "2018-10-22T23:36:18.747794: step 1127, loss 1.86927, acc 0.257812\n",
      "2018-10-22T23:36:19.316274: step 1128, loss 1.75396, acc 0.320312\n",
      "2018-10-22T23:36:19.894727: step 1129, loss 1.74007, acc 0.414062\n",
      "2018-10-22T23:36:20.385415: step 1130, loss 1.73535, acc 0.351562\n",
      "2018-10-22T23:36:20.904028: step 1131, loss 1.75236, acc 0.304688\n",
      "2018-10-22T23:36:21.371778: step 1132, loss 1.81638, acc 0.304688\n",
      "2018-10-22T23:36:22.020045: step 1133, loss 1.79404, acc 0.34375\n",
      "2018-10-22T23:36:22.596503: step 1134, loss 1.72116, acc 0.40625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:36:23.097165: step 1135, loss 1.73905, acc 0.273438\n",
      "2018-10-22T23:36:23.535991: step 1136, loss 1.77696, acc 0.320312\n",
      "2018-10-22T23:36:23.997757: step 1137, loss 1.77312, acc 0.304688\n",
      "2018-10-22T23:36:24.450546: step 1138, loss 1.82313, acc 0.273438\n",
      "2018-10-22T23:36:24.911314: step 1139, loss 1.71706, acc 0.328125\n",
      "2018-10-22T23:36:25.428930: step 1140, loss 1.73259, acc 0.34375\n",
      "2018-10-22T23:36:25.852798: step 1141, loss 1.72851, acc 0.375\n",
      "2018-10-22T23:36:26.294615: step 1142, loss 1.78375, acc 0.289062\n",
      "2018-10-22T23:36:26.721474: step 1143, loss 1.73045, acc 0.367188\n",
      "2018-10-22T23:36:27.186232: step 1144, loss 1.69798, acc 0.414062\n",
      "2018-10-22T23:36:27.632041: step 1145, loss 1.80879, acc 0.335938\n",
      "2018-10-22T23:36:28.126718: step 1146, loss 1.70435, acc 0.351562\n",
      "2018-10-22T23:36:28.662285: step 1147, loss 1.67768, acc 0.351562\n",
      "2018-10-22T23:36:29.154967: step 1148, loss 1.68528, acc 0.367188\n",
      "2018-10-22T23:36:29.577836: step 1149, loss 1.82107, acc 0.304688\n",
      "2018-10-22T23:36:29.998712: step 1150, loss 1.72879, acc 0.296875\n",
      "2018-10-22T23:36:30.442525: step 1151, loss 1.77163, acc 0.304688\n",
      "2018-10-22T23:36:30.869384: step 1152, loss 1.81838, acc 0.34375\n",
      "2018-10-22T23:36:31.319181: step 1153, loss 1.7547, acc 0.304688\n",
      "2018-10-22T23:36:31.780947: step 1154, loss 1.71909, acc 0.3125\n",
      "2018-10-22T23:36:32.268642: step 1155, loss 1.81349, acc 0.296875\n",
      "2018-10-22T23:36:32.728414: step 1156, loss 1.75913, acc 0.335938\n",
      "2018-10-22T23:36:33.149287: step 1157, loss 1.77488, acc 0.34375\n",
      "2018-10-22T23:36:33.602076: step 1158, loss 1.74354, acc 0.328125\n",
      "2018-10-22T23:36:34.049879: step 1159, loss 1.7524, acc 0.296875\n",
      "2018-10-22T23:36:34.503666: step 1160, loss 1.76932, acc 0.3125\n",
      "2018-10-22T23:36:34.916562: step 1161, loss 1.72914, acc 0.28125\n",
      "2018-10-22T23:36:35.426200: step 1162, loss 1.85906, acc 0.265625\n",
      "2018-10-22T23:36:35.936834: step 1163, loss 1.68064, acc 0.367188\n",
      "2018-10-22T23:36:36.437496: step 1164, loss 1.73335, acc 0.382812\n",
      "2018-10-22T23:36:37.016947: step 1165, loss 1.8212, acc 0.265625\n",
      "2018-10-22T23:36:37.509629: step 1166, loss 1.80388, acc 0.328125\n",
      "2018-10-22T23:36:37.977379: step 1167, loss 1.63856, acc 0.421875\n",
      "2018-10-22T23:36:38.403239: step 1168, loss 1.68748, acc 0.34375\n",
      "2018-10-22T23:36:38.876974: step 1169, loss 1.72854, acc 0.335938\n",
      "2018-10-22T23:36:39.379629: step 1170, loss 1.78039, acc 0.3125\n",
      "2018-10-22T23:36:39.888270: step 1171, loss 1.7439, acc 0.34375\n",
      "2018-10-22T23:36:40.328092: step 1172, loss 1.70689, acc 0.304688\n",
      "2018-10-22T23:36:40.783874: step 1173, loss 1.76589, acc 0.296875\n",
      "2018-10-22T23:36:41.195773: step 1174, loss 1.71786, acc 0.34375\n",
      "2018-10-22T23:36:41.716382: step 1175, loss 1.76509, acc 0.34375\n",
      "2018-10-22T23:36:42.202083: step 1176, loss 1.77149, acc 0.273438\n",
      "2018-10-22T23:36:42.808461: step 1177, loss 1.79763, acc 0.304688\n",
      "2018-10-22T23:36:43.262248: step 1178, loss 1.80483, acc 0.296875\n",
      "2018-10-22T23:36:43.728999: step 1179, loss 1.8326, acc 0.265625\n",
      "2018-10-22T23:36:44.165831: step 1180, loss 1.80839, acc 0.328125\n",
      "2018-10-22T23:36:44.585709: step 1181, loss 1.69456, acc 0.359375\n",
      "2018-10-22T23:36:45.020546: step 1182, loss 1.77632, acc 0.320312\n",
      "2018-10-22T23:36:45.446408: step 1183, loss 1.7496, acc 0.289062\n",
      "2018-10-22T23:36:45.960034: step 1184, loss 1.71186, acc 0.328125\n",
      "2018-10-22T23:36:46.466680: step 1185, loss 1.77939, acc 0.382812\n",
      "2018-10-22T23:36:46.917474: step 1186, loss 1.84213, acc 0.273438\n",
      "2018-10-22T23:36:47.371261: step 1187, loss 1.76121, acc 0.375\n",
      "2018-10-22T23:36:47.804104: step 1188, loss 1.73746, acc 0.328125\n",
      "2018-10-22T23:36:48.226973: step 1189, loss 1.73155, acc 0.289062\n",
      "2018-10-22T23:36:48.634882: step 1190, loss 1.78577, acc 0.3125\n",
      "2018-10-22T23:36:49.111607: step 1191, loss 1.80367, acc 0.34375\n",
      "2018-10-22T23:36:49.649171: step 1192, loss 1.75562, acc 0.320312\n",
      "2018-10-22T23:36:50.112931: step 1193, loss 1.77574, acc 0.335938\n",
      "2018-10-22T23:36:50.555746: step 1194, loss 1.8312, acc 0.226562\n",
      "2018-10-22T23:36:50.954680: step 1195, loss 1.72732, acc 0.367188\n",
      "2018-10-22T23:36:51.372562: step 1196, loss 1.72063, acc 0.335938\n",
      "2018-10-22T23:36:51.879208: step 1197, loss 1.85186, acc 0.273438\n",
      "2018-10-22T23:36:52.448686: step 1198, loss 1.75758, acc 0.28125\n",
      "2018-10-22T23:36:52.977271: step 1199, loss 1.71074, acc 0.359375\n",
      "2018-10-22T23:36:53.435049: step 1200, loss 1.74744, acc 0.390625\n",
      "2018-10-22T23:36:53.887837: step 1201, loss 1.86664, acc 0.3125\n",
      "2018-10-22T23:36:54.337634: step 1202, loss 1.6466, acc 0.382812\n",
      "2018-10-22T23:36:54.866221: step 1203, loss 1.7203, acc 0.375\n",
      "2018-10-22T23:36:55.347933: step 1204, loss 1.76314, acc 0.375\n",
      "2018-10-22T23:36:55.803714: step 1205, loss 1.7154, acc 0.351562\n",
      "2018-10-22T23:36:56.314348: step 1206, loss 1.7864, acc 0.289062\n",
      "2018-10-22T23:36:56.799053: step 1207, loss 1.69703, acc 0.375\n",
      "2018-10-22T23:36:57.249849: step 1208, loss 1.78073, acc 0.359375\n",
      "2018-10-22T23:36:57.779431: step 1209, loss 1.72074, acc 0.328125\n",
      "2018-10-22T23:36:58.268124: step 1210, loss 1.71269, acc 0.304688\n",
      "2018-10-22T23:36:58.763799: step 1211, loss 1.69064, acc 0.351562\n",
      "2018-10-22T23:36:59.233544: step 1212, loss 1.71329, acc 0.34375\n",
      "2018-10-22T23:36:59.729218: step 1213, loss 1.77368, acc 0.265625\n",
      "2018-10-22T23:37:00.242845: step 1214, loss 1.73195, acc 0.3125\n",
      "2018-10-22T23:37:00.756471: step 1215, loss 1.77164, acc 0.328125\n",
      "2018-10-22T23:37:01.283064: step 1216, loss 1.78086, acc 0.296875\n",
      "2018-10-22T23:37:01.823618: step 1217, loss 1.72621, acc 0.34375\n",
      "2018-10-22T23:37:02.291368: step 1218, loss 1.70457, acc 0.351562\n",
      "2018-10-22T23:37:02.729197: step 1219, loss 1.7466, acc 0.351562\n",
      "2018-10-22T23:37:03.182985: step 1220, loss 1.85134, acc 0.273438\n",
      "2018-10-22T23:37:03.645747: step 1221, loss 1.74698, acc 0.328125\n",
      "2018-10-22T23:37:04.188296: step 1222, loss 1.73418, acc 0.351562\n",
      "2018-10-22T23:37:04.655048: step 1223, loss 1.76301, acc 0.3125\n",
      "2018-10-22T23:37:05.122797: step 1224, loss 1.76393, acc 0.3125\n",
      "2018-10-22T23:37:05.555639: step 1225, loss 1.67031, acc 0.453125\n",
      "2018-10-22T23:37:06.050318: step 1226, loss 1.72238, acc 0.335938\n",
      "2018-10-22T23:37:06.516072: step 1227, loss 1.7554, acc 0.304688\n",
      "2018-10-22T23:37:07.103501: step 1228, loss 1.79584, acc 0.335938\n",
      "2018-10-22T23:37:07.599176: step 1229, loss 1.65045, acc 0.351562\n",
      "2018-10-22T23:37:08.078892: step 1230, loss 1.70982, acc 0.398438\n",
      "2018-10-22T23:37:08.512733: step 1231, loss 1.68749, acc 0.351562\n",
      "2018-10-22T23:37:09.023368: step 1232, loss 1.80525, acc 0.28125\n",
      "2018-10-22T23:37:09.479149: step 1233, loss 1.71973, acc 0.398438\n",
      "2018-10-22T23:37:09.910994: step 1234, loss 1.81998, acc 0.304688\n",
      "2018-10-22T23:37:10.366776: step 1235, loss 1.71925, acc 0.34375\n",
      "2018-10-22T23:37:10.804605: step 1236, loss 1.75073, acc 0.335938\n",
      "2018-10-22T23:37:11.240440: step 1237, loss 1.6977, acc 0.335938\n",
      "2018-10-22T23:37:11.699213: step 1238, loss 1.66063, acc 0.359375\n",
      "2018-10-22T23:37:12.190898: step 1239, loss 1.7626, acc 0.320312\n",
      "2018-10-22T23:37:12.731453: step 1240, loss 1.62731, acc 0.390625\n",
      "2018-10-22T23:37:13.174269: step 1241, loss 1.81524, acc 0.304688\n",
      "2018-10-22T23:37:13.703853: step 1242, loss 1.71148, acc 0.3125\n",
      "2018-10-22T23:37:14.147666: step 1243, loss 1.80371, acc 0.320312\n",
      "2018-10-22T23:37:14.604445: step 1244, loss 1.73217, acc 0.375\n",
      "2018-10-22T23:37:15.039282: step 1245, loss 1.72376, acc 0.351562\n",
      "2018-10-22T23:37:15.445197: step 1246, loss 1.71877, acc 0.328125\n",
      "2018-10-22T23:37:15.937879: step 1247, loss 1.80112, acc 0.34375\n",
      "2018-10-22T23:37:16.384685: step 1248, loss 1.83525, acc 0.3125\n",
      "2018-10-22T23:37:16.901305: step 1249, loss 1.7746, acc 0.351562\n",
      "2018-10-22T23:37:17.514663: step 1250, loss 1.70121, acc 0.34375\n",
      "2018-10-22T23:37:17.973437: step 1251, loss 1.74047, acc 0.359375\n",
      "2018-10-22T23:37:18.446173: step 1252, loss 1.65395, acc 0.390625\n",
      "2018-10-22T23:37:18.957805: step 1253, loss 1.71513, acc 0.375\n",
      "2018-10-22T23:37:19.383666: step 1254, loss 1.79079, acc 0.273438\n",
      "2018-10-22T23:37:19.821496: step 1255, loss 1.65072, acc 0.398438\n",
      "2018-10-22T23:37:20.240375: step 1256, loss 1.69978, acc 0.375\n",
      "2018-10-22T23:37:20.747021: step 1257, loss 1.64362, acc 0.375\n",
      "2018-10-22T23:37:21.209783: step 1258, loss 1.61808, acc 0.429688\n",
      "2018-10-22T23:37:21.670552: step 1259, loss 1.73824, acc 0.34375\n",
      "2018-10-22T23:37:22.171214: step 1260, loss 1.73112, acc 0.335938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:37:22.689827: step 1261, loss 1.80947, acc 0.21875\n",
      "2018-10-22T23:37:23.150594: step 1262, loss 1.74467, acc 0.320312\n",
      "2018-10-22T23:37:23.614354: step 1263, loss 1.7979, acc 0.3125\n",
      "2018-10-22T23:37:24.180839: step 1264, loss 1.77691, acc 0.320312\n",
      "2018-10-22T23:37:24.623656: step 1265, loss 1.75622, acc 0.289062\n",
      "2018-10-22T23:37:25.098385: step 1266, loss 1.73781, acc 0.359375\n",
      "2018-10-22T23:37:25.564387: step 1267, loss 1.78259, acc 0.28125\n",
      "2018-10-22T23:37:26.035130: step 1268, loss 1.72603, acc 0.359375\n",
      "2018-10-22T23:37:26.566708: step 1269, loss 1.87429, acc 0.257812\n",
      "2018-10-22T23:37:27.048419: step 1270, loss 1.73203, acc 0.390625\n",
      "2018-10-22T23:37:27.569028: step 1271, loss 1.71461, acc 0.359375\n",
      "2018-10-22T23:37:28.056008: step 1272, loss 1.68855, acc 0.367188\n",
      "2018-10-22T23:37:28.615512: step 1273, loss 1.72, acc 0.3125\n",
      "2018-10-22T23:37:29.035900: step 1274, loss 1.73122, acc 0.304688\n",
      "2018-10-22T23:37:29.597906: step 1275, loss 1.75802, acc 0.304688\n",
      "2018-10-22T23:37:30.043714: step 1276, loss 1.77987, acc 0.328125\n",
      "2018-10-22T23:37:30.483538: step 1277, loss 1.75335, acc 0.328125\n",
      "2018-10-22T23:37:30.973228: step 1278, loss 1.72407, acc 0.351562\n",
      "2018-10-22T23:37:31.429010: step 1279, loss 1.7869, acc 0.328125\n",
      "2018-10-22T23:37:31.881799: step 1280, loss 1.65505, acc 0.359375\n",
      "2018-10-22T23:37:32.332594: step 1281, loss 1.70242, acc 0.359375\n",
      "2018-10-22T23:37:32.781393: step 1282, loss 1.79421, acc 0.289062\n",
      "2018-10-22T23:37:33.267095: step 1283, loss 1.76338, acc 0.3125\n",
      "2018-10-22T23:37:33.770749: step 1284, loss 1.77743, acc 0.359375\n",
      "2018-10-22T23:37:34.201597: step 1285, loss 1.74941, acc 0.3125\n",
      "2018-10-22T23:37:34.697271: step 1286, loss 1.68517, acc 0.398438\n",
      "2018-10-22T23:37:35.293677: step 1287, loss 1.66739, acc 0.351562\n",
      "2018-10-22T23:37:35.900254: step 1288, loss 1.71116, acc 0.304688\n",
      "2018-10-22T23:37:36.344876: step 1289, loss 1.77387, acc 0.335938\n",
      "2018-10-22T23:37:36.840550: step 1290, loss 1.73653, acc 0.382812\n",
      "2018-10-22T23:37:37.352184: step 1291, loss 1.68841, acc 0.390625\n",
      "2018-10-22T23:37:37.877778: step 1292, loss 1.66144, acc 0.398438\n",
      "2018-10-22T23:37:38.298652: step 1293, loss 1.69955, acc 0.367188\n",
      "2018-10-22T23:37:38.750445: step 1294, loss 1.69244, acc 0.367188\n",
      "2018-10-22T23:37:39.201239: step 1295, loss 1.77172, acc 0.296875\n",
      "2018-10-22T23:37:39.652035: step 1296, loss 1.72122, acc 0.40625\n",
      "2018-10-22T23:37:40.082882: step 1297, loss 1.74674, acc 0.367188\n",
      "2018-10-22T23:37:40.550632: step 1298, loss 1.78013, acc 0.335938\n",
      "2018-10-22T23:37:41.135068: step 1299, loss 1.67831, acc 0.429688\n",
      "2018-10-22T23:37:41.706541: step 1300, loss 1.78677, acc 0.34375\n",
      "2018-10-22T23:37:42.222161: step 1301, loss 1.75668, acc 0.328125\n",
      "2018-10-22T23:37:42.683927: step 1302, loss 1.82559, acc 0.304688\n",
      "2018-10-22T23:37:43.240438: step 1303, loss 1.62587, acc 0.4375\n",
      "2018-10-22T23:37:43.703201: step 1304, loss 1.70438, acc 0.382812\n",
      "2018-10-22T23:37:44.186908: step 1305, loss 1.65548, acc 0.398438\n",
      "2018-10-22T23:37:44.615761: step 1306, loss 1.66735, acc 0.398438\n",
      "2018-10-22T23:37:45.135372: step 1307, loss 1.67184, acc 0.351562\n",
      "2018-10-22T23:37:45.568215: step 1308, loss 1.76443, acc 0.367188\n",
      "2018-10-22T23:37:46.037959: step 1309, loss 1.72704, acc 0.351562\n",
      "2018-10-22T23:37:46.473794: step 1310, loss 1.75092, acc 0.289062\n",
      "2018-10-22T23:37:46.912620: step 1311, loss 1.81687, acc 0.304688\n",
      "2018-10-22T23:37:47.364412: step 1312, loss 1.77224, acc 0.320312\n",
      "2018-10-22T23:37:47.931895: step 1313, loss 1.79998, acc 0.257812\n",
      "2018-10-22T23:37:48.436548: step 1314, loss 1.75641, acc 0.34375\n",
      "2018-10-22T23:37:49.031953: step 1315, loss 1.62902, acc 0.429688\n",
      "2018-10-22T23:37:49.601431: step 1316, loss 1.71984, acc 0.367188\n",
      "2018-10-22T23:37:50.055219: step 1317, loss 1.67467, acc 0.382812\n",
      "2018-10-22T23:37:50.523964: step 1318, loss 1.78112, acc 0.335938\n",
      "2018-10-22T23:37:50.994706: step 1319, loss 1.64623, acc 0.375\n",
      "2018-10-22T23:37:51.449489: step 1320, loss 1.80111, acc 0.289062\n",
      "2018-10-22T23:37:51.999020: step 1321, loss 1.76248, acc 0.335938\n",
      "2018-10-22T23:37:52.524615: step 1322, loss 1.66852, acc 0.375\n",
      "2018-10-22T23:37:53.015302: step 1323, loss 1.80772, acc 0.289062\n",
      "2018-10-22T23:37:53.510978: step 1324, loss 1.77886, acc 0.3125\n",
      "2018-10-22T23:37:53.968754: step 1325, loss 1.82124, acc 0.257812\n",
      "2018-10-22T23:37:54.419548: step 1326, loss 1.85226, acc 0.265625\n",
      "2018-10-22T23:37:54.854386: step 1327, loss 1.85429, acc 0.273438\n",
      "2018-10-22T23:37:55.302189: step 1328, loss 1.77599, acc 0.335938\n",
      "2018-10-22T23:37:55.794871: step 1329, loss 1.7897, acc 0.320312\n",
      "2018-10-22T23:37:56.319469: step 1330, loss 1.68582, acc 0.367188\n",
      "2018-10-22T23:37:56.817137: step 1331, loss 1.67798, acc 0.375\n",
      "2018-10-22T23:37:57.255964: step 1332, loss 1.67742, acc 0.320312\n",
      "2018-10-22T23:37:57.671852: step 1333, loss 1.76903, acc 0.289062\n",
      "2018-10-22T23:37:58.127634: step 1334, loss 1.64201, acc 0.398438\n",
      "2018-10-22T23:37:58.548508: step 1335, loss 1.72685, acc 0.3125\n",
      "2018-10-22T23:37:59.074102: step 1336, loss 1.80364, acc 0.34375\n",
      "2018-10-22T23:37:59.541851: step 1337, loss 1.73577, acc 0.3125\n",
      "2018-10-22T23:38:00.025558: step 1338, loss 1.70021, acc 0.335938\n",
      "2018-10-22T23:38:00.545507: step 1339, loss 1.69045, acc 0.382812\n",
      "2018-10-22T23:38:01.053150: step 1340, loss 1.85177, acc 0.304688\n",
      "2018-10-22T23:38:01.534861: step 1341, loss 1.80247, acc 0.25\n",
      "2018-10-22T23:38:01.985656: step 1342, loss 1.749, acc 0.265625\n",
      "2018-10-22T23:38:02.425480: step 1343, loss 1.79284, acc 0.3125\n",
      "2018-10-22T23:38:02.953069: step 1344, loss 1.72432, acc 0.304688\n",
      "2018-10-22T23:38:03.478664: step 1345, loss 1.83657, acc 0.273438\n",
      "2018-10-22T23:38:03.924472: step 1346, loss 1.77109, acc 0.328125\n",
      "2018-10-22T23:38:04.341356: step 1347, loss 1.71088, acc 0.320312\n",
      "2018-10-22T23:38:04.768216: step 1348, loss 1.70388, acc 0.304688\n",
      "2018-10-22T23:38:05.216018: step 1349, loss 1.56285, acc 0.476562\n",
      "2018-10-22T23:38:05.675789: step 1350, loss 1.70747, acc 0.335938\n",
      "2018-10-22T23:38:06.199389: step 1351, loss 1.78206, acc 0.3125\n",
      "2018-10-22T23:38:06.651182: step 1352, loss 1.56983, acc 0.445312\n",
      "2018-10-22T23:38:07.084024: step 1353, loss 1.57689, acc 0.453125\n",
      "2018-10-22T23:38:07.541799: step 1354, loss 1.65731, acc 0.398438\n",
      "2018-10-22T23:38:07.987610: step 1355, loss 1.71005, acc 0.382812\n",
      "2018-10-22T23:38:08.425437: step 1356, loss 1.74042, acc 0.328125\n",
      "2018-10-22T23:38:08.873240: step 1357, loss 1.67732, acc 0.351562\n",
      "2018-10-22T23:38:09.357944: step 1358, loss 1.81632, acc 0.320312\n",
      "2018-10-22T23:38:09.936397: step 1359, loss 1.71731, acc 0.359375\n",
      "2018-10-22T23:38:10.363255: step 1360, loss 1.79644, acc 0.3125\n",
      "2018-10-22T23:38:10.815048: step 1361, loss 1.80395, acc 0.28125\n",
      "2018-10-22T23:38:11.279806: step 1362, loss 1.74381, acc 0.320312\n",
      "2018-10-22T23:38:11.749549: step 1363, loss 1.68213, acc 0.3125\n",
      "2018-10-22T23:38:12.150477: step 1364, loss 1.77108, acc 0.289062\n",
      "2018-10-22T23:38:12.601272: step 1365, loss 1.74727, acc 0.328125\n",
      "2018-10-22T23:38:13.125870: step 1366, loss 1.74452, acc 0.304688\n",
      "2018-10-22T23:38:13.586639: step 1367, loss 1.84561, acc 0.257812\n",
      "2018-10-22T23:38:14.051394: step 1368, loss 1.7434, acc 0.320312\n",
      "2018-10-22T23:38:14.482242: step 1369, loss 1.77555, acc 0.328125\n",
      "2018-10-22T23:38:14.953982: step 1370, loss 1.67903, acc 0.359375\n",
      "2018-10-22T23:38:15.375853: step 1371, loss 1.72279, acc 0.367188\n",
      "2018-10-22T23:38:15.881501: step 1372, loss 1.68639, acc 0.40625\n",
      "2018-10-22T23:38:16.352243: step 1373, loss 1.83502, acc 0.296875\n",
      "2018-10-22T23:38:16.814007: step 1374, loss 1.66687, acc 0.34375\n",
      "2018-10-22T23:38:17.275773: step 1375, loss 1.61206, acc 0.429688\n",
      "2018-10-22T23:38:17.726568: step 1376, loss 1.80002, acc 0.289062\n",
      "2018-10-22T23:38:18.160408: step 1377, loss 1.63394, acc 0.359375\n",
      "2018-10-22T23:38:18.587266: step 1378, loss 1.84897, acc 0.296875\n",
      "2018-10-22T23:38:19.060003: step 1379, loss 1.76555, acc 0.28125\n",
      "2018-10-22T23:38:19.473896: step 1380, loss 1.79185, acc 0.351562\n",
      "2018-10-22T23:38:19.914718: step 1381, loss 1.69535, acc 0.390625\n",
      "2018-10-22T23:38:20.383464: step 1382, loss 1.87964, acc 0.28125\n",
      "2018-10-22T23:38:20.800349: step 1383, loss 1.74974, acc 0.3125\n",
      "2018-10-22T23:38:21.224216: step 1384, loss 1.68077, acc 0.351562\n",
      "2018-10-22T23:38:21.683988: step 1385, loss 1.82209, acc 0.3125\n",
      "2018-10-22T23:38:22.185644: step 1386, loss 1.76471, acc 0.328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:38:22.634446: step 1387, loss 1.79644, acc 0.320312\n",
      "2018-10-22T23:38:23.086237: step 1388, loss 1.67962, acc 0.351562\n",
      "2018-10-22T23:38:23.622802: step 1389, loss 1.73485, acc 0.335938\n",
      "2018-10-22T23:38:24.069608: step 1390, loss 1.75938, acc 0.335938\n",
      "2018-10-22T23:38:24.508435: step 1391, loss 1.8715, acc 0.28125\n",
      "2018-10-22T23:38:24.935293: step 1392, loss 1.74777, acc 0.359375\n",
      "2018-10-22T23:38:25.391074: step 1393, loss 1.77563, acc 0.296875\n",
      "2018-10-22T23:38:25.837880: step 1394, loss 1.79927, acc 0.34375\n",
      "2018-10-22T23:38:26.240803: step 1395, loss 1.7291, acc 0.351562\n",
      "2018-10-22T23:38:26.631757: step 1396, loss 1.63794, acc 0.390625\n",
      "2018-10-22T23:38:27.110477: step 1397, loss 1.75547, acc 0.296875\n",
      "2018-10-22T23:38:27.523373: step 1398, loss 1.74689, acc 0.273438\n",
      "2018-10-22T23:38:27.984142: step 1399, loss 1.69631, acc 0.359375\n",
      "2018-10-22T23:38:28.492781: step 1400, loss 1.62322, acc 0.351562\n",
      "2018-10-22T23:38:28.922631: step 1401, loss 1.72306, acc 0.34375\n",
      "2018-10-22T23:38:29.347496: step 1402, loss 1.69116, acc 0.367188\n",
      "2018-10-22T23:38:29.769368: step 1403, loss 1.79905, acc 0.265625\n",
      "2018-10-22T23:38:30.221162: step 1404, loss 1.65944, acc 0.359375\n",
      "2018-10-22T23:38:30.752740: step 1405, loss 1.67776, acc 0.398438\n",
      "2018-10-22T23:38:31.234450: step 1406, loss 1.73468, acc 0.367188\n",
      "2018-10-22T23:38:31.693225: step 1407, loss 1.70592, acc 0.351562\n",
      "2018-10-22T23:38:32.232781: step 1408, loss 1.79238, acc 0.296875\n",
      "2018-10-22T23:38:32.668616: step 1409, loss 1.67392, acc 0.398438\n",
      "2018-10-22T23:38:33.075528: step 1410, loss 1.75178, acc 0.367188\n",
      "2018-10-22T23:38:33.510366: step 1411, loss 1.70078, acc 0.367188\n",
      "2018-10-22T23:38:33.977117: step 1412, loss 1.76288, acc 0.328125\n",
      "2018-10-22T23:38:34.399987: step 1413, loss 1.80345, acc 0.328125\n",
      "2018-10-22T23:38:34.876712: step 1414, loss 1.70102, acc 0.367188\n",
      "2018-10-22T23:38:35.298584: step 1415, loss 1.74908, acc 0.320312\n",
      "2018-10-22T23:38:35.744392: step 1416, loss 1.70081, acc 0.367188\n",
      "2018-10-22T23:38:36.219122: step 1417, loss 1.78122, acc 0.367188\n",
      "2018-10-22T23:38:36.642989: step 1418, loss 1.62626, acc 0.445312\n",
      "2018-10-22T23:38:37.101762: step 1419, loss 1.80339, acc 0.34375\n",
      "2018-10-22T23:38:37.632344: step 1420, loss 1.73068, acc 0.367188\n",
      "2018-10-22T23:38:38.060200: step 1421, loss 1.6494, acc 0.398438\n",
      "2018-10-22T23:38:38.476088: step 1422, loss 1.76821, acc 0.359375\n",
      "2018-10-22T23:38:38.932866: step 1423, loss 1.65791, acc 0.367188\n",
      "2018-10-22T23:38:39.393634: step 1424, loss 1.78785, acc 0.335938\n",
      "2018-10-22T23:38:39.854403: step 1425, loss 1.65765, acc 0.335938\n",
      "2018-10-22T23:38:40.253337: step 1426, loss 1.71812, acc 0.367188\n",
      "2018-10-22T23:38:40.721085: step 1427, loss 1.80129, acc 0.28125\n",
      "2018-10-22T23:38:41.176867: step 1428, loss 1.7697, acc 0.296875\n",
      "2018-10-22T23:38:41.618686: step 1429, loss 1.72288, acc 0.335938\n",
      "2018-10-22T23:38:42.071475: step 1430, loss 1.7807, acc 0.289062\n",
      "2018-10-22T23:38:42.514290: step 1431, loss 1.82164, acc 0.304688\n",
      "2018-10-22T23:38:43.017944: step 1432, loss 1.73908, acc 0.359375\n",
      "2018-10-22T23:38:43.460760: step 1433, loss 1.71837, acc 0.34375\n",
      "2018-10-22T23:38:43.938484: step 1434, loss 1.76587, acc 0.359375\n",
      "2018-10-22T23:38:44.467069: step 1435, loss 1.79316, acc 0.289062\n",
      "2018-10-22T23:38:44.943795: step 1436, loss 1.68888, acc 0.320312\n",
      "2018-10-22T23:38:45.376637: step 1437, loss 1.71219, acc 0.34375\n",
      "2018-10-22T23:38:45.837405: step 1438, loss 1.74525, acc 0.28125\n",
      "2018-10-22T23:38:46.252296: step 1439, loss 1.68231, acc 0.398438\n",
      "2018-10-22T23:38:46.713064: step 1440, loss 1.62465, acc 0.460938\n",
      "2018-10-22T23:38:47.168845: step 1441, loss 1.69799, acc 0.390625\n",
      "2018-10-22T23:38:47.614653: step 1442, loss 1.78117, acc 0.273438\n",
      "2018-10-22T23:38:48.070435: step 1443, loss 1.641, acc 0.351562\n",
      "2018-10-22T23:38:48.517240: step 1444, loss 1.65668, acc 0.398438\n",
      "2018-10-22T23:38:49.006930: step 1445, loss 1.6504, acc 0.421875\n",
      "2018-10-22T23:38:49.461714: step 1446, loss 1.86874, acc 0.242188\n",
      "2018-10-22T23:38:49.918494: step 1447, loss 1.75114, acc 0.28125\n",
      "2018-10-22T23:38:50.377267: step 1448, loss 1.84637, acc 0.265625\n",
      "2018-10-22T23:38:50.872941: step 1449, loss 1.73819, acc 0.3125\n",
      "2018-10-22T23:38:51.372606: step 1450, loss 1.79592, acc 0.242188\n",
      "2018-10-22T23:38:51.852323: step 1451, loss 1.7619, acc 0.3125\n",
      "2018-10-22T23:38:52.342013: step 1452, loss 1.8111, acc 0.328125\n",
      "2018-10-22T23:38:52.854648: step 1453, loss 1.79392, acc 0.296875\n",
      "2018-10-22T23:38:53.288741: step 1454, loss 1.77771, acc 0.398438\n",
      "2018-10-22T23:38:53.735434: step 1455, loss 1.71324, acc 0.367188\n",
      "2018-10-22T23:38:54.162293: step 1456, loss 1.70057, acc 0.40625\n",
      "2018-10-22T23:38:54.657967: step 1457, loss 1.7054, acc 0.351562\n",
      "2018-10-22T23:38:55.192537: step 1458, loss 1.67702, acc 0.34375\n",
      "2018-10-22T23:38:55.709155: step 1459, loss 1.76409, acc 0.351562\n",
      "2018-10-22T23:38:56.160947: step 1460, loss 1.64946, acc 0.398438\n",
      "2018-10-22T23:38:56.618723: step 1461, loss 1.72494, acc 0.367188\n",
      "2018-10-22T23:38:57.093454: step 1462, loss 1.68317, acc 0.335938\n",
      "2018-10-22T23:38:57.623039: step 1463, loss 1.69436, acc 0.351562\n",
      "2018-10-22T23:38:58.156611: step 1464, loss 1.74972, acc 0.296875\n",
      "2018-10-22T23:38:58.594441: step 1465, loss 1.65725, acc 0.367188\n",
      "2018-10-22T23:38:59.052217: step 1466, loss 1.75359, acc 0.335938\n",
      "2018-10-22T23:38:59.498025: step 1467, loss 1.80952, acc 0.3125\n",
      "2018-10-22T23:38:59.910921: step 1468, loss 1.80882, acc 0.296875\n",
      "2018-10-22T23:39:00.371690: step 1469, loss 1.75667, acc 0.289062\n",
      "2018-10-22T23:39:00.843427: step 1470, loss 1.81442, acc 0.3125\n",
      "2018-10-22T23:39:01.336111: step 1471, loss 1.78242, acc 0.273438\n",
      "2018-10-22T23:39:01.973405: step 1472, loss 1.68076, acc 0.382812\n",
      "2018-10-22T23:39:02.431183: step 1473, loss 1.76999, acc 0.359375\n",
      "2018-10-22T23:39:02.871007: step 1474, loss 1.75114, acc 0.328125\n",
      "2018-10-22T23:39:03.325790: step 1475, loss 1.80569, acc 0.265625\n",
      "2018-10-22T23:39:03.785561: step 1476, loss 1.77545, acc 0.34375\n",
      "2018-10-22T23:39:04.236355: step 1477, loss 1.74834, acc 0.265625\n",
      "2018-10-22T23:39:04.702110: step 1478, loss 1.69872, acc 0.335938\n",
      "2018-10-22T23:39:05.159887: step 1479, loss 1.80782, acc 0.28125\n",
      "2018-10-22T23:39:05.620654: step 1480, loss 1.66446, acc 0.335938\n",
      "2018-10-22T23:39:06.096382: step 1481, loss 1.7565, acc 0.273438\n",
      "2018-10-22T23:39:06.557150: step 1482, loss 1.74228, acc 0.3125\n",
      "2018-10-22T23:39:07.053823: step 1483, loss 1.67767, acc 0.375\n",
      "2018-10-22T23:39:07.477689: step 1484, loss 1.69183, acc 0.34375\n",
      "2018-10-22T23:39:07.894574: step 1485, loss 1.70179, acc 0.304688\n",
      "2018-10-22T23:39:08.334398: step 1486, loss 1.81559, acc 0.195312\n",
      "2018-10-22T23:39:08.819102: step 1487, loss 1.82789, acc 0.3125\n",
      "2018-10-22T23:39:09.243965: step 1488, loss 1.66529, acc 0.3125\n",
      "2018-10-22T23:39:09.684789: step 1489, loss 1.73818, acc 0.328125\n",
      "2018-10-22T23:39:10.142563: step 1490, loss 1.68767, acc 0.359375\n",
      "2018-10-22T23:39:10.595353: step 1491, loss 1.70462, acc 0.34375\n",
      "2018-10-22T23:39:11.044153: step 1492, loss 1.77796, acc 0.328125\n",
      "2018-10-22T23:39:11.501928: step 1493, loss 1.72728, acc 0.351562\n",
      "2018-10-22T23:39:11.971674: step 1494, loss 1.76772, acc 0.34375\n",
      "2018-10-22T23:39:12.461364: step 1495, loss 1.79808, acc 0.34375\n",
      "2018-10-22T23:39:12.941081: step 1496, loss 1.69504, acc 0.375\n",
      "2018-10-22T23:39:13.397860: step 1497, loss 1.67016, acc 0.367188\n",
      "2018-10-22T23:39:13.826713: step 1498, loss 1.60339, acc 0.429688\n",
      "2018-10-22T23:39:14.301444: step 1499, loss 1.69461, acc 0.351562\n",
      "2018-10-22T23:39:14.799113: step 1500, loss 1.75564, acc 0.296875\n",
      "\n",
      "Evaluation:\n",
      "Number of batches in dev set is 62\n",
      "batch 1 in dev >> 2018-10-22T23:39:16.356948: loss 1.77122, acc 0.326\n",
      "batch 2 in dev >> 2018-10-22T23:39:18.112255: loss 1.76221, acc 0.33\n",
      "batch 3 in dev >> 2018-10-22T23:39:19.810713: loss 1.74035, acc 0.32\n",
      "batch 4 in dev >> 2018-10-22T23:39:21.409439: loss 1.72094, acc 0.356\n",
      "batch 5 in dev >> 2018-10-22T23:39:23.211619: loss 1.71745, acc 0.342\n",
      "batch 6 in dev >> 2018-10-22T23:39:24.753497: loss 1.73169, acc 0.348\n",
      "batch 7 in dev >> 2018-10-22T23:39:26.530746: loss 1.74986, acc 0.334\n",
      "batch 8 in dev >> 2018-10-22T23:39:28.088580: loss 1.7083, acc 0.332\n",
      "batch 9 in dev >> 2018-10-22T23:39:29.600537: loss 1.75608, acc 0.304\n",
      "batch 10 in dev >> 2018-10-22T23:39:31.255113: loss 1.78159, acc 0.304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 11 in dev >> 2018-10-22T23:39:32.833892: loss 1.70357, acc 0.362\n",
      "batch 12 in dev >> 2018-10-22T23:39:34.431619: loss 1.72384, acc 0.368\n",
      "batch 13 in dev >> 2018-10-22T23:39:35.933604: loss 1.73989, acc 0.304\n",
      "batch 14 in dev >> 2018-10-22T23:39:37.474485: loss 1.72476, acc 0.376\n",
      "batch 15 in dev >> 2018-10-22T23:39:39.090164: loss 1.72849, acc 0.344\n",
      "batch 16 in dev >> 2018-10-22T23:39:40.714819: loss 1.71622, acc 0.326\n",
      "batch 17 in dev >> 2018-10-22T23:39:42.480099: loss 1.73876, acc 0.326\n",
      "batch 18 in dev >> 2018-10-22T23:39:44.255352: loss 1.79107, acc 0.316\n",
      "batch 19 in dev >> 2018-10-22T23:39:45.939849: loss 1.76688, acc 0.318\n",
      "batch 20 in dev >> 2018-10-22T23:39:47.659254: loss 1.72784, acc 0.35\n",
      "batch 21 in dev >> 2018-10-22T23:39:49.625994: loss 1.71746, acc 0.348\n",
      "batch 22 in dev >> 2018-10-22T23:39:51.325449: loss 1.69152, acc 0.354\n",
      "batch 23 in dev >> 2018-10-22T23:39:53.074772: loss 1.74986, acc 0.322\n",
      "batch 24 in dev >> 2018-10-22T23:39:54.753284: loss 1.65736, acc 0.416\n",
      "batch 25 in dev >> 2018-10-22T23:39:56.409855: loss 1.73291, acc 0.322\n",
      "batch 26 in dev >> 2018-10-22T23:39:58.089363: loss 1.70954, acc 0.374\n",
      "batch 27 in dev >> 2018-10-22T23:39:59.842988: loss 1.75288, acc 0.324\n",
      "batch 28 in dev >> 2018-10-22T23:40:01.518507: loss 1.74888, acc 0.32\n",
      "batch 29 in dev >> 2018-10-22T23:40:03.121221: loss 1.80589, acc 0.322\n",
      "batch 30 in dev >> 2018-10-22T23:40:04.785771: loss 1.72314, acc 0.366\n",
      "batch 31 in dev >> 2018-10-22T23:40:06.337623: loss 1.73272, acc 0.338\n",
      "batch 32 in dev >> 2018-10-22T23:40:08.233552: loss 1.74155, acc 0.316\n",
      "batch 33 in dev >> 2018-10-22T23:40:09.918049: loss 1.78052, acc 0.292\n",
      "batch 34 in dev >> 2018-10-22T23:40:11.445964: loss 1.74865, acc 0.324\n",
      "batch 35 in dev >> 2018-10-22T23:40:13.048677: loss 1.76178, acc 0.326\n",
      "batch 36 in dev >> 2018-10-22T23:40:14.773068: loss 1.77271, acc 0.342\n",
      "batch 37 in dev >> 2018-10-22T23:40:16.495462: loss 1.73303, acc 0.318\n",
      "batch 38 in dev >> 2018-10-22T23:40:18.110145: loss 1.68932, acc 0.356\n",
      "batch 39 in dev >> 2018-10-22T23:40:19.804613: loss 1.76559, acc 0.322\n",
      "batch 40 in dev >> 2018-10-22T23:40:21.375414: loss 1.71193, acc 0.346\n",
      "batch 41 in dev >> 2018-10-22T23:40:23.058912: loss 1.74277, acc 0.348\n",
      "batch 42 in dev >> 2018-10-22T23:40:24.595804: loss 1.76632, acc 0.33\n",
      "batch 43 in dev >> 2018-10-22T23:40:26.274314: loss 1.75691, acc 0.326\n",
      "batch 44 in dev >> 2018-10-22T23:40:27.961804: loss 1.75716, acc 0.352\n",
      "batch 45 in dev >> 2018-10-22T23:40:29.798891: loss 1.743, acc 0.35\n",
      "batch 46 in dev >> 2018-10-22T23:40:31.541232: loss 1.76205, acc 0.314\n",
      "batch 47 in dev >> 2018-10-22T23:40:33.225730: loss 1.70121, acc 0.368\n",
      "batch 48 in dev >> 2018-10-22T23:40:34.799521: loss 1.75308, acc 0.32\n",
      "batch 49 in dev >> 2018-10-22T23:40:36.379298: loss 1.74921, acc 0.34\n",
      "batch 50 in dev >> 2018-10-22T23:40:37.854352: loss 1.77228, acc 0.33\n",
      "batch 51 in dev >> 2018-10-22T23:40:39.483995: loss 1.77641, acc 0.308\n",
      "batch 52 in dev >> 2018-10-22T23:40:41.095686: loss 1.70151, acc 0.342\n",
      "batch 53 in dev >> 2018-10-22T23:40:42.629584: loss 1.66622, acc 0.372\n",
      "batch 54 in dev >> 2018-10-22T23:40:44.283163: loss 1.72596, acc 0.31\n",
      "batch 55 in dev >> 2018-10-22T23:40:45.838005: loss 1.75956, acc 0.292\n",
      "batch 56 in dev >> 2018-10-22T23:40:47.455681: loss 1.71083, acc 0.354\n",
      "batch 57 in dev >> 2018-10-22T23:40:49.057398: loss 1.79011, acc 0.308\n",
      "batch 58 in dev >> 2018-10-22T23:40:50.794754: loss 1.69892, acc 0.352\n",
      "batch 59 in dev >> 2018-10-22T23:40:52.548065: loss 1.71076, acc 0.36\n",
      "batch 60 in dev >> 2018-10-22T23:40:54.299381: loss 1.72283, acc 0.35\n",
      "batch 61 in dev >> 2018-10-22T23:40:55.905088: loss 1.75211, acc 0.32\n",
      "batch 62 in dev >> 2018-10-22T23:40:57.783068: loss 1.77583, acc 0.302\n",
      "\n",
      "Mean accuracy=0.3351935479910143\n",
      "Mean loss=1.7390852628215667\n",
      "\n",
      "2018-10-22T23:40:58.307664: step 1501, loss 1.80445, acc 0.304688\n",
      "2018-10-22T23:40:58.767435: step 1502, loss 1.77923, acc 0.328125\n",
      "2018-10-22T23:40:59.222218: step 1503, loss 1.73972, acc 0.351562\n",
      "2018-10-22T23:40:59.700939: step 1504, loss 1.84128, acc 0.265625\n",
      "2018-10-22T23:41:00.161709: step 1505, loss 1.71091, acc 0.3125\n",
      "2018-10-22T23:41:00.679326: step 1506, loss 1.76233, acc 0.3125\n",
      "2018-10-22T23:41:01.231846: step 1507, loss 1.71275, acc 0.375\n",
      "2018-10-22T23:41:01.750460: step 1508, loss 1.74266, acc 0.367188\n",
      "2018-10-22T23:41:02.198262: step 1509, loss 1.72129, acc 0.320312\n",
      "2018-10-22T23:41:02.633099: step 1510, loss 1.81188, acc 0.304688\n",
      "2018-10-22T23:41:03.087882: step 1511, loss 1.6468, acc 0.421875\n",
      "2018-10-22T23:41:03.512748: step 1512, loss 1.73051, acc 0.304688\n",
      "2018-10-22T23:41:04.048316: step 1513, loss 1.70579, acc 0.414062\n",
      "2018-10-22T23:41:04.485147: step 1514, loss 1.62608, acc 0.382812\n",
      "2018-10-22T23:41:05.115463: step 1515, loss 1.77293, acc 0.367188\n",
      "2018-10-22T23:41:05.716854: step 1516, loss 1.79168, acc 0.335938\n",
      "2018-10-22T23:41:06.315362: step 1517, loss 1.77277, acc 0.304688\n",
      "2018-10-22T23:41:07.097486: step 1518, loss 1.81443, acc 0.34375\n",
      "2018-10-22T23:41:07.670951: step 1519, loss 1.61781, acc 0.382812\n",
      "2018-10-22T23:41:08.135708: step 1520, loss 1.75863, acc 0.351562\n",
      "2018-10-22T23:41:08.585507: step 1521, loss 1.64955, acc 0.359375\n",
      "2018-10-22T23:41:09.049267: step 1522, loss 1.84339, acc 0.28125\n",
      "2018-10-22T23:41:09.500061: step 1523, loss 1.80796, acc 0.304688\n",
      "2018-10-22T23:41:09.930909: step 1524, loss 1.71052, acc 0.359375\n",
      "2018-10-22T23:41:10.334829: step 1525, loss 1.75537, acc 0.328125\n",
      "2018-10-22T23:41:10.847458: step 1526, loss 1.73934, acc 0.367188\n",
      "2018-10-22T23:41:11.367070: step 1527, loss 1.77138, acc 0.335938\n",
      "2018-10-22T23:41:11.817865: step 1528, loss 1.76793, acc 0.304688\n",
      "2018-10-22T23:41:12.342461: step 1529, loss 1.68185, acc 0.34375\n",
      "2018-10-22T23:41:12.873042: step 1530, loss 1.69941, acc 0.351562\n",
      "2018-10-22T23:41:13.318850: step 1531, loss 1.75057, acc 0.320312\n",
      "2018-10-22T23:41:13.791586: step 1532, loss 1.79475, acc 0.296875\n",
      "2018-10-22T23:41:14.218445: step 1533, loss 1.73369, acc 0.320312\n",
      "2018-10-22T23:41:14.719106: step 1534, loss 1.70055, acc 0.335938\n",
      "2018-10-22T23:41:15.145965: step 1535, loss 1.80725, acc 0.328125\n",
      "2018-10-22T23:41:15.645629: step 1536, loss 1.69835, acc 0.328125\n",
      "2018-10-22T23:41:16.143298: step 1537, loss 1.70131, acc 0.328125\n",
      "2018-10-22T23:41:16.558188: step 1538, loss 1.86556, acc 0.273438\n",
      "2018-10-22T23:41:17.032919: step 1539, loss 1.68083, acc 0.40625\n",
      "2018-10-22T23:41:17.468754: step 1540, loss 1.80793, acc 0.289062\n",
      "2018-10-22T23:41:17.969415: step 1541, loss 1.67219, acc 0.320312\n",
      "2018-10-22T23:41:18.452125: step 1542, loss 1.71058, acc 0.359375\n",
      "2018-10-22T23:41:18.902919: step 1543, loss 1.69161, acc 0.335938\n",
      "2018-10-22T23:41:19.350723: step 1544, loss 1.6255, acc 0.421875\n",
      "2018-10-22T23:41:19.843404: step 1545, loss 1.80692, acc 0.28125\n",
      "2018-10-22T23:41:20.301181: step 1546, loss 1.78483, acc 0.320312\n",
      "2018-10-22T23:41:20.722055: step 1547, loss 1.71417, acc 0.367188\n",
      "2018-10-22T23:41:21.175841: step 1548, loss 1.72898, acc 0.359375\n",
      "2018-10-22T23:41:21.749309: step 1549, loss 1.73142, acc 0.328125\n",
      "2018-10-22T23:41:22.229026: step 1550, loss 1.73991, acc 0.34375\n",
      "2018-10-22T23:41:22.800498: step 1551, loss 1.6788, acc 0.34375\n",
      "2018-10-22T23:41:23.315122: step 1552, loss 1.58981, acc 0.429688\n",
      "2018-10-22T23:41:23.895571: step 1553, loss 1.75, acc 0.359375\n",
      "2018-10-22T23:41:24.444103: step 1554, loss 1.71591, acc 0.367188\n",
      "2018-10-22T23:41:25.046494: step 1555, loss 1.72858, acc 0.328125\n",
      "2018-10-22T23:41:25.615970: step 1556, loss 1.67784, acc 0.40625\n",
      "2018-10-22T23:41:26.111644: step 1557, loss 1.73727, acc 0.359375\n",
      "2018-10-22T23:41:26.644221: step 1558, loss 1.74223, acc 0.382812\n",
      "2018-10-22T23:41:27.143884: step 1559, loss 1.72698, acc 0.335938\n",
      "2018-10-22T23:41:27.711368: step 1560, loss 1.82041, acc 0.234375\n",
      "2018-10-22T23:41:28.264887: step 1561, loss 1.82604, acc 0.25\n",
      "2018-10-22T23:41:28.827384: step 1562, loss 1.80863, acc 0.257812\n",
      "2018-10-22T23:41:29.362952: step 1563, loss 1.7678, acc 0.335938\n",
      "2018-10-22T23:41:29.837682: step 1564, loss 1.77495, acc 0.359375\n",
      "2018-10-22T23:41:30.338345: step 1565, loss 1.65273, acc 0.382812\n",
      "2018-10-22T23:41:30.919789: step 1566, loss 1.82611, acc 0.3125\n",
      "2018-10-22T23:41:31.460343: step 1567, loss 1.80673, acc 0.34375\n",
      "2018-10-22T23:41:32.062733: step 1568, loss 1.80571, acc 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:41:32.705015: step 1569, loss 1.76084, acc 0.335938\n",
      "2018-10-22T23:41:33.178834: step 1570, loss 1.71701, acc 0.359375\n",
      "2018-10-22T23:41:33.666529: step 1571, loss 1.77436, acc 0.320312\n",
      "2018-10-22T23:41:34.204092: step 1572, loss 1.71805, acc 0.375\n",
      "2018-10-22T23:41:34.713729: step 1573, loss 1.7622, acc 0.320312\n",
      "2018-10-22T23:41:35.179485: step 1574, loss 1.77002, acc 0.351562\n",
      "2018-10-22T23:41:35.711062: step 1575, loss 1.6992, acc 0.421875\n",
      "2018-10-22T23:41:36.242641: step 1576, loss 1.70851, acc 0.359375\n",
      "2018-10-22T23:41:36.752278: step 1577, loss 1.72779, acc 0.328125\n",
      "2018-10-22T23:41:37.313777: step 1578, loss 1.77135, acc 0.351562\n",
      "2018-10-22T23:41:37.777861: step 1579, loss 1.69451, acc 0.40625\n",
      "2018-10-22T23:41:38.215101: step 1580, loss 1.6737, acc 0.390625\n",
      "2018-10-22T23:41:38.712784: step 1581, loss 1.68859, acc 0.335938\n",
      "2018-10-22T23:41:39.210452: step 1582, loss 1.71169, acc 0.390625\n",
      "2018-10-22T23:41:39.735051: step 1583, loss 1.72728, acc 0.320312\n",
      "2018-10-22T23:41:40.194821: step 1584, loss 1.76109, acc 0.375\n",
      "2018-10-22T23:41:40.651600: step 1585, loss 1.81768, acc 0.257812\n",
      "2018-10-22T23:41:41.097407: step 1586, loss 1.79259, acc 0.289062\n",
      "2018-10-22T23:41:41.643947: step 1587, loss 1.71681, acc 0.34375\n",
      "2018-10-22T23:41:42.152585: step 1588, loss 1.75371, acc 0.398438\n",
      "2018-10-22T23:41:42.693141: step 1589, loss 1.70776, acc 0.34375\n",
      "2018-10-22T23:41:43.203775: step 1590, loss 1.61312, acc 0.390625\n",
      "2018-10-22T23:41:43.633625: step 1591, loss 1.78289, acc 0.359375\n",
      "2018-10-22T23:41:44.103370: step 1592, loss 1.77487, acc 0.320312\n",
      "2018-10-22T23:41:44.564139: step 1593, loss 1.68564, acc 0.351562\n",
      "2018-10-22T23:41:45.077765: step 1594, loss 1.79492, acc 0.273438\n",
      "2018-10-22T23:41:45.619316: step 1595, loss 1.65758, acc 0.375\n",
      "2018-10-22T23:41:46.109007: step 1596, loss 1.66672, acc 0.382812\n",
      "2018-10-22T23:41:46.647567: step 1597, loss 1.72112, acc 0.304688\n",
      "2018-10-22T23:41:47.144240: step 1598, loss 1.77193, acc 0.320312\n",
      "2018-10-22T23:41:47.646895: step 1599, loss 1.75223, acc 0.367188\n",
      "2018-10-22T23:41:48.188447: step 1600, loss 1.71092, acc 0.28125\n",
      "2018-10-22T23:41:48.680133: step 1601, loss 1.77384, acc 0.304688\n",
      "2018-10-22T23:41:49.183785: step 1602, loss 1.75399, acc 0.351562\n",
      "2018-10-22T23:41:49.764235: step 1603, loss 1.69633, acc 0.375\n",
      "2018-10-22T23:41:50.232980: step 1604, loss 1.69354, acc 0.359375\n",
      "2018-10-22T23:41:50.654852: step 1605, loss 1.62727, acc 0.453125\n",
      "2018-10-22T23:41:51.096671: step 1606, loss 1.73007, acc 0.328125\n",
      "2018-10-22T23:41:51.630245: step 1607, loss 1.75498, acc 0.296875\n",
      "2018-10-22T23:41:52.214682: step 1608, loss 1.7824, acc 0.359375\n",
      "2018-10-22T23:41:52.737284: step 1609, loss 1.90219, acc 0.273438\n",
      "2018-10-22T23:41:53.303770: step 1610, loss 1.69463, acc 0.3125\n",
      "2018-10-22T23:41:53.769524: step 1611, loss 1.71116, acc 0.34375\n",
      "2018-10-22T23:41:54.217332: step 1612, loss 1.74348, acc 0.265625\n",
      "2018-10-22T23:41:54.725971: step 1613, loss 1.70797, acc 0.398438\n",
      "2018-10-22T23:41:55.204692: step 1614, loss 1.74669, acc 0.296875\n",
      "2018-10-22T23:41:55.682414: step 1615, loss 1.69602, acc 0.382812\n",
      "2018-10-22T23:41:56.197038: step 1616, loss 1.77612, acc 0.296875\n",
      "2018-10-22T23:41:56.751557: step 1617, loss 1.68912, acc 0.367188\n",
      "2018-10-22T23:41:57.271167: step 1618, loss 1.79026, acc 0.304688\n",
      "2018-10-22T23:41:57.747892: step 1619, loss 1.76616, acc 0.328125\n",
      "2018-10-22T23:41:58.374217: step 1620, loss 1.80142, acc 0.210938\n",
      "2018-10-22T23:41:58.956660: step 1621, loss 1.71803, acc 0.304688\n",
      "2018-10-22T23:41:59.534115: step 1622, loss 1.7047, acc 0.328125\n",
      "2018-10-22T23:42:00.069685: step 1623, loss 1.86041, acc 0.25\n",
      "2018-10-22T23:42:00.734905: step 1624, loss 1.73254, acc 0.320312\n",
      "2018-10-22T23:42:01.196671: step 1625, loss 1.71882, acc 0.375\n",
      "2018-10-22T23:42:01.660431: step 1626, loss 1.71371, acc 0.320312\n",
      "2018-10-22T23:42:02.109230: step 1627, loss 1.73804, acc 0.296875\n",
      "2018-10-22T23:42:02.542073: step 1628, loss 1.67708, acc 0.351562\n",
      "2018-10-22T23:42:02.973918: step 1629, loss 1.65753, acc 0.375\n",
      "2018-10-22T23:42:03.432691: step 1630, loss 1.76091, acc 0.304688\n",
      "2018-10-22T23:42:03.945321: step 1631, loss 1.7807, acc 0.328125\n",
      "2018-10-22T23:42:04.358216: step 1632, loss 1.63683, acc 0.40625\n",
      "2018-10-22T23:42:04.759145: step 1633, loss 1.79492, acc 0.3125\n",
      "2018-10-22T23:42:05.203957: step 1634, loss 1.61738, acc 0.40625\n",
      "2018-10-22T23:42:05.617849: step 1635, loss 1.77727, acc 0.3125\n",
      "2018-10-22T23:42:06.045707: step 1636, loss 1.73599, acc 0.351562\n",
      "2018-10-22T23:42:06.548362: step 1637, loss 1.73265, acc 0.351562\n",
      "2018-10-22T23:42:07.064980: step 1638, loss 1.71247, acc 0.304688\n",
      "2018-10-22T23:42:07.578607: step 1639, loss 1.66686, acc 0.359375\n",
      "2018-10-22T23:42:08.035385: step 1640, loss 1.77051, acc 0.335938\n",
      "2018-10-22T23:42:08.515103: step 1641, loss 1.78716, acc 0.304688\n",
      "2018-10-22T23:42:08.997812: step 1642, loss 1.68841, acc 0.382812\n",
      "2018-10-22T23:42:09.448606: step 1643, loss 1.74084, acc 0.382812\n",
      "2018-10-22T23:42:09.931317: step 1644, loss 1.68396, acc 0.351562\n",
      "2018-10-22T23:42:10.353188: step 1645, loss 1.59352, acc 0.484375\n",
      "2018-10-22T23:42:10.820938: step 1646, loss 1.75828, acc 0.328125\n",
      "2018-10-22T23:42:11.248793: step 1647, loss 1.78896, acc 0.3125\n",
      "2018-10-22T23:42:11.716542: step 1648, loss 1.72181, acc 0.367188\n",
      "2018-10-22T23:42:12.159358: step 1649, loss 1.74077, acc 0.335938\n",
      "2018-10-22T23:42:12.589210: step 1650, loss 1.88847, acc 0.3125\n",
      "2018-10-22T23:42:13.053966: step 1651, loss 1.70565, acc 0.367188\n",
      "2018-10-22T23:42:13.498778: step 1652, loss 1.82083, acc 0.3125\n",
      "2018-10-22T23:42:13.971513: step 1653, loss 1.69947, acc 0.375\n",
      "2018-10-22T23:42:14.570910: step 1654, loss 1.71146, acc 0.367188\n",
      "2018-10-22T23:42:15.187264: step 1655, loss 1.73944, acc 0.3125\n",
      "2018-10-22T23:42:16.124756: step 1656, loss 1.69849, acc 0.414062\n",
      "2018-10-22T23:42:16.761562: step 1657, loss 1.76381, acc 0.34375\n",
      "2018-10-22T23:42:17.238358: step 1658, loss 1.81222, acc 0.3125\n",
      "2018-10-22T23:42:17.782901: step 1659, loss 1.78532, acc 0.328125\n",
      "2018-10-22T23:42:18.314797: step 1660, loss 1.74493, acc 0.289062\n",
      "2018-10-22T23:42:18.778239: step 1661, loss 1.822, acc 0.257812\n",
      "2018-10-22T23:42:19.184153: step 1662, loss 1.74117, acc 0.273438\n",
      "2018-10-22T23:42:19.593061: step 1663, loss 1.76189, acc 0.296875\n",
      "2018-10-22T23:42:20.026901: step 1664, loss 1.70488, acc 0.335938\n",
      "2018-10-22T23:42:20.470714: step 1665, loss 1.63805, acc 0.328125\n",
      "2018-10-22T23:42:20.905551: step 1666, loss 1.65595, acc 0.375\n",
      "2018-10-22T23:42:21.406213: step 1667, loss 1.72914, acc 0.367188\n",
      "2018-10-22T23:42:21.937792: step 1668, loss 1.63985, acc 0.367188\n",
      "2018-10-22T23:42:22.446431: step 1669, loss 1.75881, acc 0.34375\n",
      "2018-10-22T23:42:22.950542: step 1670, loss 1.74302, acc 0.34375\n",
      "2018-10-22T23:42:23.397202: step 1671, loss 1.72857, acc 0.3125\n",
      "2018-10-22T23:42:23.822210: step 1672, loss 1.6994, acc 0.429688\n",
      "2018-10-22T23:42:24.249131: step 1673, loss 1.70182, acc 0.359375\n",
      "2018-10-22T23:42:24.669010: step 1674, loss 1.70201, acc 0.34375\n",
      "2018-10-22T23:42:25.205575: step 1675, loss 1.81777, acc 0.273438\n",
      "2018-10-22T23:42:25.678310: step 1676, loss 1.7473, acc 0.375\n",
      "2018-10-22T23:42:26.346524: step 1677, loss 1.67127, acc 0.398438\n",
      "2018-10-22T23:42:26.985814: step 1678, loss 1.75645, acc 0.296875\n",
      "2018-10-22T23:42:27.783681: step 1679, loss 1.63091, acc 0.40625\n",
      "2018-10-22T23:42:28.256417: step 1680, loss 1.71852, acc 0.34375\n",
      "2018-10-22T23:42:29.157009: step 1681, loss 1.63178, acc 0.4375\n",
      "2018-10-22T23:42:29.595837: step 1682, loss 1.68961, acc 0.359375\n",
      "2018-10-22T23:42:30.052726: step 1683, loss 1.83238, acc 0.3125\n",
      "2018-10-22T23:42:30.513452: step 1684, loss 1.72791, acc 0.304688\n",
      "2018-10-22T23:42:31.066973: step 1685, loss 1.64981, acc 0.351562\n",
      "2018-10-22T23:42:31.606529: step 1686, loss 1.71513, acc 0.390625\n",
      "2018-10-22T23:42:32.085249: step 1687, loss 1.68127, acc 0.367188\n",
      "2018-10-22T23:42:32.888312: step 1688, loss 1.64939, acc 0.359375\n",
      "2018-10-22T23:42:33.431179: step 1689, loss 1.76888, acc 0.273438\n",
      "2018-10-22T23:42:34.049695: step 1690, loss 1.74551, acc 0.34375\n",
      "2018-10-22T23:42:34.566054: step 1691, loss 1.65329, acc 0.367188\n",
      "2018-10-22T23:42:35.031808: step 1692, loss 1.7785, acc 0.320312\n",
      "2018-10-22T23:42:35.472675: step 1693, loss 1.81726, acc 0.3125\n",
      "2018-10-22T23:42:35.953390: step 1694, loss 1.76607, acc 0.335938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:42:36.462030: step 1695, loss 1.74465, acc 0.34375\n",
      "2018-10-22T23:42:36.966681: step 1696, loss 1.70395, acc 0.34375\n",
      "2018-10-22T23:42:37.497262: step 1697, loss 1.68988, acc 0.359375\n",
      "2018-10-22T23:42:37.996926: step 1698, loss 1.79242, acc 0.328125\n",
      "2018-10-22T23:42:38.437747: step 1699, loss 1.7315, acc 0.367188\n",
      "2018-10-22T23:42:38.904500: step 1700, loss 1.64305, acc 0.398438\n",
      "2018-10-22T23:42:39.349311: step 1701, loss 1.7304, acc 0.375\n",
      "2018-10-22T23:42:39.879891: step 1702, loss 1.71742, acc 0.3125\n",
      "2018-10-22T23:42:40.379556: step 1703, loss 1.73996, acc 0.320312\n",
      "2018-10-22T23:42:40.804419: step 1704, loss 1.73822, acc 0.398438\n",
      "2018-10-22T23:42:41.228286: step 1705, loss 1.69307, acc 0.3125\n",
      "2018-10-22T23:42:41.630212: step 1706, loss 1.74881, acc 0.351562\n",
      "2018-10-22T23:42:42.052083: step 1707, loss 1.82253, acc 0.28125\n",
      "2018-10-22T23:42:42.521827: step 1708, loss 1.7813, acc 0.328125\n",
      "2018-10-22T23:42:43.067369: step 1709, loss 1.70958, acc 0.398438\n",
      "2018-10-22T23:42:43.695689: step 1710, loss 1.7526, acc 0.34375\n",
      "2018-10-22T23:42:44.162441: step 1711, loss 1.80979, acc 0.273438\n",
      "2018-10-22T23:42:44.622211: step 1712, loss 1.75538, acc 0.328125\n",
      "2018-10-22T23:42:45.088963: step 1713, loss 1.78234, acc 0.304688\n",
      "2018-10-22T23:42:45.570675: step 1714, loss 1.62948, acc 0.429688\n",
      "2018-10-22T23:42:46.023465: step 1715, loss 1.81859, acc 0.3125\n",
      "2018-10-22T23:42:46.480243: step 1716, loss 1.81806, acc 0.335938\n",
      "2018-10-22T23:42:46.999855: step 1717, loss 1.76488, acc 0.304688\n",
      "2018-10-22T23:42:47.463613: step 1718, loss 1.71688, acc 0.382812\n",
      "2018-10-22T23:42:47.904435: step 1719, loss 1.66363, acc 0.40625\n",
      "2018-10-22T23:42:48.352238: step 1720, loss 1.70602, acc 0.351562\n",
      "2018-10-22T23:42:48.837939: step 1721, loss 1.70887, acc 0.359375\n",
      "2018-10-22T23:42:49.283123: step 1722, loss 1.79215, acc 0.289062\n",
      "2018-10-22T23:42:49.712031: step 1723, loss 1.57203, acc 0.421875\n",
      "2018-10-22T23:42:50.134900: step 1724, loss 1.71579, acc 0.3125\n",
      "2018-10-22T23:42:50.589685: step 1725, loss 1.65074, acc 0.421875\n",
      "2018-10-22T23:42:51.056437: step 1726, loss 1.78896, acc 0.328125\n",
      "2018-10-22T23:42:51.514212: step 1727, loss 1.78472, acc 0.328125\n",
      "2018-10-22T23:42:52.080698: step 1728, loss 1.70895, acc 0.382812\n",
      "2018-10-22T23:42:52.632223: step 1729, loss 1.77351, acc 0.335938\n",
      "2018-10-22T23:42:53.079029: step 1730, loss 1.71968, acc 0.335938\n",
      "2018-10-22T23:42:53.495914: step 1731, loss 1.77229, acc 0.273438\n",
      "2018-10-22T23:42:54.036469: step 1732, loss 1.72078, acc 0.359375\n",
      "2018-10-22T23:42:54.503220: step 1733, loss 1.73424, acc 0.34375\n",
      "2018-10-22T23:42:55.029812: step 1734, loss 1.74042, acc 0.304688\n",
      "2018-10-22T23:42:55.514516: step 1735, loss 1.69009, acc 0.359375\n",
      "2018-10-22T23:42:55.986256: step 1736, loss 1.79911, acc 0.335938\n",
      "2018-10-22T23:42:56.487913: step 1737, loss 1.6601, acc 0.40625\n",
      "2018-10-22T23:42:56.968629: step 1738, loss 1.72964, acc 0.3125\n",
      "2018-10-22T23:42:57.551073: step 1739, loss 1.71188, acc 0.296875\n",
      "2018-10-22T23:42:58.082651: step 1740, loss 1.75563, acc 0.3125\n",
      "2018-10-22T23:42:58.580320: step 1741, loss 1.71405, acc 0.375\n",
      "2018-10-22T23:42:59.038095: step 1742, loss 1.77632, acc 0.335938\n",
      "2018-10-22T23:42:59.512826: step 1743, loss 1.67392, acc 0.382812\n",
      "2018-10-22T23:43:00.001519: step 1744, loss 1.69281, acc 0.398438\n",
      "2018-10-22T23:43:00.485227: step 1745, loss 1.76556, acc 0.3125\n",
      "2018-10-22T23:43:01.051713: step 1746, loss 1.6813, acc 0.421875\n",
      "2018-10-22T23:43:01.587279: step 1747, loss 1.68294, acc 0.382812\n",
      "2018-10-22T23:43:02.084949: step 1748, loss 1.69848, acc 0.40625\n",
      "2018-10-22T23:43:02.505823: step 1749, loss 1.63101, acc 0.421875\n",
      "2018-10-22T23:43:02.957615: step 1750, loss 1.79988, acc 0.3125\n",
      "2018-10-22T23:43:03.388464: step 1751, loss 1.76846, acc 0.304688\n",
      "2018-10-22T23:43:03.857210: step 1752, loss 1.73335, acc 0.382812\n",
      "2018-10-22T23:43:04.288058: step 1753, loss 1.77895, acc 0.289062\n",
      "2018-10-22T23:43:04.781739: step 1754, loss 1.71562, acc 0.304688\n",
      "2018-10-22T23:43:05.241508: step 1755, loss 1.70103, acc 0.375\n",
      "2018-10-22T23:43:05.718235: step 1756, loss 1.80002, acc 0.328125\n",
      "2018-10-22T23:43:06.230863: step 1757, loss 1.65835, acc 0.429688\n",
      "2018-10-22T23:43:06.709583: step 1758, loss 1.53127, acc 0.445312\n",
      "2018-10-22T23:43:07.196282: step 1759, loss 1.69464, acc 0.34375\n",
      "2018-10-22T23:43:07.641094: step 1760, loss 1.73831, acc 0.328125\n",
      "2018-10-22T23:43:08.136293: step 1761, loss 1.62964, acc 0.367188\n",
      "2018-10-22T23:43:08.548510: step 1762, loss 1.71212, acc 0.335938\n",
      "2018-10-22T23:43:08.973373: step 1763, loss 1.6263, acc 0.414062\n",
      "2018-10-22T23:43:09.424169: step 1764, loss 1.69924, acc 0.390625\n",
      "2018-10-22T23:43:09.922835: step 1765, loss 1.78158, acc 0.335938\n",
      "2018-10-22T23:43:10.349695: step 1766, loss 1.76009, acc 0.296875\n",
      "2018-10-22T23:43:10.806473: step 1767, loss 1.63426, acc 0.414062\n",
      "2018-10-22T23:43:11.314116: step 1768, loss 1.83281, acc 0.28125\n",
      "2018-10-22T23:43:11.831732: step 1769, loss 1.71562, acc 0.34375\n",
      "2018-10-22T23:43:12.253604: step 1770, loss 1.86151, acc 0.210938\n",
      "2018-10-22T23:43:12.701406: step 1771, loss 1.76282, acc 0.335938\n",
      "2018-10-22T23:43:13.130260: step 1772, loss 1.72488, acc 0.40625\n",
      "2018-10-22T23:43:13.541162: step 1773, loss 1.6309, acc 0.429688\n",
      "2018-10-22T23:43:13.998937: step 1774, loss 1.61956, acc 0.414062\n",
      "2018-10-22T23:43:14.503587: step 1775, loss 1.63836, acc 0.3125\n",
      "2018-10-22T23:43:14.974328: step 1776, loss 1.73095, acc 0.320312\n",
      "2018-10-22T23:43:15.463022: step 1777, loss 1.80543, acc 0.273438\n",
      "2018-10-22T23:43:16.022527: step 1778, loss 1.70825, acc 0.328125\n",
      "2018-10-22T23:43:16.549119: step 1779, loss 1.79133, acc 0.304688\n",
      "2018-10-22T23:43:17.071721: step 1780, loss 1.68342, acc 0.359375\n",
      "2018-10-22T23:43:17.568503: step 1781, loss 1.69824, acc 0.328125\n",
      "2018-10-22T23:43:18.148951: step 1782, loss 1.67706, acc 0.382812\n",
      "2018-10-22T23:43:18.751342: step 1783, loss 1.76305, acc 0.273438\n",
      "2018-10-22T23:43:19.241032: step 1784, loss 1.71743, acc 0.34375\n",
      "2018-10-22T23:43:19.800535: step 1785, loss 1.62234, acc 0.460938\n",
      "2018-10-22T23:43:20.372007: step 1786, loss 1.68766, acc 0.34375\n",
      "2018-10-22T23:43:20.848732: step 1787, loss 1.57574, acc 0.4375\n",
      "2018-10-22T23:43:21.302520: step 1788, loss 1.67967, acc 0.359375\n",
      "2018-10-22T23:43:21.891943: step 1789, loss 1.67643, acc 0.375\n",
      "2018-10-22T23:43:22.441473: step 1790, loss 1.70991, acc 0.390625\n",
      "2018-10-22T23:43:22.984023: step 1791, loss 1.74866, acc 0.335938\n",
      "2018-10-22T23:43:23.452769: step 1792, loss 1.64997, acc 0.46875\n",
      "2018-10-22T23:43:23.885613: step 1793, loss 1.57623, acc 0.46875\n",
      "2018-10-22T23:43:24.357352: step 1794, loss 1.7493, acc 0.335938\n",
      "2018-10-22T23:43:24.892919: step 1795, loss 1.76118, acc 0.328125\n",
      "2018-10-22T23:43:25.467383: step 1796, loss 1.76593, acc 0.320312\n",
      "2018-10-22T23:43:26.003868: step 1797, loss 1.76242, acc 0.351562\n",
      "2018-10-22T23:43:26.448678: step 1798, loss 1.71169, acc 0.304688\n",
      "2018-10-22T23:43:26.884513: step 1799, loss 1.79276, acc 0.34375\n",
      "2018-10-22T23:43:27.314363: step 1800, loss 1.7992, acc 0.273438\n",
      "2018-10-22T23:43:27.798071: step 1801, loss 1.69679, acc 0.375\n",
      "2018-10-22T23:43:28.253852: step 1802, loss 1.80329, acc 0.28125\n",
      "2018-10-22T23:43:28.712626: step 1803, loss 1.73997, acc 0.328125\n",
      "2018-10-22T23:43:29.204311: step 1804, loss 1.72422, acc 0.34375\n",
      "2018-10-22T23:43:29.639148: step 1805, loss 1.65522, acc 0.351562\n",
      "2018-10-22T23:43:30.091937: step 1806, loss 1.71811, acc 0.328125\n",
      "2018-10-22T23:43:30.514806: step 1807, loss 1.72977, acc 0.359375\n",
      "2018-10-22T23:43:30.988541: step 1808, loss 1.67391, acc 0.421875\n",
      "2018-10-22T23:43:31.423377: step 1809, loss 1.7198, acc 0.351562\n",
      "2018-10-22T23:43:31.895116: step 1810, loss 1.79893, acc 0.304688\n",
      "2018-10-22T23:43:32.392786: step 1811, loss 1.73424, acc 0.304688\n",
      "2018-10-22T23:43:32.916385: step 1812, loss 1.71411, acc 0.320312\n",
      "2018-10-22T23:43:33.353218: step 1813, loss 1.77498, acc 0.289062\n",
      "2018-10-22T23:43:33.809997: step 1814, loss 1.73667, acc 0.382812\n",
      "2018-10-22T23:43:34.228876: step 1815, loss 1.67925, acc 0.367188\n",
      "2018-10-22T23:43:34.674684: step 1816, loss 1.7387, acc 0.3125\n",
      "2018-10-22T23:43:35.142433: step 1817, loss 1.81323, acc 0.28125\n",
      "2018-10-22T23:43:35.576273: step 1818, loss 1.62613, acc 0.335938\n",
      "2018-10-22T23:43:36.081921: step 1819, loss 1.66309, acc 0.359375\n",
      "2018-10-22T23:43:36.565628: step 1820, loss 1.60345, acc 0.460938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:43:37.014428: step 1821, loss 1.76851, acc 0.289062\n",
      "2018-10-22T23:43:37.470209: step 1822, loss 1.60016, acc 0.40625\n",
      "2018-10-22T23:43:37.920007: step 1823, loss 1.67421, acc 0.390625\n",
      "2018-10-22T23:43:38.400721: step 1824, loss 1.75082, acc 0.3125\n",
      "2018-10-22T23:43:38.887420: step 1825, loss 1.75251, acc 0.34375\n",
      "2018-10-22T23:43:39.320264: step 1826, loss 1.65635, acc 0.390625\n",
      "2018-10-22T23:43:39.856828: step 1827, loss 1.74511, acc 0.289062\n",
      "2018-10-22T23:43:40.409351: step 1828, loss 1.68973, acc 0.320312\n",
      "2018-10-22T23:43:40.933949: step 1829, loss 1.68937, acc 0.429688\n",
      "2018-10-22T23:43:41.477494: step 1830, loss 1.76025, acc 0.375\n",
      "2018-10-22T23:43:42.110801: step 1831, loss 1.69353, acc 0.382812\n",
      "2018-10-22T23:43:42.608471: step 1832, loss 1.77431, acc 0.335938\n",
      "2018-10-22T23:43:43.147031: step 1833, loss 1.7174, acc 0.335938\n",
      "2018-10-22T23:43:43.606801: step 1834, loss 1.68945, acc 0.34375\n",
      "2018-10-22T23:43:44.073554: step 1835, loss 1.59968, acc 0.40625\n",
      "2018-10-22T23:43:44.521356: step 1836, loss 1.69973, acc 0.367188\n",
      "2018-10-22T23:43:45.056924: step 1837, loss 1.57541, acc 0.421875\n",
      "2018-10-22T23:43:45.583516: step 1838, loss 1.58953, acc 0.414062\n",
      "2018-10-22T23:43:46.057249: step 1839, loss 1.69974, acc 0.351562\n",
      "2018-10-22T23:43:46.608774: step 1840, loss 1.76331, acc 0.296875\n",
      "2018-10-22T23:43:47.157309: step 1841, loss 1.76867, acc 0.34375\n",
      "2018-10-22T23:43:47.653980: step 1842, loss 1.7181, acc 0.375\n",
      "2018-10-22T23:43:48.210491: step 1843, loss 1.71063, acc 0.351562\n",
      "2018-10-22T23:43:48.645329: step 1844, loss 1.95273, acc 0.242188\n",
      "2018-10-22T23:43:49.183889: step 1845, loss 1.58565, acc 0.414062\n",
      "2018-10-22T23:43:49.684550: step 1846, loss 1.66283, acc 0.375\n",
      "2018-10-22T23:43:50.175239: step 1847, loss 1.68849, acc 0.320312\n",
      "2018-10-22T23:43:50.698838: step 1848, loss 1.68851, acc 0.382812\n",
      "2018-10-22T23:43:51.182545: step 1849, loss 1.66249, acc 0.367188\n",
      "2018-10-22T23:43:51.676226: step 1850, loss 1.81005, acc 0.367188\n",
      "2018-10-22T23:43:52.234732: step 1851, loss 1.77694, acc 0.304688\n",
      "2018-10-22T23:43:52.743670: step 1852, loss 1.6505, acc 0.40625\n",
      "2018-10-22T23:43:53.199483: step 1853, loss 1.74591, acc 0.398438\n",
      "2018-10-22T23:43:53.667233: step 1854, loss 1.70206, acc 0.382812\n",
      "2018-10-22T23:43:54.094092: step 1855, loss 1.63048, acc 0.382812\n",
      "2018-10-22T23:43:54.617692: step 1856, loss 1.62376, acc 0.390625\n",
      "2018-10-22T23:43:55.066492: step 1857, loss 1.71974, acc 0.328125\n",
      "2018-10-22T23:43:55.746674: step 1858, loss 1.63666, acc 0.414062\n",
      "2018-10-22T23:43:56.372001: step 1859, loss 1.72232, acc 0.40625\n",
      "2018-10-22T23:43:56.904447: step 1860, loss 1.80004, acc 0.289062\n",
      "2018-10-22T23:43:57.459961: step 1861, loss 1.67082, acc 0.390625\n",
      "2018-10-22T23:43:58.000516: step 1862, loss 1.74973, acc 0.335938\n",
      "2018-10-22T23:43:58.511150: step 1863, loss 1.76984, acc 0.328125\n",
      "2018-10-22T23:43:59.015801: step 1864, loss 1.81924, acc 0.304688\n",
      "2018-10-22T23:43:59.543390: step 1865, loss 1.72442, acc 0.359375\n",
      "2018-10-22T23:44:00.127828: step 1866, loss 1.75077, acc 0.328125\n",
      "2018-10-22T23:44:00.804021: step 1867, loss 1.75377, acc 0.320312\n",
      "2018-10-22T23:44:01.256809: step 1868, loss 1.79369, acc 0.289062\n",
      "2018-10-22T23:44:01.783401: step 1869, loss 1.73008, acc 0.320312\n",
      "2018-10-22T23:44:02.365843: step 1870, loss 1.73182, acc 0.34375\n",
      "2018-10-22T23:44:03.125811: step 1871, loss 1.74566, acc 0.320312\n",
      "2018-10-22T23:44:03.583587: step 1872, loss 1.78416, acc 0.296875\n",
      "2018-10-22T23:44:04.033385: step 1873, loss 1.65893, acc 0.40625\n",
      "2018-10-22T23:44:04.507118: step 1874, loss 1.74015, acc 0.328125\n",
      "2018-10-22T23:44:04.994816: step 1875, loss 1.77144, acc 0.34375\n",
      "2018-10-22T23:44:05.710960: step 1876, loss 1.66545, acc 0.390625\n",
      "2018-10-22T23:44:06.350924: step 1877, loss 1.70971, acc 0.320312\n",
      "2018-10-22T23:44:06.902448: step 1878, loss 1.66542, acc 0.390625\n",
      "2018-10-22T23:44:07.374187: step 1879, loss 1.67847, acc 0.335938\n",
      "2018-10-22T23:44:07.900779: step 1880, loss 1.71971, acc 0.34375\n",
      "2018-10-22T23:44:08.332625: step 1881, loss 1.67374, acc 0.335938\n",
      "2018-10-22T23:44:08.774443: step 1882, loss 1.63936, acc 0.40625\n",
      "2018-10-22T23:44:09.190331: step 1883, loss 1.71381, acc 0.320312\n",
      "2018-10-22T23:44:09.658081: step 1884, loss 1.75262, acc 0.328125\n",
      "2018-10-22T23:44:10.119846: step 1885, loss 1.58006, acc 0.4375\n",
      "2018-10-22T23:44:10.565654: step 1886, loss 1.75731, acc 0.34375\n",
      "2018-10-22T23:44:11.051355: step 1887, loss 1.71567, acc 0.328125\n",
      "2018-10-22T23:44:11.573958: step 1888, loss 1.63655, acc 0.359375\n",
      "2018-10-22T23:44:12.028742: step 1889, loss 1.65802, acc 0.414062\n",
      "2018-10-22T23:44:12.490507: step 1890, loss 1.67346, acc 0.351562\n",
      "2018-10-22T23:44:12.953270: step 1891, loss 1.63875, acc 0.398438\n",
      "2018-10-22T23:44:13.356192: step 1892, loss 1.68067, acc 0.390625\n",
      "2018-10-22T23:44:13.802000: step 1893, loss 1.63645, acc 0.429688\n",
      "2018-10-22T23:44:14.284710: step 1894, loss 1.79448, acc 0.328125\n",
      "2018-10-22T23:44:14.860171: step 1895, loss 1.78403, acc 0.359375\n",
      "2018-10-22T23:44:15.309968: step 1896, loss 1.6813, acc 0.304688\n",
      "2018-10-22T23:44:15.799660: step 1897, loss 1.65848, acc 0.367188\n",
      "2018-10-22T23:44:16.234496: step 1898, loss 1.82683, acc 0.3125\n",
      "2018-10-22T23:44:16.667338: step 1899, loss 1.61629, acc 0.382812\n",
      "2018-10-22T23:44:17.093201: step 1900, loss 1.69295, acc 0.34375\n",
      "2018-10-22T23:44:17.546987: step 1901, loss 1.74522, acc 0.328125\n",
      "2018-10-22T23:44:18.072582: step 1902, loss 1.76008, acc 0.351562\n",
      "2018-10-22T23:44:18.577233: step 1903, loss 1.79866, acc 0.328125\n",
      "2018-10-22T23:44:19.031019: step 1904, loss 1.70642, acc 0.3125\n",
      "2018-10-22T23:44:19.474833: step 1905, loss 1.6924, acc 0.382812\n",
      "2018-10-22T23:44:19.921639: step 1906, loss 1.75928, acc 0.34375\n",
      "2018-10-22T23:44:20.403350: step 1907, loss 1.78124, acc 0.273438\n",
      "2018-10-22T23:44:20.878081: step 1908, loss 1.74842, acc 0.304688\n",
      "2018-10-22T23:44:21.308928: step 1909, loss 1.80757, acc 0.25\n",
      "2018-10-22T23:44:21.879404: step 1910, loss 1.66761, acc 0.382812\n",
      "2018-10-22T23:44:22.350144: step 1911, loss 1.64203, acc 0.382812\n",
      "2018-10-22T23:44:22.809915: step 1912, loss 1.78248, acc 0.296875\n",
      "2018-10-22T23:44:23.258715: step 1913, loss 1.70899, acc 0.40625\n",
      "2018-10-22T23:44:23.751398: step 1914, loss 1.68615, acc 0.375\n",
      "2018-10-22T23:44:24.198203: step 1915, loss 1.78243, acc 0.34375\n",
      "2018-10-22T23:44:24.665953: step 1916, loss 1.82246, acc 0.273438\n",
      "2018-10-22T23:44:25.189554: step 1917, loss 1.66951, acc 0.34375\n",
      "2018-10-22T23:44:25.630373: step 1918, loss 1.61635, acc 0.414062\n",
      "2018-10-22T23:44:26.089147: step 1919, loss 1.77999, acc 0.304688\n",
      "2018-10-22T23:44:26.543931: step 1920, loss 1.67506, acc 0.359375\n",
      "2018-10-22T23:44:27.035616: step 1921, loss 1.80757, acc 0.304688\n",
      "2018-10-22T23:44:27.510347: step 1922, loss 1.67479, acc 0.3125\n",
      "2018-10-22T23:44:28.155623: step 1923, loss 1.70431, acc 0.335938\n",
      "2018-10-22T23:44:28.768983: step 1924, loss 1.74848, acc 0.359375\n",
      "2018-10-22T23:44:29.285600: step 1925, loss 1.6934, acc 0.34375\n",
      "2018-10-22T23:44:29.719440: step 1926, loss 1.75781, acc 0.296875\n",
      "2018-10-22T23:44:30.171233: step 1927, loss 1.67831, acc 0.414062\n",
      "2018-10-22T23:44:30.692839: step 1928, loss 1.71706, acc 0.335938\n",
      "2018-10-22T23:44:31.282265: step 1929, loss 1.66085, acc 0.382812\n",
      "2018-10-22T23:44:31.928539: step 1930, loss 1.67304, acc 0.3125\n",
      "2018-10-22T23:44:32.417232: step 1931, loss 1.78426, acc 0.304688\n",
      "2018-10-22T23:44:32.960779: step 1932, loss 1.75563, acc 0.335938\n",
      "2018-10-22T23:44:33.392625: step 1933, loss 1.60552, acc 0.398438\n",
      "2018-10-22T23:44:33.854390: step 1934, loss 1.62817, acc 0.398438\n",
      "2018-10-22T23:44:34.276262: step 1935, loss 1.68241, acc 0.34375\n",
      "2018-10-22T23:44:34.746006: step 1936, loss 1.79873, acc 0.28125\n",
      "2018-10-22T23:44:35.189819: step 1937, loss 1.8759, acc 0.304688\n",
      "2018-10-22T23:44:35.768272: step 1938, loss 1.70083, acc 0.367188\n",
      "2018-10-22T23:44:36.258961: step 1939, loss 1.70253, acc 0.375\n",
      "2018-10-22T23:44:36.718730: step 1940, loss 1.6979, acc 0.40625\n",
      "2018-10-22T23:44:37.220390: step 1941, loss 1.76951, acc 0.351562\n",
      "2018-10-22T23:44:37.690135: step 1942, loss 1.54556, acc 0.421875\n",
      "2018-10-22T23:44:38.133947: step 1943, loss 1.75073, acc 0.34375\n",
      "2018-10-22T23:44:38.655553: step 1944, loss 1.65667, acc 0.351562\n",
      "2018-10-22T23:44:39.238993: step 1945, loss 1.85324, acc 0.335938\n",
      "2018-10-22T23:44:39.723697: step 1946, loss 1.67818, acc 0.390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:44:40.230341: step 1947, loss 1.73597, acc 0.328125\n",
      "2018-10-22T23:44:40.672161: step 1948, loss 1.67229, acc 0.421875\n",
      "2018-10-22T23:44:41.118965: step 1949, loss 1.75952, acc 0.3125\n",
      "2018-10-22T23:44:41.543830: step 1950, loss 1.58554, acc 0.390625\n",
      "2018-10-22T23:44:42.058454: step 1951, loss 1.77314, acc 0.335938\n",
      "2018-10-22T23:44:42.550139: step 1952, loss 1.76766, acc 0.320312\n",
      "2018-10-22T23:44:43.075734: step 1953, loss 1.76816, acc 0.351562\n",
      "2018-10-22T23:44:43.546475: step 1954, loss 1.63244, acc 0.390625\n",
      "2018-10-22T23:44:44.061099: step 1955, loss 1.65784, acc 0.359375\n",
      "2018-10-22T23:44:44.483967: step 1956, loss 1.71045, acc 0.382812\n",
      "2018-10-22T23:44:44.885893: step 1957, loss 1.7419, acc 0.335938\n",
      "2018-10-22T23:44:45.356636: step 1958, loss 1.63653, acc 0.398438\n",
      "2018-10-22T23:44:45.855302: step 1959, loss 1.65258, acc 0.359375\n",
      "2018-10-22T23:44:46.376907: step 1960, loss 1.70372, acc 0.375\n",
      "2018-10-22T23:44:46.812741: step 1961, loss 1.71494, acc 0.320312\n",
      "2018-10-22T23:44:47.302433: step 1962, loss 1.6616, acc 0.375\n",
      "2018-10-22T23:44:47.809078: step 1963, loss 1.78717, acc 0.265625\n",
      "2018-10-22T23:44:48.305750: step 1964, loss 1.70643, acc 0.390625\n",
      "2018-10-22T23:44:48.760534: step 1965, loss 1.81447, acc 0.296875\n",
      "2018-10-22T23:44:49.242423: step 1966, loss 1.81513, acc 0.296875\n",
      "2018-10-22T23:44:49.762619: step 1967, loss 1.75735, acc 0.28125\n",
      "2018-10-22T23:44:50.224384: step 1968, loss 1.71862, acc 0.34375\n",
      "2018-10-22T23:44:50.676176: step 1969, loss 1.74662, acc 0.359375\n",
      "2018-10-22T23:44:51.099046: step 1970, loss 1.68266, acc 0.351562\n",
      "2018-10-22T23:44:51.554827: step 1971, loss 1.71797, acc 0.320312\n",
      "2018-10-22T23:44:52.188134: step 1972, loss 1.71898, acc 0.320312\n",
      "2018-10-22T23:44:52.725696: step 1973, loss 1.70136, acc 0.328125\n",
      "2018-10-22T23:44:53.307141: step 1974, loss 1.77523, acc 0.304688\n",
      "2018-10-22T23:44:53.769904: step 1975, loss 1.68339, acc 0.328125\n",
      "2018-10-22T23:44:54.205740: step 1976, loss 1.69927, acc 0.3125\n",
      "2018-10-22T23:44:54.641573: step 1977, loss 1.70835, acc 0.359375\n",
      "2018-10-22T23:44:55.129270: step 1978, loss 1.65505, acc 0.359375\n",
      "2018-10-22T23:44:55.619957: step 1979, loss 1.69402, acc 0.390625\n",
      "2018-10-22T23:44:56.069755: step 1980, loss 1.6674, acc 0.375\n",
      "2018-10-22T23:44:56.688103: step 1981, loss 1.72417, acc 0.328125\n",
      "2018-10-22T23:44:57.219679: step 1982, loss 1.68471, acc 0.375\n",
      "2018-10-22T23:44:57.810102: step 1983, loss 1.67975, acc 0.34375\n",
      "2018-10-22T23:44:58.658832: step 1984, loss 1.88056, acc 0.28125\n",
      "2018-10-22T23:44:59.131568: step 1985, loss 1.80071, acc 0.328125\n",
      "2018-10-22T23:44:59.772853: step 1986, loss 1.77771, acc 0.289062\n",
      "2018-10-22T23:45:00.270523: step 1987, loss 1.78163, acc 0.328125\n",
      "2018-10-22T23:45:00.745253: step 1988, loss 1.67932, acc 0.40625\n",
      "2018-10-22T23:45:01.236938: step 1989, loss 1.74143, acc 0.367188\n",
      "2018-10-22T23:45:01.743584: step 1990, loss 1.69911, acc 0.34375\n",
      "2018-10-22T23:45:02.184405: step 1991, loss 1.86813, acc 0.265625\n",
      "2018-10-22T23:45:02.623233: step 1992, loss 1.73793, acc 0.335938\n",
      "2018-10-22T23:45:03.079013: step 1993, loss 1.59277, acc 0.460938\n",
      "2018-10-22T23:45:03.520832: step 1994, loss 1.63481, acc 0.375\n",
      "2018-10-22T23:45:04.073355: step 1995, loss 1.71837, acc 0.367188\n",
      "2018-10-22T23:45:04.533125: step 1996, loss 1.75578, acc 0.3125\n",
      "2018-10-22T23:45:04.973948: step 1997, loss 1.69536, acc 0.328125\n",
      "2018-10-22T23:45:05.409782: step 1998, loss 1.66868, acc 0.375\n",
      "2018-10-22T23:45:05.853595: step 1999, loss 1.78704, acc 0.320312\n",
      "2018-10-22T23:45:06.286437: step 2000, loss 1.66395, acc 0.382812\n",
      "\n",
      "Evaluation:\n",
      "Number of batches in dev set is 62\n",
      "batch 1 in dev >> 2018-10-22T23:45:08.133802: loss 1.74457, acc 0.336\n",
      "batch 2 in dev >> 2018-10-22T23:45:10.111513: loss 1.73992, acc 0.35\n",
      "batch 3 in dev >> 2018-10-22T23:45:11.791024: loss 1.71681, acc 0.344\n",
      "batch 4 in dev >> 2018-10-22T23:45:13.333897: loss 1.70072, acc 0.372\n",
      "batch 5 in dev >> 2018-10-22T23:45:15.133086: loss 1.70731, acc 0.356\n",
      "batch 6 in dev >> 2018-10-22T23:45:16.635070: loss 1.70992, acc 0.36\n",
      "batch 7 in dev >> 2018-10-22T23:45:18.314579: loss 1.72705, acc 0.354\n",
      "batch 8 in dev >> 2018-10-22T23:45:19.919290: loss 1.6947, acc 0.354\n",
      "batch 9 in dev >> 2018-10-22T23:45:21.364426: loss 1.75019, acc 0.328\n",
      "batch 10 in dev >> 2018-10-22T23:45:23.092804: loss 1.76529, acc 0.312\n",
      "batch 11 in dev >> 2018-10-22T23:45:24.643658: loss 1.66692, acc 0.376\n",
      "batch 12 in dev >> 2018-10-22T23:45:26.281278: loss 1.70755, acc 0.376\n",
      "batch 13 in dev >> 2018-10-22T23:45:28.005668: loss 1.72347, acc 0.33\n",
      "batch 14 in dev >> 2018-10-22T23:45:29.657252: loss 1.7028, acc 0.376\n",
      "batch 15 in dev >> 2018-10-22T23:45:31.337758: loss 1.71537, acc 0.346\n",
      "batch 16 in dev >> 2018-10-22T23:45:33.130964: loss 1.69718, acc 0.35\n",
      "batch 17 in dev >> 2018-10-22T23:45:34.780552: loss 1.71521, acc 0.354\n",
      "batch 18 in dev >> 2018-10-22T23:45:36.454078: loss 1.77451, acc 0.328\n",
      "batch 19 in dev >> 2018-10-22T23:45:37.936115: loss 1.73847, acc 0.336\n",
      "batch 20 in dev >> 2018-10-22T23:45:39.621609: loss 1.71073, acc 0.364\n",
      "batch 21 in dev >> 2018-10-22T23:45:41.193407: loss 1.69607, acc 0.37\n",
      "batch 22 in dev >> 2018-10-22T23:45:43.052436: loss 1.67553, acc 0.368\n",
      "batch 23 in dev >> 2018-10-22T23:45:44.734937: loss 1.73912, acc 0.336\n",
      "batch 24 in dev >> 2018-10-22T23:45:46.589977: loss 1.62824, acc 0.438\n",
      "batch 25 in dev >> 2018-10-22T23:45:48.384179: loss 1.71228, acc 0.33\n",
      "batch 26 in dev >> 2018-10-22T23:45:50.153449: loss 1.68946, acc 0.386\n",
      "batch 27 in dev >> 2018-10-22T23:45:51.727241: loss 1.74262, acc 0.324\n",
      "batch 28 in dev >> 2018-10-22T23:45:53.518451: loss 1.72064, acc 0.322\n",
      "batch 29 in dev >> 2018-10-22T23:45:55.158068: loss 1.77469, acc 0.336\n",
      "batch 30 in dev >> 2018-10-22T23:45:57.036046: loss 1.70917, acc 0.37\n",
      "batch 31 in dev >> 2018-10-22T23:45:58.705581: loss 1.71829, acc 0.362\n",
      "batch 32 in dev >> 2018-10-22T23:46:00.596036: loss 1.72264, acc 0.33\n",
      "batch 33 in dev >> 2018-10-22T23:46:02.614982: loss 1.74947, acc 0.31\n",
      "batch 34 in dev >> 2018-10-22T23:46:04.812107: loss 1.73666, acc 0.332\n",
      "batch 35 in dev >> 2018-10-22T23:46:06.750924: loss 1.7409, acc 0.348\n",
      "batch 36 in dev >> 2018-10-22T23:46:08.711681: loss 1.74434, acc 0.364\n",
      "batch 37 in dev >> 2018-10-22T23:46:10.514175: loss 1.697, acc 0.356\n",
      "batch 38 in dev >> 2018-10-22T23:46:12.209641: loss 1.67301, acc 0.382\n",
      "batch 39 in dev >> 2018-10-22T23:46:13.910095: loss 1.75072, acc 0.348\n",
      "batch 40 in dev >> 2018-10-22T23:46:15.544725: loss 1.67592, acc 0.37\n",
      "batch 41 in dev >> 2018-10-22T23:46:17.279087: loss 1.71877, acc 0.356\n",
      "batch 42 in dev >> 2018-10-22T23:46:19.394431: loss 1.73126, acc 0.342\n",
      "batch 43 in dev >> 2018-10-22T23:46:21.361172: loss 1.74822, acc 0.324\n",
      "batch 44 in dev >> 2018-10-22T23:46:22.909032: loss 1.7308, acc 0.354\n",
      "batch 45 in dev >> 2018-10-22T23:46:24.633422: loss 1.72607, acc 0.36\n",
      "batch 46 in dev >> 2018-10-22T23:46:26.409672: loss 1.73462, acc 0.33\n",
      "batch 47 in dev >> 2018-10-22T23:46:28.011391: loss 1.6807, acc 0.38\n",
      "batch 48 in dev >> 2018-10-22T23:46:29.663972: loss 1.73253, acc 0.328\n",
      "batch 49 in dev >> 2018-10-22T23:46:31.340489: loss 1.71858, acc 0.352\n",
      "batch 50 in dev >> 2018-10-22T23:46:33.030968: loss 1.7609, acc 0.346\n",
      "batch 51 in dev >> 2018-10-22T23:46:34.999706: loss 1.75803, acc 0.308\n",
      "batch 52 in dev >> 2018-10-22T23:46:37.191844: loss 1.68394, acc 0.37\n",
      "batch 53 in dev >> 2018-10-22T23:46:39.629326: loss 1.65515, acc 0.386\n",
      "batch 54 in dev >> 2018-10-22T23:46:41.673370: loss 1.72219, acc 0.33\n",
      "batch 55 in dev >> 2018-10-22T23:46:44.037050: loss 1.72829, acc 0.298\n",
      "batch 56 in dev >> 2018-10-22T23:46:45.913559: loss 1.68401, acc 0.35\n",
      "batch 57 in dev >> 2018-10-22T23:46:47.698784: loss 1.7608, acc 0.33\n",
      "batch 58 in dev >> 2018-10-22T23:46:49.382282: loss 1.66938, acc 0.378\n",
      "batch 59 in dev >> 2018-10-22T23:46:51.108666: loss 1.69444, acc 0.382\n",
      "batch 60 in dev >> 2018-10-22T23:46:53.162176: loss 1.70286, acc 0.364\n",
      "batch 61 in dev >> 2018-10-22T23:46:54.840687: loss 1.72459, acc 0.338\n",
      "batch 62 in dev >> 2018-10-22T23:46:56.675781: loss 1.73789, acc 0.316\n",
      "\n",
      "Mean accuracy=0.3500967751587591\n",
      "Mean loss=1.7178955424216487\n",
      "\n",
      "2018-10-22T23:46:57.184421: step 2001, loss 1.71785, acc 0.34375\n",
      "2018-10-22T23:46:57.783820: step 2002, loss 1.74292, acc 0.367188\n",
      "2018-10-22T23:46:58.379227: step 2003, loss 1.64648, acc 0.382812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:46:58.814063: step 2004, loss 1.68706, acc 0.34375\n",
      "2018-10-22T23:46:59.258874: step 2005, loss 1.56297, acc 0.460938\n",
      "2018-10-22T23:46:59.786463: step 2006, loss 1.69521, acc 0.375\n",
      "2018-10-22T23:47:00.241247: step 2007, loss 1.61681, acc 0.390625\n",
      "2018-10-22T23:47:00.703013: step 2008, loss 1.70387, acc 0.351562\n",
      "2018-10-22T23:47:01.212652: step 2009, loss 1.69063, acc 0.359375\n",
      "2018-10-22T23:47:01.845957: step 2010, loss 1.75544, acc 0.28125\n",
      "2018-10-22T23:47:02.286778: step 2011, loss 1.82756, acc 0.3125\n",
      "2018-10-22T23:47:02.706656: step 2012, loss 1.73419, acc 0.335938\n",
      "2018-10-22T23:47:03.200337: step 2013, loss 1.77944, acc 0.34375\n",
      "2018-10-22T23:47:03.637168: step 2014, loss 1.6553, acc 0.367188\n",
      "2018-10-22T23:47:04.062032: step 2015, loss 1.67953, acc 0.351562\n",
      "2018-10-22T23:47:04.481909: step 2016, loss 1.75958, acc 0.304688\n",
      "2018-10-22T23:47:04.924725: step 2017, loss 1.7087, acc 0.351562\n",
      "2018-10-22T23:47:05.395466: step 2018, loss 1.72672, acc 0.390625\n",
      "2018-10-22T23:47:05.863217: step 2019, loss 1.6821, acc 0.398438\n",
      "2018-10-22T23:47:06.290074: step 2020, loss 1.71521, acc 0.3125\n",
      "2018-10-22T23:47:06.829632: step 2021, loss 1.71684, acc 0.390625\n",
      "2018-10-22T23:47:07.264469: step 2022, loss 1.55574, acc 0.460938\n",
      "2018-10-22T23:47:07.690330: step 2023, loss 1.765, acc 0.398438\n",
      "2018-10-22T23:47:08.109210: step 2024, loss 1.82467, acc 0.28125\n",
      "2018-10-22T23:47:08.548037: step 2025, loss 1.71657, acc 0.328125\n",
      "2018-10-22T23:47:08.981877: step 2026, loss 1.76261, acc 0.3125\n",
      "2018-10-22T23:47:09.396768: step 2027, loss 1.70558, acc 0.390625\n",
      "2018-10-22T23:47:09.860528: step 2028, loss 1.73468, acc 0.359375\n",
      "2018-10-22T23:47:10.341242: step 2029, loss 1.66628, acc 0.382812\n",
      "2018-10-22T23:47:10.820959: step 2030, loss 1.72417, acc 0.289062\n",
      "2018-10-22T23:47:11.306661: step 2031, loss 1.77238, acc 0.273438\n",
      "2018-10-22T23:47:11.766431: step 2032, loss 1.65093, acc 0.367188\n",
      "2018-10-22T23:47:12.183317: step 2033, loss 1.74202, acc 0.335938\n",
      "2018-10-22T23:47:12.613168: step 2034, loss 1.82128, acc 0.320312\n",
      "2018-10-22T23:47:13.080918: step 2035, loss 1.68684, acc 0.375\n",
      "2018-10-22T23:47:13.579583: step 2036, loss 1.79, acc 0.257812\n",
      "2018-10-22T23:47:14.043343: step 2037, loss 1.75999, acc 0.34375\n",
      "2018-10-22T23:47:14.436293: step 2038, loss 1.70492, acc 0.390625\n",
      "2018-10-22T23:47:14.858165: step 2039, loss 1.75951, acc 0.335938\n",
      "2018-10-22T23:47:15.296993: step 2040, loss 1.68991, acc 0.34375\n",
      "2018-10-22T23:47:15.723850: step 2041, loss 1.79626, acc 0.320312\n",
      "2018-10-22T23:47:16.156692: step 2042, loss 1.60915, acc 0.421875\n",
      "2018-10-22T23:47:16.586544: step 2043, loss 1.68445, acc 0.3125\n",
      "2018-10-22T23:47:17.086209: step 2044, loss 1.64833, acc 0.40625\n",
      "2018-10-22T23:47:17.493119: step 2045, loss 1.70426, acc 0.335938\n",
      "2018-10-22T23:47:17.944913: step 2046, loss 1.6901, acc 0.367188\n",
      "2018-10-22T23:47:18.410666: step 2047, loss 1.75517, acc 0.367188\n",
      "2018-10-22T23:47:18.849493: step 2048, loss 1.73871, acc 0.34375\n",
      "2018-10-22T23:47:19.310261: step 2049, loss 1.67349, acc 0.382812\n",
      "2018-10-22T23:47:19.749088: step 2050, loss 1.62144, acc 0.414062\n",
      "2018-10-22T23:47:20.214843: step 2051, loss 1.68835, acc 0.328125\n",
      "2018-10-22T23:47:20.714506: step 2052, loss 1.75826, acc 0.289062\n",
      "2018-10-22T23:47:21.158320: step 2053, loss 1.68092, acc 0.359375\n",
      "2018-10-22T23:47:21.611109: step 2054, loss 1.55558, acc 0.445312\n",
      "2018-10-22T23:47:22.131717: step 2055, loss 1.74588, acc 0.367188\n",
      "2018-10-22T23:47:22.695210: step 2056, loss 1.65624, acc 0.367188\n",
      "2018-10-22T23:47:23.129050: step 2057, loss 1.80104, acc 0.320312\n",
      "2018-10-22T23:47:23.614751: step 2058, loss 1.65198, acc 0.375\n",
      "2018-10-22T23:47:24.126384: step 2059, loss 1.65416, acc 0.382812\n",
      "2018-10-22T23:47:24.618069: step 2060, loss 1.76867, acc 0.34375\n",
      "2018-10-22T23:47:25.093797: step 2061, loss 1.84342, acc 0.296875\n",
      "2018-10-22T23:47:25.542597: step 2062, loss 1.66722, acc 0.375\n",
      "2018-10-22T23:47:26.102101: step 2063, loss 1.71158, acc 0.375\n",
      "2018-10-22T23:47:26.538933: step 2064, loss 1.60493, acc 0.398438\n",
      "2018-10-22T23:47:26.998704: step 2065, loss 1.63932, acc 0.390625\n",
      "2018-10-22T23:47:27.545243: step 2066, loss 1.7055, acc 0.359375\n",
      "2018-10-22T23:47:28.011994: step 2067, loss 1.58798, acc 0.421875\n",
      "2018-10-22T23:47:28.452815: step 2068, loss 1.71582, acc 0.320312\n",
      "2018-10-22T23:47:28.899621: step 2069, loss 1.6818, acc 0.390625\n",
      "2018-10-22T23:47:29.345428: step 2070, loss 1.7708, acc 0.320312\n",
      "2018-10-22T23:47:29.803205: step 2071, loss 1.69039, acc 0.34375\n",
      "2018-10-22T23:47:30.226075: step 2072, loss 1.70686, acc 0.375\n",
      "2018-10-22T23:47:30.687840: step 2073, loss 1.70252, acc 0.304688\n",
      "2018-10-22T23:47:31.152596: step 2074, loss 1.63463, acc 0.40625\n",
      "2018-10-22T23:47:31.609375: step 2075, loss 1.83433, acc 0.265625\n",
      "2018-10-22T23:47:32.065157: step 2076, loss 1.64897, acc 0.375\n",
      "2018-10-22T23:47:32.515951: step 2077, loss 1.68704, acc 0.367188\n",
      "2018-10-22T23:47:32.977721: step 2078, loss 1.73723, acc 0.3125\n",
      "2018-10-22T23:47:33.413692: step 2079, loss 1.72749, acc 0.421875\n",
      "2018-10-22T23:47:33.849926: step 2080, loss 1.50526, acc 0.421875\n",
      "2018-10-22T23:47:34.303712: step 2081, loss 1.67279, acc 0.390625\n",
      "2018-10-22T23:47:34.757500: step 2082, loss 1.67464, acc 0.359375\n",
      "2018-10-22T23:47:35.217270: step 2083, loss 1.73161, acc 0.34375\n",
      "2018-10-22T23:47:35.611215: step 2084, loss 1.643, acc 0.34375\n",
      "2018-10-22T23:47:36.037077: step 2085, loss 1.70834, acc 0.34375\n",
      "2018-10-22T23:47:36.534747: step 2086, loss 1.80872, acc 0.304688\n",
      "2018-10-22T23:47:36.984544: step 2087, loss 1.6306, acc 0.46875\n",
      "2018-10-22T23:47:37.445312: step 2088, loss 1.67243, acc 0.40625\n",
      "2018-10-22T23:47:37.966919: step 2089, loss 1.70784, acc 0.398438\n",
      "2018-10-22T23:47:38.413723: step 2090, loss 1.64502, acc 0.453125\n",
      "2018-10-22T23:47:38.864519: step 2091, loss 1.64104, acc 0.390625\n",
      "2018-10-22T23:47:39.287387: step 2092, loss 1.81277, acc 0.359375\n",
      "2018-10-22T23:47:39.743167: step 2093, loss 1.78102, acc 0.34375\n",
      "2018-10-22T23:47:40.209920: step 2094, loss 1.72812, acc 0.335938\n",
      "2018-10-22T23:47:40.679664: step 2095, loss 1.67769, acc 0.382812\n",
      "2018-10-22T23:47:41.288038: step 2096, loss 1.81688, acc 0.289062\n",
      "2018-10-22T23:47:41.718886: step 2097, loss 1.73574, acc 0.34375\n",
      "2018-10-22T23:47:42.195610: step 2098, loss 1.73324, acc 0.351562\n",
      "2018-10-22T23:47:42.676325: step 2099, loss 1.6673, acc 0.359375\n",
      "2018-10-22T23:47:43.087227: step 2100, loss 1.69936, acc 0.382812\n",
      "2018-10-22T23:47:43.587889: step 2101, loss 1.75561, acc 0.296875\n",
      "2018-10-22T23:47:44.060624: step 2102, loss 1.6846, acc 0.34375\n",
      "2018-10-22T23:47:44.501445: step 2103, loss 1.77312, acc 0.296875\n",
      "2018-10-22T23:47:45.020059: step 2104, loss 1.66341, acc 0.421875\n",
      "2018-10-22T23:47:45.474843: step 2105, loss 1.78291, acc 0.382812\n",
      "2018-10-22T23:47:45.880758: step 2106, loss 1.64378, acc 0.398438\n",
      "2018-10-22T23:47:46.363467: step 2107, loss 1.64412, acc 0.367188\n",
      "2018-10-22T23:47:46.833211: step 2108, loss 1.74008, acc 0.359375\n",
      "2018-10-22T23:47:47.388725: step 2109, loss 1.72533, acc 0.351562\n",
      "2018-10-22T23:47:47.929280: step 2110, loss 1.7146, acc 0.375\n",
      "2018-10-22T23:47:48.485792: step 2111, loss 1.74274, acc 0.351562\n",
      "2018-10-22T23:47:48.988449: step 2112, loss 1.75866, acc 0.304688\n",
      "2018-10-22T23:47:49.549947: step 2113, loss 1.6623, acc 0.429688\n",
      "2018-10-22T23:47:50.049611: step 2114, loss 1.64292, acc 0.4375\n",
      "2018-10-22T23:47:50.487440: step 2115, loss 1.7267, acc 0.34375\n",
      "2018-10-22T23:47:50.936240: step 2116, loss 1.78394, acc 0.320312\n",
      "2018-10-22T23:47:51.399004: step 2117, loss 1.72541, acc 0.28125\n",
      "2018-10-22T23:47:51.928587: step 2118, loss 1.73086, acc 0.335938\n",
      "2018-10-22T23:47:52.414288: step 2119, loss 1.70064, acc 0.3125\n",
      "2018-10-22T23:47:52.928912: step 2120, loss 1.73296, acc 0.382812\n",
      "2018-10-22T23:47:53.362752: step 2121, loss 1.65933, acc 0.359375\n",
      "2018-10-22T23:47:53.795595: step 2122, loss 1.65704, acc 0.351562\n",
      "2018-10-22T23:47:54.227439: step 2123, loss 1.6323, acc 0.375\n",
      "2018-10-22T23:47:54.647318: step 2124, loss 1.67823, acc 0.351562\n",
      "2018-10-22T23:47:55.143989: step 2125, loss 1.66771, acc 0.320312\n",
      "2018-10-22T23:47:55.580822: step 2126, loss 1.70212, acc 0.320312\n",
      "2018-10-22T23:47:56.101430: step 2127, loss 1.69925, acc 0.414062\n",
      "2018-10-22T23:47:56.629019: step 2128, loss 1.69911, acc 0.328125\n",
      "2018-10-22T23:47:57.097766: step 2129, loss 1.6924, acc 0.328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:47:57.529611: step 2130, loss 1.69957, acc 0.390625\n",
      "2018-10-22T23:47:57.998358: step 2131, loss 1.72877, acc 0.328125\n",
      "2018-10-22T23:47:58.500016: step 2132, loss 1.6803, acc 0.351562\n",
      "2018-10-22T23:47:58.976741: step 2133, loss 1.71606, acc 0.367188\n",
      "2018-10-22T23:47:59.416565: step 2134, loss 1.73619, acc 0.3125\n",
      "2018-10-22T23:47:59.904261: step 2135, loss 1.69275, acc 0.421875\n",
      "2018-10-22T23:48:00.365030: step 2136, loss 1.65925, acc 0.375\n",
      "2018-10-22T23:48:00.861701: step 2137, loss 1.72462, acc 0.351562\n",
      "2018-10-22T23:48:01.327455: step 2138, loss 1.7383, acc 0.320312\n",
      "2018-10-22T23:48:01.783238: step 2139, loss 1.79995, acc 0.34375\n",
      "2018-10-22T23:48:02.280908: step 2140, loss 1.75314, acc 0.328125\n",
      "2018-10-22T23:48:02.730704: step 2141, loss 1.58164, acc 0.429688\n",
      "2018-10-22T23:48:03.144060: step 2142, loss 1.77545, acc 0.328125\n",
      "2018-10-22T23:48:03.581890: step 2143, loss 1.58579, acc 0.421875\n",
      "2018-10-22T23:48:04.068588: step 2144, loss 1.65654, acc 0.40625\n",
      "2018-10-22T23:48:04.535340: step 2145, loss 1.62434, acc 0.429688\n",
      "2018-10-22T23:48:05.020045: step 2146, loss 1.67942, acc 0.335938\n",
      "2018-10-22T23:48:05.531676: step 2147, loss 1.62019, acc 0.382812\n",
      "2018-10-22T23:48:06.055277: step 2148, loss 1.56628, acc 0.390625\n",
      "2018-10-22T23:48:06.533001: step 2149, loss 1.69002, acc 0.320312\n",
      "2018-10-22T23:48:06.984791: step 2150, loss 1.73002, acc 0.304688\n",
      "2018-10-22T23:48:07.767698: step 2151, loss 1.72524, acc 0.296875\n",
      "2018-10-22T23:48:08.356125: step 2152, loss 1.69714, acc 0.328125\n",
      "2018-10-22T23:48:08.956519: step 2153, loss 1.68963, acc 0.375\n",
      "2018-10-22T23:48:09.753389: step 2154, loss 1.70972, acc 0.3125\n",
      "2018-10-22T23:48:10.267015: step 2155, loss 1.72672, acc 0.34375\n",
      "2018-10-22T23:48:10.769671: step 2156, loss 1.69654, acc 0.34375\n",
      "2018-10-22T23:48:11.227447: step 2157, loss 1.7094, acc 0.390625\n",
      "2018-10-22T23:48:11.662285: step 2158, loss 1.72497, acc 0.34375\n",
      "2018-10-22T23:48:12.152972: step 2159, loss 1.87442, acc 0.265625\n",
      "2018-10-22T23:48:12.885015: step 2160, loss 1.82928, acc 0.328125\n",
      "2018-10-22T23:48:13.488402: step 2161, loss 1.74801, acc 0.304688\n",
      "2018-10-22T23:48:14.114728: step 2162, loss 1.61509, acc 0.367188\n",
      "2018-10-22T23:48:14.605415: step 2163, loss 1.66471, acc 0.375\n",
      "2018-10-22T23:48:15.181874: step 2164, loss 1.68872, acc 0.359375\n",
      "2018-10-22T23:48:15.748359: step 2165, loss 1.64102, acc 0.367188\n",
      "2018-10-22T23:48:16.524285: step 2166, loss 1.66978, acc 0.320312\n",
      "2018-10-22T23:48:17.376008: step 2167, loss 1.84184, acc 0.234375\n",
      "2018-10-22T23:48:17.900604: step 2168, loss 1.63319, acc 0.367188\n",
      "2018-10-22T23:48:18.429191: step 2169, loss 1.70846, acc 0.367188\n",
      "2018-10-22T23:48:19.051527: step 2170, loss 1.82342, acc 0.296875\n",
      "2018-10-22T23:48:19.949127: step 2171, loss 1.54163, acc 0.445312\n",
      "2018-10-22T23:48:20.554509: step 2172, loss 1.7569, acc 0.34375\n",
      "2018-10-22T23:48:21.180833: step 2173, loss 1.75601, acc 0.335938\n",
      "2018-10-22T23:48:21.856028: step 2174, loss 1.68871, acc 0.28125\n",
      "2018-10-22T23:48:22.349709: step 2175, loss 1.58418, acc 0.414062\n",
      "2018-10-22T23:48:22.838846: step 2176, loss 1.75305, acc 0.351562\n",
      "2018-10-22T23:48:23.272686: step 2177, loss 1.59372, acc 0.445312\n",
      "2018-10-22T23:48:23.740437: step 2178, loss 1.76603, acc 0.296875\n",
      "2018-10-22T23:48:24.167295: step 2179, loss 1.67117, acc 0.359375\n",
      "2018-10-22T23:48:24.636042: step 2180, loss 1.70024, acc 0.351562\n",
      "2018-10-22T23:48:25.146676: step 2181, loss 1.67835, acc 0.320312\n",
      "2018-10-22T23:48:25.667290: step 2182, loss 1.6753, acc 0.375\n",
      "2018-10-22T23:48:26.102494: step 2183, loss 1.69259, acc 0.390625\n",
      "2018-10-22T23:48:26.523370: step 2184, loss 1.64581, acc 0.390625\n",
      "2018-10-22T23:48:27.033419: step 2185, loss 1.78932, acc 0.335938\n",
      "2018-10-22T23:48:27.749526: step 2186, loss 1.63175, acc 0.421875\n",
      "2018-10-22T23:48:28.445724: step 2187, loss 1.79471, acc 0.382812\n",
      "2018-10-22T23:48:29.123911: step 2188, loss 1.6789, acc 0.351562\n",
      "2018-10-22T23:48:29.666460: step 2189, loss 1.73851, acc 0.335938\n",
      "2018-10-22T23:48:30.247905: step 2190, loss 1.61331, acc 0.375\n",
      "2018-10-22T23:48:30.921105: step 2191, loss 1.73088, acc 0.320312\n",
      "2018-10-22T23:48:31.470635: step 2192, loss 1.72835, acc 0.359375\n",
      "2018-10-22T23:48:31.978277: step 2193, loss 1.69277, acc 0.375\n",
      "2018-10-22T23:48:32.496892: step 2194, loss 1.73067, acc 0.3125\n",
      "2018-10-22T23:48:33.167101: step 2195, loss 1.63797, acc 0.382812\n",
      "2018-10-22T23:48:33.803398: step 2196, loss 1.71019, acc 0.382812\n",
      "2018-10-22T23:48:34.397242: step 2197, loss 1.77359, acc 0.296875\n",
      "2018-10-22T23:48:34.944777: step 2198, loss 1.70903, acc 0.351562\n",
      "2018-10-22T23:48:35.404548: step 2199, loss 1.73876, acc 0.34375\n",
      "2018-10-22T23:48:35.942111: step 2200, loss 1.78419, acc 0.359375\n",
      "2018-10-22T23:48:36.491640: step 2201, loss 1.68851, acc 0.34375\n",
      "2018-10-22T23:48:37.049151: step 2202, loss 1.74898, acc 0.320312\n",
      "2018-10-22T23:48:37.662511: step 2203, loss 1.64935, acc 0.429688\n",
      "2018-10-22T23:48:38.186111: step 2204, loss 1.65818, acc 0.367188\n",
      "2018-10-22T23:48:38.735642: step 2205, loss 1.78692, acc 0.265625\n",
      "2018-10-22T23:48:39.270036: step 2206, loss 1.76705, acc 0.351562\n",
      "2018-10-22T23:48:39.970759: step 2207, loss 1.60474, acc 0.40625\n",
      "2018-10-22T23:48:40.627003: step 2208, loss 1.70492, acc 0.382812\n",
      "2018-10-22T23:48:41.094753: step 2209, loss 1.7353, acc 0.34375\n",
      "2018-10-22T23:48:41.738183: step 2210, loss 1.69106, acc 0.367188\n",
      "2018-10-22T23:48:42.264119: step 2211, loss 1.77308, acc 0.320312\n",
      "2018-10-22T23:48:42.800687: step 2212, loss 1.74859, acc 0.335938\n",
      "2018-10-22T23:48:43.290376: step 2213, loss 1.63924, acc 0.390625\n",
      "2018-10-22T23:48:43.793031: step 2214, loss 1.59326, acc 0.398438\n",
      "2018-10-22T23:48:44.228866: step 2215, loss 1.74461, acc 0.328125\n",
      "2018-10-22T23:48:44.678023: step 2216, loss 1.73201, acc 0.304688\n",
      "2018-10-22T23:48:45.198631: step 2217, loss 1.72881, acc 0.359375\n",
      "2018-10-22T23:48:45.672365: step 2218, loss 1.6856, acc 0.390625\n",
      "2018-10-22T23:48:46.141112: step 2219, loss 1.62623, acc 0.382812\n",
      "2018-10-22T23:48:46.618834: step 2220, loss 1.71522, acc 0.3125\n",
      "2018-10-22T23:48:47.174348: step 2221, loss 1.73779, acc 0.359375\n",
      "2018-10-22T23:48:47.607191: step 2222, loss 1.67468, acc 0.375\n",
      "2018-10-22T23:48:48.102867: step 2223, loss 1.83082, acc 0.304688\n",
      "2018-10-22T23:48:48.748141: step 2224, loss 1.51941, acc 0.4375\n",
      "2018-10-22T23:48:49.298668: step 2225, loss 1.65786, acc 0.367188\n",
      "2018-10-22T23:48:49.774396: step 2226, loss 1.72967, acc 0.320312\n",
      "2018-10-22T23:48:50.213223: step 2227, loss 1.6811, acc 0.414062\n",
      "2018-10-22T23:48:50.704909: step 2228, loss 1.77606, acc 0.320312\n",
      "2018-10-22T23:48:51.236487: step 2229, loss 1.59462, acc 0.4375\n",
      "2018-10-22T23:48:51.838878: step 2230, loss 1.65905, acc 0.375\n",
      "2018-10-22T23:48:52.555958: step 2231, loss 1.7457, acc 0.351562\n",
      "2018-10-22T23:48:53.170317: step 2232, loss 1.84493, acc 0.25\n",
      "2018-10-22T23:48:53.727825: step 2233, loss 1.71464, acc 0.34375\n",
      "2018-10-22T23:48:54.225495: step 2234, loss 1.65284, acc 0.398438\n",
      "2018-10-22T23:48:54.699228: step 2235, loss 1.7219, acc 0.320312\n",
      "2018-10-22T23:48:55.162989: step 2236, loss 1.73673, acc 0.335938\n",
      "2018-10-22T23:48:55.720499: step 2237, loss 1.7718, acc 0.34375\n",
      "2018-10-22T23:48:56.282994: step 2238, loss 1.62604, acc 0.40625\n",
      "2018-10-22T23:48:56.808589: step 2239, loss 1.65464, acc 0.3125\n",
      "2018-10-22T23:48:57.239437: step 2240, loss 1.80821, acc 0.289062\n",
      "2018-10-22T23:48:57.864764: step 2241, loss 1.75305, acc 0.304688\n",
      "2018-10-22T23:48:58.355452: step 2242, loss 1.63853, acc 0.421875\n",
      "2018-10-22T23:48:58.915953: step 2243, loss 1.75857, acc 0.289062\n",
      "2018-10-22T23:48:59.553251: step 2244, loss 1.67341, acc 0.328125\n",
      "2018-10-22T23:49:00.018009: step 2245, loss 1.66263, acc 0.375\n",
      "2018-10-22T23:49:00.534626: step 2246, loss 1.67971, acc 0.351562\n",
      "2018-10-22T23:49:01.068200: step 2247, loss 1.68726, acc 0.351562\n",
      "2018-10-22T23:49:01.540935: step 2248, loss 1.74834, acc 0.367188\n",
      "2018-10-22T23:49:02.117394: step 2249, loss 1.60766, acc 0.414062\n",
      "2018-10-22T23:49:02.577165: step 2250, loss 1.59455, acc 0.414062\n",
      "2018-10-22T23:49:03.038929: step 2251, loss 1.72348, acc 0.335938\n",
      "2018-10-22T23:49:03.452824: step 2252, loss 1.61885, acc 0.414062\n",
      "2018-10-22T23:49:03.874696: step 2253, loss 1.70228, acc 0.382812\n",
      "2018-10-22T23:49:04.331474: step 2254, loss 1.68932, acc 0.359375\n",
      "2018-10-22T23:49:04.814183: step 2255, loss 1.63616, acc 0.359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:49:05.324818: step 2256, loss 1.66433, acc 0.335938\n",
      "2018-10-22T23:49:05.853407: step 2257, loss 1.68017, acc 0.414062\n",
      "2018-10-22T23:49:06.424877: step 2258, loss 1.69559, acc 0.320312\n",
      "2018-10-22T23:49:06.916561: step 2259, loss 1.66178, acc 0.34375\n",
      "2018-10-22T23:49:07.560838: step 2260, loss 1.72258, acc 0.320312\n",
      "2018-10-22T23:49:08.160237: step 2261, loss 1.74185, acc 0.328125\n",
      "2018-10-22T23:49:08.927186: step 2262, loss 1.56296, acc 0.460938\n",
      "2018-10-22T23:49:09.766941: step 2263, loss 1.66345, acc 0.359375\n",
      "2018-10-22T23:49:10.442135: step 2264, loss 1.56921, acc 0.460938\n",
      "2018-10-22T23:49:11.090403: step 2265, loss 1.66621, acc 0.382812\n",
      "2018-10-22T23:49:11.642924: step 2266, loss 1.7663, acc 0.320312\n",
      "2018-10-22T23:49:12.095714: step 2267, loss 1.65683, acc 0.359375\n",
      "2018-10-22T23:49:12.545511: step 2268, loss 1.5173, acc 0.453125\n",
      "2018-10-22T23:49:12.990322: step 2269, loss 1.70045, acc 0.328125\n",
      "2018-10-22T23:49:13.477020: step 2270, loss 1.76204, acc 0.3125\n",
      "2018-10-22T23:49:13.958732: step 2271, loss 1.64302, acc 0.398438\n",
      "2018-10-22T23:49:14.399554: step 2272, loss 1.70309, acc 0.367188\n",
      "2018-10-22T23:49:14.857330: step 2273, loss 1.66178, acc 0.390625\n",
      "2018-10-22T23:49:15.354002: step 2274, loss 1.68206, acc 0.390625\n",
      "2018-10-22T23:49:15.840700: step 2275, loss 1.64915, acc 0.429688\n",
      "2018-10-22T23:49:16.258583: step 2276, loss 1.76309, acc 0.34375\n",
      "2018-10-22T23:49:16.762236: step 2277, loss 1.74023, acc 0.335938\n",
      "2018-10-22T23:49:17.313762: step 2278, loss 1.71601, acc 0.3125\n",
      "2018-10-22T23:49:17.754582: step 2279, loss 1.76547, acc 0.359375\n",
      "2018-10-22T23:49:18.174460: step 2280, loss 1.60178, acc 0.445312\n",
      "2018-10-22T23:49:18.639219: step 2281, loss 1.67211, acc 0.398438\n",
      "2018-10-22T23:49:19.115943: step 2282, loss 1.7277, acc 0.3125\n",
      "2018-10-22T23:49:19.534824: step 2283, loss 1.60666, acc 0.398438\n",
      "2018-10-22T23:49:20.040471: step 2284, loss 1.71605, acc 0.367188\n",
      "2018-10-22T23:49:20.594988: step 2285, loss 1.67058, acc 0.40625\n",
      "2018-10-22T23:49:21.081687: step 2286, loss 1.58999, acc 0.390625\n",
      "2018-10-22T23:49:21.558412: step 2287, loss 1.73619, acc 0.367188\n",
      "2018-10-22T23:49:22.046109: step 2288, loss 1.75494, acc 0.398438\n",
      "2018-10-22T23:49:22.547766: step 2289, loss 1.71513, acc 0.34375\n",
      "2018-10-22T23:49:23.015516: step 2290, loss 1.66182, acc 0.351562\n",
      "2018-10-22T23:49:23.452349: step 2291, loss 1.70251, acc 0.367188\n",
      "2018-10-22T23:49:24.014844: step 2292, loss 1.77768, acc 0.3125\n",
      "2018-10-22T23:49:24.487581: step 2293, loss 1.71386, acc 0.320312\n",
      "2018-10-22T23:49:24.976273: step 2294, loss 1.85827, acc 0.21875\n",
      "2018-10-22T23:49:25.502867: step 2295, loss 1.65658, acc 0.390625\n",
      "2018-10-22T23:49:25.954657: step 2296, loss 1.64255, acc 0.382812\n",
      "2018-10-22T23:49:26.456316: step 2297, loss 1.65203, acc 0.421875\n",
      "2018-10-22T23:49:26.916086: step 2298, loss 1.66214, acc 0.382812\n",
      "2018-10-22T23:49:27.446669: step 2299, loss 1.62919, acc 0.367188\n",
      "2018-10-22T23:49:27.930375: step 2300, loss 1.67821, acc 0.367188\n",
      "2018-10-22T23:49:28.385159: step 2301, loss 1.69072, acc 0.34375\n",
      "2018-10-22T23:49:28.866870: step 2302, loss 1.64805, acc 0.359375\n",
      "2018-10-22T23:49:29.330631: step 2303, loss 1.6872, acc 0.398438\n",
      "2018-10-22T23:49:29.825308: step 2304, loss 1.67016, acc 0.398438\n",
      "2018-10-22T23:49:30.319986: step 2305, loss 1.73306, acc 0.359375\n",
      "2018-10-22T23:49:30.848573: step 2306, loss 1.67198, acc 0.335938\n",
      "2018-10-22T23:49:31.381148: step 2307, loss 1.62855, acc 0.367188\n",
      "2018-10-22T23:49:31.902754: step 2308, loss 1.7316, acc 0.359375\n",
      "2018-10-22T23:49:32.352552: step 2309, loss 1.77411, acc 0.28125\n",
      "2018-10-22T23:49:32.800354: step 2310, loss 1.69206, acc 0.335938\n",
      "2018-10-22T23:49:33.249154: step 2311, loss 1.76498, acc 0.359375\n",
      "2018-10-22T23:49:33.719895: step 2312, loss 1.77793, acc 0.320312\n",
      "2018-10-22T23:49:34.182657: step 2313, loss 1.79345, acc 0.296875\n",
      "2018-10-22T23:49:34.671351: step 2314, loss 1.6585, acc 0.390625\n",
      "2018-10-22T23:49:35.143090: step 2315, loss 1.70109, acc 0.328125\n",
      "2018-10-22T23:49:35.581917: step 2316, loss 1.67698, acc 0.375\n",
      "2018-10-22T23:49:36.099532: step 2317, loss 1.67628, acc 0.414062\n",
      "2018-10-22T23:49:36.520407: step 2318, loss 1.62343, acc 0.40625\n",
      "2018-10-22T23:49:36.932305: step 2319, loss 1.71641, acc 0.351562\n",
      "2018-10-22T23:49:37.353180: step 2320, loss 1.67126, acc 0.335938\n",
      "2018-10-22T23:49:37.802979: step 2321, loss 1.8069, acc 0.296875\n",
      "2018-10-22T23:49:38.229837: step 2322, loss 1.67786, acc 0.351562\n",
      "2018-10-22T23:49:38.673650: step 2323, loss 1.83498, acc 0.296875\n",
      "2018-10-22T23:49:39.193262: step 2324, loss 1.64049, acc 0.382812\n",
      "2018-10-22T23:49:39.655025: step 2325, loss 1.68003, acc 0.34375\n",
      "2018-10-22T23:49:40.070914: step 2326, loss 1.73863, acc 0.34375\n",
      "2018-10-22T23:49:40.536668: step 2327, loss 1.66496, acc 0.351562\n",
      "2018-10-22T23:49:41.073234: step 2328, loss 1.68883, acc 0.382812\n",
      "2018-10-22T23:49:41.682604: step 2329, loss 1.62786, acc 0.429688\n",
      "2018-10-22T23:49:42.262055: step 2330, loss 1.71457, acc 0.375\n",
      "2018-10-22T23:49:42.738780: step 2331, loss 1.65465, acc 0.421875\n",
      "2018-10-22T23:49:43.212513: step 2332, loss 1.67892, acc 0.3125\n",
      "2018-10-22T23:49:43.711180: step 2333, loss 1.6813, acc 0.398438\n",
      "2018-10-22T23:49:44.190898: step 2334, loss 1.62083, acc 0.359375\n",
      "2018-10-22T23:49:44.723627: step 2335, loss 1.56354, acc 0.460938\n",
      "2018-10-22T23:49:45.218303: step 2336, loss 1.75277, acc 0.328125\n",
      "2018-10-22T23:49:45.734923: step 2337, loss 1.7371, acc 0.304688\n",
      "2018-10-22T23:49:46.188709: step 2338, loss 1.78608, acc 0.3125\n",
      "2018-10-22T23:49:46.634517: step 2339, loss 1.72449, acc 0.3125\n",
      "2018-10-22T23:49:47.124208: step 2340, loss 1.6686, acc 0.351562\n",
      "2018-10-22T23:49:47.620239: step 2341, loss 1.63845, acc 0.34375\n",
      "2018-10-22T23:49:48.118962: step 2342, loss 1.71463, acc 0.375\n",
      "2018-10-22T23:49:48.726338: step 2343, loss 1.58333, acc 0.40625\n",
      "2018-10-22T23:49:49.273875: step 2344, loss 1.72087, acc 0.390625\n",
      "2018-10-22T23:49:49.787173: step 2345, loss 1.74461, acc 0.375\n",
      "2018-10-22T23:49:50.239601: step 2346, loss 1.67522, acc 0.34375\n",
      "2018-10-22T23:49:50.728634: step 2347, loss 1.75602, acc 0.359375\n",
      "2018-10-22T23:49:51.235547: step 2348, loss 1.60675, acc 0.398438\n",
      "2018-10-22T23:49:51.689337: step 2349, loss 1.73192, acc 0.367188\n",
      "2018-10-22T23:49:52.175036: step 2350, loss 1.68138, acc 0.351562\n",
      "2018-10-22T23:49:52.621841: step 2351, loss 1.71283, acc 0.375\n",
      "2018-10-22T23:49:53.091584: step 2352, loss 1.76639, acc 0.359375\n",
      "2018-10-22T23:49:53.526422: step 2353, loss 1.73726, acc 0.320312\n",
      "2018-10-22T23:49:54.073958: step 2354, loss 1.58134, acc 0.4375\n",
      "2018-10-22T23:49:54.550683: step 2355, loss 1.65083, acc 0.382812\n",
      "2018-10-22T23:49:55.035388: step 2356, loss 1.5466, acc 0.4375\n",
      "2018-10-22T23:49:55.572951: step 2357, loss 1.73474, acc 0.296875\n",
      "2018-10-22T23:49:56.172349: step 2358, loss 1.64198, acc 0.382812\n",
      "2018-10-22T23:49:56.702929: step 2359, loss 1.66752, acc 0.367188\n",
      "2018-10-22T23:49:57.209574: step 2360, loss 1.76032, acc 0.320312\n",
      "2018-10-22T23:49:57.710235: step 2361, loss 1.74871, acc 0.359375\n",
      "2018-10-22T23:49:58.197931: step 2362, loss 1.77698, acc 0.320312\n",
      "2018-10-22T23:49:58.641745: step 2363, loss 1.7755, acc 0.273438\n",
      "2018-10-22T23:49:59.241142: step 2364, loss 1.72051, acc 0.328125\n",
      "2018-10-22T23:49:59.735821: step 2365, loss 1.82108, acc 0.3125\n",
      "2018-10-22T23:50:00.185617: step 2366, loss 1.64417, acc 0.382812\n",
      "2018-10-22T23:50:00.615467: step 2367, loss 1.68623, acc 0.34375\n",
      "2018-10-22T23:50:01.108150: step 2368, loss 1.70927, acc 0.375\n",
      "2018-10-22T23:50:01.610806: step 2369, loss 1.8057, acc 0.328125\n",
      "2018-10-22T23:50:02.046641: step 2370, loss 1.77405, acc 0.320312\n",
      "2018-10-22T23:50:02.522370: step 2371, loss 1.77229, acc 0.304688\n",
      "2018-10-22T23:50:03.014054: step 2372, loss 1.81312, acc 0.265625\n",
      "2018-10-22T23:50:03.587522: step 2373, loss 1.61193, acc 0.40625\n",
      "2018-10-22T23:50:04.035147: step 2374, loss 1.73196, acc 0.335938\n",
      "2018-10-22T23:50:04.541795: step 2375, loss 1.78172, acc 0.34375\n",
      "2018-10-22T23:50:04.988598: step 2376, loss 1.78261, acc 0.3125\n",
      "2018-10-22T23:50:05.411468: step 2377, loss 1.69051, acc 0.390625\n",
      "2018-10-22T23:50:05.887196: step 2378, loss 1.71451, acc 0.351562\n",
      "2018-10-22T23:50:06.392844: step 2379, loss 1.65039, acc 0.414062\n",
      "2018-10-22T23:50:06.932401: step 2380, loss 1.69309, acc 0.34375\n",
      "2018-10-22T23:50:07.443036: step 2381, loss 1.59075, acc 0.421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:50:07.975613: step 2382, loss 1.74838, acc 0.367188\n",
      "2018-10-22T23:50:08.434386: step 2383, loss 1.6491, acc 0.382812\n",
      "2018-10-22T23:50:08.909116: step 2384, loss 1.78334, acc 0.28125\n",
      "2018-10-22T23:50:09.367890: step 2385, loss 1.66533, acc 0.367188\n",
      "2018-10-22T23:50:09.861570: step 2386, loss 1.68438, acc 0.390625\n",
      "2018-10-22T23:50:10.287430: step 2387, loss 1.82526, acc 0.296875\n",
      "2018-10-22T23:50:10.813025: step 2388, loss 1.74245, acc 0.34375\n",
      "2018-10-22T23:50:11.343607: step 2389, loss 1.70932, acc 0.359375\n",
      "2018-10-22T23:50:11.793404: step 2390, loss 1.72285, acc 0.367188\n",
      "2018-10-22T23:50:12.245196: step 2391, loss 1.67197, acc 0.34375\n",
      "2018-10-22T23:50:12.695990: step 2392, loss 1.70911, acc 0.382812\n",
      "2018-10-22T23:50:13.164737: step 2393, loss 1.60092, acc 0.367188\n",
      "2018-10-22T23:50:13.594588: step 2394, loss 1.77611, acc 0.367188\n",
      "2018-10-22T23:50:14.067324: step 2395, loss 1.63646, acc 0.429688\n",
      "2018-10-22T23:50:14.493185: step 2396, loss 1.687, acc 0.375\n",
      "2018-10-22T23:50:14.966918: step 2397, loss 1.68262, acc 0.335938\n",
      "2018-10-22T23:50:15.389789: step 2398, loss 1.72595, acc 0.359375\n",
      "2018-10-22T23:50:15.857537: step 2399, loss 1.75432, acc 0.335938\n",
      "2018-10-22T23:50:16.311323: step 2400, loss 1.68531, acc 0.367188\n",
      "2018-10-22T23:50:16.821959: step 2401, loss 1.69416, acc 0.3125\n",
      "2018-10-22T23:50:17.338578: step 2402, loss 1.69781, acc 0.351562\n",
      "2018-10-22T23:50:17.761446: step 2403, loss 1.65457, acc 0.359375\n",
      "2018-10-22T23:50:18.227201: step 2404, loss 1.62115, acc 0.390625\n",
      "2018-10-22T23:50:18.657052: step 2405, loss 1.66164, acc 0.40625\n",
      "2018-10-22T23:50:19.096876: step 2406, loss 1.67046, acc 0.320312\n",
      "2018-10-22T23:50:19.542685: step 2407, loss 1.80964, acc 0.265625\n",
      "2018-10-22T23:50:20.034369: step 2408, loss 1.65423, acc 0.375\n",
      "2018-10-22T23:50:20.448262: step 2409, loss 1.74814, acc 0.296875\n",
      "2018-10-22T23:50:20.889086: step 2410, loss 1.69982, acc 0.34375\n",
      "2018-10-22T23:50:21.315943: step 2411, loss 1.65863, acc 0.398438\n",
      "2018-10-22T23:50:21.807629: step 2412, loss 1.59845, acc 0.445312\n",
      "2018-10-22T23:50:22.287345: step 2413, loss 1.69683, acc 0.34375\n",
      "2018-10-22T23:50:22.760081: step 2414, loss 1.65094, acc 0.414062\n",
      "2018-10-22T23:50:23.220850: step 2415, loss 1.78005, acc 0.304688\n",
      "2018-10-22T23:50:23.690593: step 2416, loss 1.75175, acc 0.273438\n",
      "2018-10-22T23:50:24.127425: step 2417, loss 1.74238, acc 0.351562\n",
      "2018-10-22T23:50:24.570241: step 2418, loss 1.69082, acc 0.320312\n",
      "2018-10-22T23:50:25.044972: step 2419, loss 1.76996, acc 0.328125\n",
      "2018-10-22T23:50:25.462855: step 2420, loss 1.68355, acc 0.320312\n",
      "2018-10-22T23:50:25.935590: step 2421, loss 1.62036, acc 0.390625\n",
      "2018-10-22T23:50:26.375415: step 2422, loss 1.8323, acc 0.296875\n",
      "2018-10-22T23:50:26.852358: step 2423, loss 1.80336, acc 0.28125\n",
      "2018-10-22T23:50:27.286197: step 2424, loss 1.54623, acc 0.515625\n",
      "2018-10-22T23:50:27.821766: step 2425, loss 1.60381, acc 0.46875\n",
      "2018-10-22T23:50:28.393238: step 2426, loss 1.66969, acc 0.398438\n",
      "2018-10-22T23:50:28.904871: step 2427, loss 1.84205, acc 0.28125\n",
      "2018-10-22T23:50:29.356663: step 2428, loss 1.74307, acc 0.335938\n",
      "2018-10-22T23:50:29.830395: step 2429, loss 1.62225, acc 0.398438\n",
      "2018-10-22T23:50:30.378930: step 2430, loss 1.6131, acc 0.421875\n",
      "2018-10-22T23:50:30.901531: step 2431, loss 1.66418, acc 0.40625\n",
      "2018-10-22T23:50:31.409174: step 2432, loss 1.74238, acc 0.273438\n",
      "2018-10-22T23:50:31.874652: step 2433, loss 1.8132, acc 0.328125\n",
      "2018-10-22T23:50:32.378304: step 2434, loss 1.6777, acc 0.359375\n",
      "2018-10-22T23:50:32.881958: step 2435, loss 1.58023, acc 0.382812\n",
      "2018-10-22T23:50:33.346715: step 2436, loss 1.70597, acc 0.335938\n",
      "2018-10-22T23:50:33.922177: step 2437, loss 1.70949, acc 0.367188\n",
      "2018-10-22T23:50:34.425829: step 2438, loss 1.75714, acc 0.34375\n",
      "2018-10-22T23:50:34.887595: step 2439, loss 1.72407, acc 0.382812\n",
      "2018-10-22T23:50:35.361328: step 2440, loss 1.70139, acc 0.359375\n",
      "2018-10-22T23:50:35.838054: step 2441, loss 1.67836, acc 0.34375\n",
      "2018-10-22T23:50:36.345697: step 2442, loss 1.61697, acc 0.382812\n",
      "2018-10-22T23:50:36.786518: step 2443, loss 1.66545, acc 0.40625\n",
      "2018-10-22T23:50:37.306129: step 2444, loss 1.69154, acc 0.34375\n",
      "2018-10-22T23:50:37.774875: step 2445, loss 1.65097, acc 0.367188\n",
      "2018-10-22T23:50:38.223677: step 2446, loss 1.7822, acc 0.3125\n",
      "2018-10-22T23:50:38.689430: step 2447, loss 1.60513, acc 0.390625\n",
      "2018-10-22T23:50:39.179120: step 2448, loss 1.68195, acc 0.320312\n",
      "2018-10-22T23:50:39.711697: step 2449, loss 1.74268, acc 0.3125\n",
      "2018-10-22T23:50:40.207372: step 2450, loss 1.70234, acc 0.335938\n",
      "2018-10-22T23:50:40.724988: step 2451, loss 1.75572, acc 0.328125\n",
      "2018-10-22T23:50:41.242603: step 2452, loss 1.5894, acc 0.414062\n",
      "2018-10-22T23:50:41.694395: step 2453, loss 1.69338, acc 0.390625\n",
      "2018-10-22T23:50:42.124246: step 2454, loss 1.60698, acc 0.429688\n",
      "2018-10-22T23:50:42.622912: step 2455, loss 1.73035, acc 0.390625\n",
      "2018-10-22T23:50:43.034812: step 2456, loss 1.71448, acc 0.351562\n",
      "2018-10-22T23:50:43.436737: step 2457, loss 1.59826, acc 0.382812\n",
      "2018-10-22T23:50:43.923435: step 2458, loss 1.73776, acc 0.304688\n",
      "2018-10-22T23:50:44.412128: step 2459, loss 1.56469, acc 0.414062\n",
      "2018-10-22T23:50:44.862923: step 2460, loss 1.71181, acc 0.335938\n",
      "2018-10-22T23:50:45.312721: step 2461, loss 1.58675, acc 0.4375\n",
      "2018-10-22T23:50:45.772492: step 2462, loss 1.67164, acc 0.382812\n",
      "2018-10-22T23:50:46.238246: step 2463, loss 1.6246, acc 0.398438\n",
      "2018-10-22T23:50:46.726940: step 2464, loss 1.58382, acc 0.421875\n",
      "2018-10-22T23:50:47.149808: step 2465, loss 1.74653, acc 0.335938\n",
      "2018-10-22T23:50:47.624542: step 2466, loss 1.69688, acc 0.390625\n",
      "2018-10-22T23:50:48.140161: step 2467, loss 1.65701, acc 0.390625\n",
      "2018-10-22T23:50:48.562033: step 2468, loss 1.70549, acc 0.335938\n",
      "2018-10-22T23:50:49.039756: step 2469, loss 1.63855, acc 0.40625\n",
      "2018-10-22T23:50:49.459633: step 2470, loss 1.66768, acc 0.34375\n",
      "2018-10-22T23:50:49.901451: step 2471, loss 1.75665, acc 0.359375\n",
      "2018-10-22T23:50:50.357233: step 2472, loss 1.87065, acc 0.273438\n",
      "2018-10-22T23:50:50.873851: step 2473, loss 1.69987, acc 0.414062\n",
      "2018-10-22T23:50:51.321654: step 2474, loss 1.63984, acc 0.390625\n",
      "2018-10-22T23:50:51.847248: step 2475, loss 1.74729, acc 0.3125\n",
      "2018-10-22T23:50:52.372844: step 2476, loss 1.64989, acc 0.390625\n",
      "2018-10-22T23:50:52.863531: step 2477, loss 1.68174, acc 0.382812\n",
      "2018-10-22T23:50:53.284406: step 2478, loss 1.5323, acc 0.445312\n",
      "2018-10-22T23:50:53.756144: step 2479, loss 1.59183, acc 0.445312\n",
      "2018-10-22T23:50:54.204944: step 2480, loss 1.63427, acc 0.421875\n",
      "2018-10-22T23:50:54.669702: step 2481, loss 1.64235, acc 0.367188\n",
      "2018-10-22T23:50:55.171361: step 2482, loss 1.68059, acc 0.335938\n",
      "2018-10-22T23:50:55.670027: step 2483, loss 1.70232, acc 0.398438\n",
      "2018-10-22T23:50:56.219557: step 2484, loss 1.60737, acc 0.40625\n",
      "2018-10-22T23:50:56.657388: step 2485, loss 1.73866, acc 0.296875\n",
      "2018-10-22T23:50:57.240828: step 2486, loss 1.73481, acc 0.367188\n",
      "2018-10-22T23:50:57.875133: step 2487, loss 1.70455, acc 0.34375\n",
      "2018-10-22T23:50:58.376790: step 2488, loss 1.69036, acc 0.304688\n",
      "2018-10-22T23:50:58.852518: step 2489, loss 1.60016, acc 0.398438\n",
      "2018-10-22T23:50:59.301318: step 2490, loss 1.80651, acc 0.273438\n",
      "2018-10-22T23:50:59.803975: step 2491, loss 1.66967, acc 0.390625\n",
      "2018-10-22T23:51:00.326577: step 2492, loss 1.73042, acc 0.351562\n",
      "2018-10-22T23:51:00.873115: step 2493, loss 1.69254, acc 0.351562\n",
      "2018-10-22T23:51:01.414668: step 2494, loss 1.70663, acc 0.328125\n",
      "2018-10-22T23:51:01.933280: step 2495, loss 1.76288, acc 0.304688\n",
      "2018-10-22T23:51:02.397040: step 2496, loss 1.7375, acc 0.3125\n",
      "2018-10-22T23:51:02.841851: step 2497, loss 1.55927, acc 0.414062\n",
      "2018-10-22T23:51:03.293643: step 2498, loss 1.77126, acc 0.351562\n",
      "2018-10-22T23:51:03.756406: step 2499, loss 1.67835, acc 0.335938\n",
      "2018-10-22T23:51:04.238118: step 2500, loss 1.56355, acc 0.445312\n",
      "\n",
      "Evaluation:\n",
      "Number of batches in dev set is 62\n",
      "batch 1 in dev >> 2018-10-22T23:51:05.781990: loss 1.69537, acc 0.346\n",
      "batch 2 in dev >> 2018-10-22T23:51:07.439558: loss 1.69556, acc 0.366\n",
      "batch 3 in dev >> 2018-10-22T23:51:09.082166: loss 1.65509, acc 0.38\n",
      "batch 4 in dev >> 2018-10-22T23:51:10.702832: loss 1.67278, acc 0.394\n",
      "batch 5 in dev >> 2018-10-22T23:51:12.399296: loss 1.65704, acc 0.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 6 in dev >> 2018-10-22T23:51:13.990043: loss 1.66072, acc 0.374\n",
      "batch 7 in dev >> 2018-10-22T23:51:15.598742: loss 1.67911, acc 0.386\n",
      "batch 8 in dev >> 2018-10-22T23:51:17.270272: loss 1.63498, acc 0.38\n",
      "batch 9 in dev >> 2018-10-22T23:51:18.943798: loss 1.69941, acc 0.344\n",
      "batch 10 in dev >> 2018-10-22T23:51:20.578428: loss 1.72684, acc 0.348\n",
      "batch 11 in dev >> 2018-10-22T23:51:22.305808: loss 1.6473, acc 0.406\n",
      "batch 12 in dev >> 2018-10-22T23:51:23.982326: loss 1.66498, acc 0.38\n",
      "batch 13 in dev >> 2018-10-22T23:51:25.809440: loss 1.65192, acc 0.366\n",
      "batch 14 in dev >> 2018-10-22T23:51:27.340347: loss 1.67367, acc 0.388\n",
      "batch 15 in dev >> 2018-10-22T23:51:28.932091: loss 1.67486, acc 0.366\n",
      "batch 16 in dev >> 2018-10-22T23:51:30.493915: loss 1.63729, acc 0.384\n",
      "batch 17 in dev >> 2018-10-22T23:51:32.089648: loss 1.68856, acc 0.37\n",
      "batch 18 in dev >> 2018-10-22T23:51:33.741232: loss 1.73123, acc 0.364\n",
      "batch 19 in dev >> 2018-10-22T23:51:35.250198: loss 1.66599, acc 0.368\n",
      "batch 20 in dev >> 2018-10-22T23:51:36.927713: loss 1.70152, acc 0.392\n",
      "batch 21 in dev >> 2018-10-22T23:51:38.574310: loss 1.64113, acc 0.384\n",
      "batch 22 in dev >> 2018-10-22T23:51:40.320641: loss 1.61776, acc 0.422\n",
      "batch 23 in dev >> 2018-10-22T23:51:42.202609: loss 1.69851, acc 0.328\n",
      "batch 24 in dev >> 2018-10-22T23:51:44.268086: loss 1.60381, acc 0.442\n",
      "batch 25 in dev >> 2018-10-22T23:51:46.386422: loss 1.67793, acc 0.36\n",
      "batch 26 in dev >> 2018-10-22T23:51:48.481664: loss 1.66674, acc 0.404\n",
      "batch 27 in dev >> 2018-10-22T23:51:50.670860: loss 1.70055, acc 0.368\n",
      "batch 28 in dev >> 2018-10-22T23:51:52.609677: loss 1.65937, acc 0.368\n",
      "batch 29 in dev >> 2018-10-22T23:51:54.335063: loss 1.72784, acc 0.362\n",
      "batch 30 in dev >> 2018-10-22T23:51:56.210050: loss 1.68485, acc 0.368\n",
      "batch 31 in dev >> 2018-10-22T23:51:57.908507: loss 1.68762, acc 0.382\n",
      "batch 32 in dev >> 2018-10-22T23:51:59.671793: loss 1.65354, acc 0.38\n",
      "batch 33 in dev >> 2018-10-22T23:52:01.422113: loss 1.69707, acc 0.346\n",
      "batch 34 in dev >> 2018-10-22T23:52:03.734929: loss 1.69641, acc 0.342\n",
      "batch 35 in dev >> 2018-10-22T23:52:05.632855: loss 1.7013, acc 0.356\n",
      "batch 36 in dev >> 2018-10-22T23:52:07.479916: loss 1.71383, acc 0.366\n",
      "batch 37 in dev >> 2018-10-22T23:52:09.221260: loss 1.64638, acc 0.37\n",
      "batch 38 in dev >> 2018-10-22T23:52:10.833952: loss 1.64006, acc 0.408\n",
      "batch 39 in dev >> 2018-10-22T23:52:12.369844: loss 1.69805, acc 0.358\n",
      "batch 40 in dev >> 2018-10-22T23:52:13.922692: loss 1.63924, acc 0.372\n",
      "batch 41 in dev >> 2018-10-22T23:52:15.614169: loss 1.68571, acc 0.364\n",
      "batch 42 in dev >> 2018-10-22T23:52:17.233839: loss 1.70967, acc 0.352\n",
      "batch 43 in dev >> 2018-10-22T23:52:18.826581: loss 1.69594, acc 0.35\n",
      "batch 44 in dev >> 2018-10-22T23:52:20.392393: loss 1.68523, acc 0.358\n",
      "batch 45 in dev >> 2018-10-22T23:52:22.158671: loss 1.69651, acc 0.368\n",
      "batch 46 in dev >> 2018-10-22T23:52:23.752410: loss 1.68999, acc 0.38\n",
      "batch 47 in dev >> 2018-10-22T23:52:25.407983: loss 1.63643, acc 0.406\n",
      "batch 48 in dev >> 2018-10-22T23:52:27.060564: loss 1.72546, acc 0.348\n",
      "batch 49 in dev >> 2018-10-22T23:52:28.815871: loss 1.68897, acc 0.354\n",
      "batch 50 in dev >> 2018-10-22T23:52:30.427561: loss 1.71237, acc 0.352\n",
      "batch 51 in dev >> 2018-10-22T23:52:32.220766: loss 1.71208, acc 0.352\n",
      "batch 52 in dev >> 2018-10-22T23:52:33.873349: loss 1.63847, acc 0.39\n",
      "batch 53 in dev >> 2018-10-22T23:52:35.736367: loss 1.63366, acc 0.376\n",
      "batch 54 in dev >> 2018-10-22T23:52:37.258297: loss 1.67383, acc 0.35\n",
      "batch 55 in dev >> 2018-10-22T23:52:38.921849: loss 1.67152, acc 0.324\n",
      "batch 56 in dev >> 2018-10-22T23:52:40.592383: loss 1.66626, acc 0.376\n",
      "batch 57 in dev >> 2018-10-22T23:52:42.205071: loss 1.7138, acc 0.334\n",
      "batch 58 in dev >> 2018-10-22T23:52:43.774873: loss 1.6548, acc 0.392\n",
      "batch 59 in dev >> 2018-10-22T23:52:45.438425: loss 1.66464, acc 0.386\n",
      "batch 60 in dev >> 2018-10-22T23:52:47.093001: loss 1.67969, acc 0.354\n",
      "batch 61 in dev >> 2018-10-22T23:52:48.701700: loss 1.68032, acc 0.364\n",
      "batch 62 in dev >> 2018-10-22T23:52:50.413125: loss 1.68423, acc 0.356\n",
      "\n",
      "Mean accuracy=0.3699032246105133\n",
      "Mean loss=1.6768667736361105\n",
      "\n",
      "2018-10-22T23:52:50.914782: step 2501, loss 1.6263, acc 0.398438\n",
      "2018-10-22T23:52:51.345631: step 2502, loss 1.62897, acc 0.367188\n",
      "2018-10-22T23:52:51.770495: step 2503, loss 1.70013, acc 0.40625\n",
      "2018-10-22T23:52:52.283124: step 2504, loss 1.66153, acc 0.359375\n",
      "2018-10-22T23:52:52.868444: step 2505, loss 1.79889, acc 0.296875\n",
      "2018-10-22T23:52:53.336194: step 2506, loss 1.68178, acc 0.351562\n",
      "2018-10-22T23:52:53.782001: step 2507, loss 1.65616, acc 0.382812\n",
      "2018-10-22T23:52:54.209858: step 2508, loss 1.62841, acc 0.4375\n",
      "2018-10-22T23:52:54.685586: step 2509, loss 1.66296, acc 0.359375\n",
      "2018-10-22T23:52:55.142364: step 2510, loss 1.62325, acc 0.382812\n",
      "2018-10-22T23:52:55.610114: step 2511, loss 1.6276, acc 0.398438\n",
      "2018-10-22T23:52:56.152663: step 2512, loss 1.73162, acc 0.328125\n",
      "2018-10-22T23:52:56.577528: step 2513, loss 1.54881, acc 0.460938\n",
      "2018-10-22T23:52:56.987430: step 2514, loss 1.68974, acc 0.328125\n",
      "2018-10-22T23:52:57.441217: step 2515, loss 1.77234, acc 0.34375\n",
      "2018-10-22T23:52:57.931905: step 2516, loss 1.72998, acc 0.359375\n",
      "2018-10-22T23:52:58.402647: step 2517, loss 1.66336, acc 0.359375\n",
      "2018-10-22T23:52:58.861421: step 2518, loss 1.67554, acc 0.421875\n",
      "2018-10-22T23:52:59.406962: step 2519, loss 1.70322, acc 0.351562\n",
      "2018-10-22T23:52:59.949511: step 2520, loss 1.81227, acc 0.289062\n",
      "2018-10-22T23:53:00.423244: step 2521, loss 1.62631, acc 0.40625\n",
      "2018-10-22T23:53:00.950833: step 2522, loss 1.71974, acc 0.328125\n",
      "2018-10-22T23:53:01.429554: step 2523, loss 1.70492, acc 0.367188\n",
      "2018-10-22T23:53:01.916252: step 2524, loss 1.76842, acc 0.289062\n",
      "2018-10-22T23:53:02.328151: step 2525, loss 1.74078, acc 0.320312\n",
      "2018-10-22T23:53:02.787922: step 2526, loss 1.73655, acc 0.320312\n",
      "2018-10-22T23:53:03.299554: step 2527, loss 1.721, acc 0.359375\n",
      "2018-10-22T23:53:03.775282: step 2528, loss 1.72201, acc 0.382812\n",
      "2018-10-22T23:53:04.253004: step 2529, loss 1.63452, acc 0.375\n",
      "2018-10-22T23:53:04.709783: step 2530, loss 1.72827, acc 0.359375\n",
      "2018-10-22T23:53:05.151601: step 2531, loss 1.52984, acc 0.492188\n",
      "2018-10-22T23:53:05.630322: step 2532, loss 1.78306, acc 0.335938\n",
      "2018-10-22T23:53:06.095079: step 2533, loss 1.64439, acc 0.382812\n",
      "2018-10-22T23:53:06.649596: step 2534, loss 1.74643, acc 0.320312\n",
      "2018-10-22T23:53:07.115351: step 2535, loss 1.65052, acc 0.351562\n",
      "2018-10-22T23:53:07.644935: step 2536, loss 1.62257, acc 0.398438\n",
      "2018-10-22T23:53:08.168535: step 2537, loss 1.65683, acc 0.359375\n",
      "2018-10-22T23:53:08.669196: step 2538, loss 1.7046, acc 0.359375\n",
      "2018-10-22T23:53:09.213741: step 2539, loss 1.66821, acc 0.34375\n",
      "2018-10-22T23:53:09.794189: step 2540, loss 1.6745, acc 0.359375\n",
      "2018-10-22T23:53:10.332749: step 2541, loss 1.5683, acc 0.46875\n",
      "2018-10-22T23:53:10.745644: step 2542, loss 1.65964, acc 0.421875\n",
      "2018-10-22T23:53:11.190455: step 2543, loss 1.66254, acc 0.398438\n",
      "2018-10-22T23:53:11.652221: step 2544, loss 1.67548, acc 0.453125\n",
      "2018-10-22T23:53:12.070103: step 2545, loss 1.76669, acc 0.3125\n",
      "2018-10-22T23:53:12.529874: step 2546, loss 1.65211, acc 0.335938\n",
      "2018-10-22T23:53:13.018567: step 2547, loss 1.65003, acc 0.40625\n",
      "2018-10-22T23:53:13.517234: step 2548, loss 1.71904, acc 0.375\n",
      "2018-10-22T23:53:13.984983: step 2549, loss 1.6462, acc 0.375\n",
      "2018-10-22T23:53:14.426802: step 2550, loss 1.63172, acc 0.390625\n",
      "2018-10-22T23:53:14.836705: step 2551, loss 1.71967, acc 0.3125\n",
      "2018-10-22T23:53:15.284509: step 2552, loss 1.63223, acc 0.4375\n",
      "2018-10-22T23:53:15.753254: step 2553, loss 1.70182, acc 0.328125\n",
      "2018-10-22T23:53:16.236961: step 2554, loss 1.70305, acc 0.335938\n",
      "2018-10-22T23:53:16.764552: step 2555, loss 1.71563, acc 0.359375\n",
      "2018-10-22T23:53:17.270199: step 2556, loss 1.67737, acc 0.335938\n",
      "2018-10-22T23:53:17.716008: step 2557, loss 1.61026, acc 0.351562\n",
      "2018-10-22T23:53:18.188743: step 2558, loss 1.69768, acc 0.328125\n",
      "2018-10-22T23:53:18.618594: step 2559, loss 1.69537, acc 0.40625\n",
      "2018-10-22T23:53:19.066396: step 2560, loss 1.64944, acc 0.359375\n",
      "2018-10-22T23:53:19.500236: step 2561, loss 1.57631, acc 0.429688\n",
      "2018-10-22T23:53:19.945047: step 2562, loss 1.67756, acc 0.328125\n",
      "2018-10-22T23:53:20.426759: step 2563, loss 1.70544, acc 0.335938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:53:20.895507: step 2564, loss 1.69499, acc 0.335938\n",
      "2018-10-22T23:53:21.336326: step 2565, loss 1.70679, acc 0.359375\n",
      "2018-10-22T23:53:21.804076: step 2566, loss 1.76417, acc 0.382812\n",
      "2018-10-22T23:53:22.309724: step 2567, loss 1.57991, acc 0.4375\n",
      "2018-10-22T23:53:22.817367: step 2568, loss 1.75173, acc 0.296875\n",
      "2018-10-22T23:53:23.286113: step 2569, loss 1.75693, acc 0.328125\n",
      "2018-10-22T23:53:23.815698: step 2570, loss 1.77571, acc 0.3125\n",
      "2018-10-22T23:53:24.287437: step 2571, loss 1.75574, acc 0.367188\n",
      "2018-10-22T23:53:24.786103: step 2572, loss 1.77153, acc 0.328125\n",
      "2018-10-22T23:53:25.268812: step 2573, loss 1.67012, acc 0.375\n",
      "2018-10-22T23:53:25.696669: step 2574, loss 1.71858, acc 0.3125\n",
      "2018-10-22T23:53:26.177382: step 2575, loss 1.6281, acc 0.390625\n",
      "2018-10-22T23:53:26.620199: step 2576, loss 1.69687, acc 0.34375\n",
      "2018-10-22T23:53:27.101911: step 2577, loss 1.68433, acc 0.34375\n",
      "2018-10-22T23:53:27.611548: step 2578, loss 1.75952, acc 0.328125\n",
      "2018-10-22T23:53:28.029430: step 2579, loss 1.61079, acc 0.414062\n",
      "2018-10-22T23:53:28.429361: step 2580, loss 1.84463, acc 0.273438\n",
      "2018-10-22T23:53:28.851234: step 2581, loss 1.73062, acc 0.351562\n",
      "2018-10-22T23:53:29.279090: step 2582, loss 1.65292, acc 0.351562\n",
      "2018-10-22T23:53:29.747837: step 2583, loss 1.69034, acc 0.34375\n",
      "2018-10-22T23:53:30.213591: step 2584, loss 1.72307, acc 0.328125\n",
      "2018-10-22T23:53:30.723228: step 2585, loss 1.55583, acc 0.375\n",
      "2018-10-22T23:53:31.179010: step 2586, loss 1.66678, acc 0.390625\n",
      "2018-10-22T23:53:31.614845: step 2587, loss 1.59338, acc 0.40625\n",
      "2018-10-22T23:53:32.078604: step 2588, loss 1.62855, acc 0.390625\n",
      "2018-10-22T23:53:32.532391: step 2589, loss 1.76114, acc 0.351562\n",
      "2018-10-22T23:53:32.955260: step 2590, loss 1.86746, acc 0.265625\n",
      "2018-10-22T23:53:33.404059: step 2591, loss 1.66352, acc 0.335938\n",
      "2018-10-22T23:53:33.860214: step 2592, loss 1.62986, acc 0.390625\n",
      "2018-10-22T23:53:34.372728: step 2593, loss 1.55292, acc 0.453125\n",
      "2018-10-22T23:53:34.885358: step 2594, loss 1.64695, acc 0.359375\n",
      "2018-10-22T23:53:35.349119: step 2595, loss 1.68985, acc 0.328125\n",
      "2018-10-22T23:53:35.787945: step 2596, loss 1.68349, acc 0.4375\n",
      "2018-10-22T23:53:36.187876: step 2597, loss 1.76769, acc 0.359375\n",
      "2018-10-22T23:53:36.613737: step 2598, loss 1.67382, acc 0.382812\n",
      "2018-10-22T23:53:37.027630: step 2599, loss 1.71114, acc 0.335938\n",
      "2018-10-22T23:53:37.447509: step 2600, loss 1.65334, acc 0.359375\n",
      "2018-10-22T23:53:37.904287: step 2601, loss 1.68424, acc 0.367188\n",
      "2018-10-22T23:53:38.339124: step 2602, loss 1.61235, acc 0.4375\n",
      "2018-10-22T23:53:38.815850: step 2603, loss 1.77416, acc 0.296875\n",
      "2018-10-22T23:53:39.382335: step 2604, loss 1.63884, acc 0.335938\n",
      "2018-10-22T23:53:39.935855: step 2605, loss 1.62367, acc 0.421875\n",
      "2018-10-22T23:53:40.489374: step 2606, loss 1.78552, acc 0.289062\n",
      "2018-10-22T23:53:41.035912: step 2607, loss 1.80262, acc 0.242188\n",
      "2018-10-22T23:53:41.696147: step 2608, loss 1.66249, acc 0.34375\n",
      "2018-10-22T23:53:42.356382: step 2609, loss 1.65043, acc 0.40625\n",
      "2018-10-22T23:53:42.898932: step 2610, loss 1.65748, acc 0.421875\n",
      "2018-10-22T23:53:43.480379: step 2611, loss 1.60858, acc 0.421875\n",
      "2018-10-22T23:53:44.029907: step 2612, loss 1.7555, acc 0.296875\n",
      "2018-10-22T23:53:44.548522: step 2613, loss 1.63349, acc 0.4375\n",
      "2018-10-22T23:53:45.078105: step 2614, loss 1.7713, acc 0.328125\n",
      "2018-10-22T23:53:45.616667: step 2615, loss 1.66227, acc 0.414062\n",
      "2018-10-22T23:53:46.179160: step 2616, loss 1.62498, acc 0.375\n",
      "2018-10-22T23:53:46.693785: step 2617, loss 1.79673, acc 0.320312\n",
      "2018-10-22T23:53:47.224366: step 2618, loss 1.73387, acc 0.335938\n",
      "2018-10-22T23:53:47.766917: step 2619, loss 1.81642, acc 0.328125\n",
      "2018-10-22T23:53:48.355341: step 2620, loss 1.72559, acc 0.34375\n",
      "2018-10-22T23:53:49.027545: step 2621, loss 1.6467, acc 0.398438\n",
      "2018-10-22T23:53:49.648884: step 2622, loss 1.63597, acc 0.34375\n",
      "2018-10-22T23:53:50.181460: step 2623, loss 1.65824, acc 0.34375\n",
      "2018-10-22T23:53:50.646217: step 2624, loss 1.69881, acc 0.375\n",
      "2018-10-22T23:53:51.111971: step 2625, loss 1.75199, acc 0.320312\n",
      "2018-10-22T23:53:51.665492: step 2626, loss 1.72667, acc 0.34375\n",
      "2018-10-22T23:53:52.150196: step 2627, loss 1.58707, acc 0.414062\n",
      "2018-10-22T23:53:52.622932: step 2628, loss 1.68726, acc 0.359375\n",
      "2018-10-22T23:53:53.182437: step 2629, loss 1.6978, acc 0.328125\n",
      "2018-10-22T23:53:53.647194: step 2630, loss 1.75249, acc 0.367188\n",
      "2018-10-22T23:53:54.094995: step 2631, loss 1.72139, acc 0.359375\n",
      "2018-10-22T23:53:54.532825: step 2632, loss 1.73312, acc 0.304688\n",
      "2018-10-22T23:53:54.986612: step 2633, loss 1.64976, acc 0.414062\n",
      "2018-10-22T23:53:55.470318: step 2634, loss 1.59646, acc 0.40625\n",
      "2018-10-22T23:53:55.929091: step 2635, loss 1.66245, acc 0.375\n",
      "2018-10-22T23:53:56.438729: step 2636, loss 1.60014, acc 0.40625\n",
      "2018-10-22T23:53:56.919444: step 2637, loss 1.73039, acc 0.359375\n",
      "2018-10-22T23:53:57.367247: step 2638, loss 1.79584, acc 0.335938\n",
      "2018-10-22T23:53:57.796099: step 2639, loss 1.68476, acc 0.359375\n",
      "2018-10-22T23:53:58.217972: step 2640, loss 1.60693, acc 0.398438\n",
      "2018-10-22T23:53:58.733593: step 2641, loss 1.64249, acc 0.375\n",
      "2018-10-22T23:53:59.197353: step 2642, loss 1.65206, acc 0.34375\n",
      "2018-10-22T23:53:59.682057: step 2643, loss 1.75309, acc 0.304688\n",
      "2018-10-22T23:54:00.177732: step 2644, loss 1.6806, acc 0.398438\n",
      "2018-10-22T23:54:00.629524: step 2645, loss 1.75168, acc 0.335938\n",
      "2018-10-22T23:54:01.055385: step 2646, loss 1.71201, acc 0.335938\n",
      "2018-10-22T23:54:01.506180: step 2647, loss 1.62829, acc 0.4375\n",
      "2018-10-22T23:54:01.999861: step 2648, loss 1.64515, acc 0.398438\n",
      "2018-10-22T23:54:02.468607: step 2649, loss 1.707, acc 0.359375\n",
      "2018-10-22T23:54:02.919401: step 2650, loss 1.625, acc 0.367188\n",
      "2018-10-22T23:54:03.372190: step 2651, loss 1.69645, acc 0.359375\n",
      "2018-10-22T23:54:03.805032: step 2652, loss 1.70612, acc 0.414062\n",
      "2018-10-22T23:54:04.200974: step 2653, loss 1.71357, acc 0.382812\n",
      "2018-10-22T23:54:04.630825: step 2654, loss 1.69817, acc 0.375\n",
      "2018-10-22T23:54:05.064665: step 2655, loss 1.65707, acc 0.328125\n",
      "2018-10-22T23:54:05.636137: step 2656, loss 1.71251, acc 0.367188\n",
      "2018-10-22T23:54:06.106878: step 2657, loss 1.6298, acc 0.367188\n",
      "2018-10-22T23:54:06.591582: step 2658, loss 1.74227, acc 0.3125\n",
      "2018-10-22T23:54:07.066319: step 2659, loss 1.63286, acc 0.382812\n",
      "2018-10-22T23:54:07.504812: step 2660, loss 1.59944, acc 0.375\n",
      "2018-10-22T23:54:07.945633: step 2661, loss 1.81003, acc 0.273438\n",
      "2018-10-22T23:54:08.403409: step 2662, loss 1.56953, acc 0.398438\n",
      "2018-10-22T23:54:08.948950: step 2663, loss 1.64083, acc 0.382812\n",
      "2018-10-22T23:54:09.407724: step 2664, loss 1.62799, acc 0.398438\n",
      "2018-10-22T23:54:09.929330: step 2665, loss 1.67939, acc 0.375\n",
      "2018-10-22T23:54:10.369153: step 2666, loss 1.63699, acc 0.414062\n",
      "2018-10-22T23:54:10.810972: step 2667, loss 1.65397, acc 0.359375\n",
      "2018-10-22T23:54:11.287698: step 2668, loss 1.67431, acc 0.40625\n",
      "2018-10-22T23:54:11.740487: step 2669, loss 1.64963, acc 0.367188\n",
      "2018-10-22T23:54:12.248130: step 2670, loss 1.88807, acc 0.28125\n",
      "2018-10-22T23:54:12.761756: step 2671, loss 1.69471, acc 0.335938\n",
      "2018-10-22T23:54:13.248455: step 2672, loss 1.80838, acc 0.335938\n",
      "2018-10-22T23:54:13.730167: step 2673, loss 1.73323, acc 0.351562\n",
      "2018-10-22T23:54:14.205895: step 2674, loss 1.60706, acc 0.382812\n",
      "2018-10-22T23:54:14.705558: step 2675, loss 1.67209, acc 0.382812\n",
      "2018-10-22T23:54:15.256089: step 2676, loss 1.6869, acc 0.390625\n",
      "2018-10-22T23:54:15.816588: step 2677, loss 1.71496, acc 0.382812\n",
      "2018-10-22T23:54:16.267383: step 2678, loss 1.72748, acc 0.335938\n",
      "2018-10-22T23:54:16.692247: step 2679, loss 1.69934, acc 0.390625\n",
      "2018-10-22T23:54:17.138054: step 2680, loss 1.5923, acc 0.445312\n",
      "2018-10-22T23:54:17.563917: step 2681, loss 1.61149, acc 0.40625\n",
      "2018-10-22T23:54:18.033661: step 2682, loss 1.59555, acc 0.414062\n",
      "2018-10-22T23:54:18.455533: step 2683, loss 1.80039, acc 0.328125\n",
      "2018-10-22T23:54:18.889372: step 2684, loss 1.65753, acc 0.429688\n",
      "2018-10-22T23:54:19.365100: step 2685, loss 1.70968, acc 0.320312\n",
      "2018-10-22T23:54:19.820881: step 2686, loss 1.68878, acc 0.359375\n",
      "2018-10-22T23:54:20.313564: step 2687, loss 1.77151, acc 0.296875\n",
      "2018-10-22T23:54:20.772338: step 2688, loss 1.57865, acc 0.429688\n",
      "2018-10-22T23:54:21.225126: step 2689, loss 1.81532, acc 0.328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22T23:54:21.680908: step 2690, loss 1.68834, acc 0.359375\n",
      "2018-10-22T23:54:22.099789: step 2691, loss 1.68472, acc 0.390625\n",
      "2018-10-22T23:54:22.529639: step 2692, loss 1.75717, acc 0.304688\n",
      "2018-10-22T23:54:23.014342: step 2693, loss 1.69142, acc 0.351562\n",
      "2018-10-22T23:54:23.446188: step 2694, loss 1.69274, acc 0.367188\n",
      "2018-10-22T23:54:23.946849: step 2695, loss 1.73501, acc 0.359375\n",
      "2018-10-22T23:54:24.396647: step 2696, loss 1.56069, acc 0.445312\n",
      "2018-10-22T23:54:24.817522: step 2697, loss 1.68229, acc 0.367188\n",
      "2018-10-22T23:54:25.282279: step 2698, loss 1.71529, acc 0.375\n",
      "2018-10-22T23:54:25.757009: step 2699, loss 1.6632, acc 0.382812\n",
      "2018-10-22T23:54:26.290583: step 2700, loss 1.64254, acc 0.375\n",
      "2018-10-22T23:54:26.763319: step 2701, loss 1.7772, acc 0.335938\n",
      "2018-10-22T23:54:27.239047: step 2702, loss 1.71138, acc 0.351562\n",
      "2018-10-22T23:54:27.686849: step 2703, loss 1.5887, acc 0.421875\n",
      "2018-10-22T23:54:28.166568: step 2704, loss 1.74885, acc 0.34375\n",
      "2018-10-22T23:54:28.679196: step 2705, loss 1.65987, acc 0.414062\n",
      "2018-10-22T23:54:29.107051: step 2706, loss 1.70486, acc 0.40625\n",
      "2018-10-22T23:54:29.615692: step 2707, loss 1.70509, acc 0.40625\n",
      "2018-10-22T23:54:30.114358: step 2708, loss 1.73647, acc 0.335938\n",
      "2018-10-22T23:54:30.580114: step 2709, loss 1.69072, acc 0.304688\n",
      "2018-10-22T23:54:31.068808: step 2710, loss 1.71411, acc 0.382812\n",
      "2018-10-22T23:54:31.589414: step 2711, loss 1.70167, acc 0.382812\n",
      "2018-10-22T23:54:32.120993: step 2712, loss 1.70611, acc 0.359375\n",
      "2018-10-22T23:54:32.635617: step 2713, loss 1.82723, acc 0.265625\n",
      "2018-10-22T23:54:33.134284: step 2714, loss 1.69355, acc 0.320312\n",
      "2018-10-22T23:54:33.613004: step 2715, loss 1.74969, acc 0.34375\n",
      "2018-10-22T23:54:34.101698: step 2716, loss 1.73729, acc 0.375\n",
      "2018-10-22T23:54:34.557478: step 2717, loss 1.62533, acc 0.390625\n",
      "2018-10-22T23:54:35.033208: step 2718, loss 1.69102, acc 0.414062\n",
      "2018-10-22T23:54:35.472033: step 2719, loss 1.69033, acc 0.34375\n",
      "2018-10-22T23:54:35.907869: step 2720, loss 1.5345, acc 0.421875\n",
      "2018-10-22T23:54:36.348690: step 2721, loss 1.70294, acc 0.367188\n",
      "2018-10-22T23:54:36.839377: step 2722, loss 1.66185, acc 0.375\n",
      "2018-10-22T23:54:37.252274: step 2723, loss 1.71252, acc 0.351562\n",
      "2018-10-22T23:54:37.696086: step 2724, loss 1.80849, acc 0.328125\n",
      "2018-10-22T23:54:38.147878: step 2725, loss 1.59909, acc 0.429688\n",
      "2018-10-22T23:54:38.715361: step 2726, loss 1.65168, acc 0.328125\n",
      "2018-10-22T23:54:39.200066: step 2727, loss 1.69524, acc 0.375\n",
      "2018-10-22T23:54:39.677788: step 2728, loss 1.64252, acc 0.398438\n",
      "2018-10-22T23:54:40.221335: step 2729, loss 1.58674, acc 0.453125\n",
      "2018-10-22T23:54:40.665148: step 2730, loss 1.72196, acc 0.351562\n",
      "2018-10-22T23:54:41.101981: step 2731, loss 1.60874, acc 0.414062\n",
      "2018-10-22T23:54:41.572722: step 2732, loss 1.6072, acc 0.398438\n",
      "2018-10-22T23:54:42.081363: step 2733, loss 1.73213, acc 0.328125\n",
      "2018-10-22T23:54:42.567063: step 2734, loss 1.60303, acc 0.398438\n",
      "2018-10-22T23:54:43.009879: step 2735, loss 1.66, acc 0.398438\n",
      "2018-10-22T23:54:43.468652: step 2736, loss 1.77959, acc 0.335938\n",
      "2018-10-22T23:54:43.984274: step 2737, loss 1.92098, acc 0.265625\n",
      "2018-10-22T23:54:44.434071: step 2738, loss 1.58575, acc 0.40625\n",
      "2018-10-22T23:54:44.887858: step 2739, loss 1.66533, acc 0.3125\n",
      "2018-10-22T23:54:45.307734: step 2740, loss 1.64216, acc 0.398438\n",
      "2018-10-22T23:54:45.712652: step 2741, loss 1.7295, acc 0.375\n",
      "2018-10-22T23:54:46.141505: step 2742, loss 1.69058, acc 0.351562\n",
      "2018-10-22T23:54:46.598285: step 2743, loss 1.66079, acc 0.367188\n",
      "2018-10-22T23:54:47.140834: step 2744, loss 1.77523, acc 0.265625\n",
      "2018-10-22T23:54:47.563703: step 2745, loss 1.63172, acc 0.445312\n",
      "2018-10-22T23:54:48.006519: step 2746, loss 1.68395, acc 0.351562\n",
      "2018-10-22T23:54:48.447340: step 2747, loss 1.68636, acc 0.382812\n",
      "2018-10-22T23:54:48.888161: step 2748, loss 1.70008, acc 0.34375\n",
      "2018-10-22T23:54:49.342946: step 2749, loss 1.56421, acc 0.390625\n",
      "2018-10-22T23:54:49.816678: step 2750, loss 1.63299, acc 0.382812\n",
      "2018-10-22T23:54:50.277447: step 2751, loss 1.60553, acc 0.359375\n",
      "2018-10-22T23:54:50.799053: step 2752, loss 1.71387, acc 0.328125\n",
      "2018-10-22T23:54:51.258824: step 2753, loss 1.71025, acc 0.398438\n",
      "2018-10-22T23:54:51.749511: step 2754, loss 1.7421, acc 0.367188\n",
      "2018-10-22T23:54:52.269533: step 2755, loss 1.72182, acc 0.335938\n",
      "2018-10-22T23:54:52.765435: step 2756, loss 1.66982, acc 0.398438\n",
      "2018-10-22T23:54:53.205259: step 2757, loss 1.569, acc 0.460938\n",
      "2018-10-22T23:54:53.656053: step 2758, loss 1.55098, acc 0.398438\n",
      "2018-10-22T23:54:54.151728: step 2759, loss 1.66878, acc 0.367188\n",
      "2018-10-22T23:54:54.599530: step 2760, loss 1.77996, acc 0.265625\n",
      "2018-10-22T23:54:55.078251: step 2761, loss 1.66343, acc 0.390625\n",
      "2018-10-22T23:54:55.574923: step 2762, loss 1.71461, acc 0.351562\n",
      "2018-10-22T23:54:56.042672: step 2763, loss 1.74929, acc 0.296875\n",
      "2018-10-22T23:54:56.552310: step 2764, loss 1.57902, acc 0.40625\n",
      "2018-10-22T23:54:57.137744: step 2765, loss 1.5856, acc 0.460938\n",
      "2018-10-22T23:54:57.690267: step 2766, loss 1.70562, acc 0.367188\n",
      "2018-10-22T23:54:58.187937: step 2767, loss 1.59938, acc 0.390625\n",
      "2018-10-22T23:54:58.637734: step 2768, loss 1.62829, acc 0.335938\n",
      "2018-10-22T23:54:59.095510: step 2769, loss 1.73136, acc 0.375\n",
      "2018-10-22T23:54:59.577221: step 2770, loss 1.74647, acc 0.359375\n",
      "2018-10-22T23:54:59.991115: step 2771, loss 1.80069, acc 0.328125\n",
      "2018-10-22T23:55:00.412987: step 2772, loss 1.75455, acc 0.328125\n",
      "2018-10-22T23:55:00.924620: step 2773, loss 1.66896, acc 0.335938\n",
      "2018-10-22T23:55:01.398353: step 2774, loss 1.69945, acc 0.390625\n",
      "2018-10-22T23:55:01.933920: step 2775, loss 1.78766, acc 0.328125\n",
      "2018-10-22T23:55:02.442560: step 2776, loss 1.54083, acc 0.421875\n",
      "2018-10-22T23:55:02.925270: step 2777, loss 1.75916, acc 0.328125\n",
      "2018-10-22T23:55:03.386038: step 2778, loss 1.72833, acc 0.351562\n",
      "2018-10-22T23:55:03.838827: step 2779, loss 1.61961, acc 0.414062\n",
      "2018-10-22T23:55:04.388357: step 2780, loss 1.6716, acc 0.367188\n",
      "2018-10-22T23:55:04.818209: step 2781, loss 1.6943, acc 0.328125\n",
      "2018-10-22T23:55:05.289947: step 2782, loss 1.65533, acc 0.382812\n",
      "2018-10-22T23:55:05.782629: step 2783, loss 1.71171, acc 0.34375\n",
      "2018-10-22T23:55:06.266336: step 2784, loss 1.78439, acc 0.273438\n",
      "2018-10-22T23:55:06.721120: step 2785, loss 1.59693, acc 0.40625\n",
      "2018-10-22T23:55:07.192859: step 2786, loss 1.78519, acc 0.296875\n",
      "2018-10-22T23:55:07.673574: step 2787, loss 1.70914, acc 0.328125\n",
      "2018-10-22T23:55:08.102427: step 2788, loss 1.73978, acc 0.382812\n",
      "2018-10-22T23:55:08.579152: step 2789, loss 1.7848, acc 0.265625\n",
      "2018-10-22T23:55:09.072832: step 2790, loss 1.71904, acc 0.367188\n",
      "2018-10-22T23:55:09.575489: step 2791, loss 1.65281, acc 0.367188\n",
      "2018-10-22T23:55:10.122026: step 2792, loss 1.71229, acc 0.382812\n",
      "2018-10-22T23:55:10.619697: step 2793, loss 1.63539, acc 0.382812\n",
      "2018-10-22T23:55:11.168230: step 2794, loss 1.65066, acc 0.40625\n",
      "2018-10-22T23:55:11.644955: step 2795, loss 1.78945, acc 0.28125\n",
      "2018-10-22T23:55:12.068821: step 2796, loss 1.63442, acc 0.414062\n",
      "2018-10-22T23:55:12.534576: step 2797, loss 1.63998, acc 0.351562\n",
      "2018-10-22T23:55:13.067152: step 2798, loss 1.66029, acc 0.421875\n",
      "2018-10-22T23:55:13.157909: step 2799, loss 1.68786, acc 0.357143\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "dropout_keep_prob=0.5\n",
    "l2_reg_lambda=0.0\n",
    "\n",
    "# Training parameters\n",
    "batch_size=128\n",
    "num_epochs=1\n",
    "evaluate_every=500\n",
    "checkpoint_every=100\n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\\n\"\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x, y = myload()\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "# Split train/test set\n",
    "n_dev_samples = int(0.08*len(x))-1\n",
    "# TODO: Create a fuckin' correct cross validation procedure\n",
    "x_train, x_dev = x_shuffled[:-n_dev_samples], x_shuffled[-n_dev_samples:]\n",
    "y_train, y_dev = y_shuffled[:-n_dev_samples], y_shuffled[-n_dev_samples:]\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "tf.reset_default_graph()\n",
    "session_config=tf.ConfigProto( gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5))\n",
    "session_config.gpu_options.allow_growth = True\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session(config=session_config) as sess:   \n",
    "    sess.run(init)\n",
    "    \n",
    "    cnn = CharCNN(num_classes=7,sequence_max_length=1014)\n",
    "    saver=tf.train.Saver()\n",
    "    # Define Training procedure\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    def train_step(x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "          cnn.input_x: x_batch,\n",
    "          cnn.input_y: y_batch,\n",
    "          cnn.dropout_keep_prob: dropout_keep_prob\n",
    "        }\n",
    "        _, step, loss, accuracy = sess.run(\n",
    "            [train_op, global_step, cnn.loss, cnn.accuracy],\n",
    "            feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "    def dev_step(x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Evaluates model on a dev set\n",
    "        \"\"\"\n",
    "        dev_size = len(x_batch)\n",
    "        max_batch_size = 500\n",
    "        num_batches = int(dev_size/max_batch_size)\n",
    "        acc = []\n",
    "        losses = []\n",
    "        print(\"Number of batches in dev set is \" + str(num_batches))\n",
    "        for i in range(num_batches):\n",
    "            x_batch_dev, y_batch_dev = preprocessing.get_batched_one_hot(\n",
    "                x_batch, y_batch, i * max_batch_size, (i + 1) * max_batch_size)\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch_dev,\n",
    "              cnn.input_y: y_batch_dev,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, loss, accuracy = sess.run(\n",
    "                [global_step,  cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            acc.append(accuracy)\n",
    "            losses.append(loss)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"batch \" + str(i + 1) + \" in dev >>\" +\n",
    "                  \" {}: loss {:g}, acc {:g}\".format(time_str, loss, accuracy))\n",
    "        print(\"\\nMean accuracy=\" + str(sum(acc)/len(acc)))\n",
    "        print(\"Mean loss=\" + str(sum(losses)/len(losses)))\n",
    "\n",
    "\n",
    "    # Generate batches in one-hot-encoding format\n",
    "    batches = preprocessing.batch_iter(x_train, y_train, batch_size, num_epochs)\n",
    "    # Training loop. For each batch...\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        train_step(x_batch, y_batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % evaluate_every == 0:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            dev_step(x_dev, y_dev)\n",
    "            print(\"\")\n",
    "            \n",
    "    saver.save(sess, \"./model/train.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_transform(values):\n",
    "    alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\\n\"\n",
    "    text_end_extracted=preprocessing.extract_end(list(values.lower()))\n",
    "    padded=preprocessing.pad_sentence(text_end_extracted)\n",
    "    text_int8_repr = preprocessing.string_to_int8_conversion(padded,alphabet)\n",
    "    feed_value=np.array(text_int8_repr, dtype=np.int8)\n",
    "    \n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\\n\"\n",
    "        \n",
    "    x_batch_one_hot = np.zeros(shape=[1, len(alphabet), len(feed_value), 1])\n",
    "    for char_pos_in_seq, char_seq_char_ind in enumerate(feed_value):\n",
    "        if char_seq_char_ind != -1:\n",
    "            x_batch_one_hot[0][char_seq_char_ind][char_pos_in_seq][0] = 1\n",
    "\n",
    "    return x_batch_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(input_text):\n",
    "    tf.reset_default_graph()\n",
    "    session_config=tf.ConfigProto( gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5))\n",
    "    session_config.gpu_options.allow_growth = True\n",
    "    saver=tf.train.import_meta_graph(\"./model/train.ckpt.meta\")\n",
    "\n",
    "    with tf.Session(config=session_config) as sess:      \n",
    "        saver.restore(sess,'./model/train.ckpt')\n",
    "        input_x = tf.get_default_graph().get_tensor_by_name('input_x:0')\n",
    "        dropout_keep_prob= tf.get_default_graph().get_tensor_by_name('dropout_keep_prob:0')\n",
    "        answer=sess.run(['fc-3/predictions:0'] , feed_dict={ input_x : single_transform(input_text) , dropout_keep_prob: 1.0})\n",
    "        \n",
    "    sentiment_list={\n",
    "    0: 'SMILING FACE WITH SMILING EYES',\n",
    "    1: 'LOUDLY CRYING FACE',\n",
    "    2: 'PERSON WITH FOLDED HANDS',\n",
    "    3: 'SMILING FACE WITH OPEN MOUTH AND COLD SWEAT',\n",
    "    4:'PENSIVE FACE',\n",
    "    5: 'WEARY FACE',\n",
    "    6: 'HEAVY BLACK HEART',\n",
    "    }\n",
    "        \n",
    "    return sentiment_list[answer[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/train.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HEAVY BLACK HEART'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred('I am praying')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tbop02]",
   "language": "python",
   "name": "conda-env-tbop02-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
